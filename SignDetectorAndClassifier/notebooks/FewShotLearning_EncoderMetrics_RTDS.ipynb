{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b91b328",
   "metadata": {},
   "source": [
    "### Метрики энкодера на основе Resnet18.  \n",
    "#### Выходной слой: *nn.Linear(in_features=512, out_features=1024, bias=True)*\n",
    "\n",
    "### Визуализация в 3 ГК помимо того что не дает колличественных оценок точности энкодера, так и несет в себе в лучшем случае около 40% информации от выходного вектора длинной 1024. \n",
    "\n",
    "###  Необходимо ознакомится с метриками и оценками модели энкодера. исп.:\n",
    "* kMeans\n",
    "* OneClass SVM\n",
    "* Gaussian Mixture\n",
    "\n",
    "### Конечная цель: оценка целесообразности применения энкодера в рамках *данной* задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246547b7",
   "metadata": {},
   "source": [
    "Что откуда качать:\n",
    "\n",
    "* https://drive.google.com/file/d/1-oIPyg3uFT1n--MXyR4Uzx95YqR3NsNT/view?usp=sharing - дополнительные знаки - не референсные. Часть из них - вырезка из видосов, часть - собранно ручками. Разместить в папке *data/additional_sign/*\n",
    "* https://drive.google.com/file/d/1-rTwhmdUdcuPMYz8BiPQV3fiWCSJjE20/view?usp=sharing - *last_encoder_1024_98* - веса энкодера. Разместить в папке с ноутбуком.\n",
    "* https://drive.google.com/file/d/1-K3ee1NbMmx_0T5uwMesStmKnZO_6mWi/view?usp=sharing - rtds с csv, содержащей инфу. Разместить в папке *data*: data/R_MERGED/.. и data/RTDS_DATASET.csv.\n",
    "* https://drive.google.com/file/d/1-l3VvU-WtSoXbW_AaTFUreVD-tgXV8Q0/view?usp=sharing - стоковые знаки. Используются как референс, то есть объеденяются с rtds с пометкой 'train'. Разместить так: data/STOCK_SIGNS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cc73ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import pathlib\n",
    "import shutil\n",
    "import cv2\n",
    "import PIL\n",
    "import cv2\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "TEXT_COLOR = 'black'\n",
    "# Зафиксируем состояние случайных чисел\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (17,10)\n",
    "\n",
    "USE_COLAB_GPU = False\n",
    "IN_COLAB = False\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    USE_COLAB_GPU = True\n",
    "    from google.colab import drive\n",
    "except:\n",
    "    if IN_COLAB:\n",
    "        print('[!] YOU ARE IN COLAB, BUT DIDNT MOUND A DRIVE. Model wont be synced[!]')\n",
    "\n",
    "        if not os.path.isfile(CURRENT_FILE_NAME):\n",
    "            print(\"FIX ME\")\n",
    "        IN_COLAB = False\n",
    "\n",
    "    else:\n",
    "        print('[!] RUNNING NOT IN COLAB')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f40ac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not IN_COLAB:\n",
    "    PROJECT_ROOT = pathlib.Path(os.path.join(os.curdir, os.pardir))\n",
    "else:\n",
    "    PROJECT_ROOT = pathlib.Path('..')\n",
    "    \n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "NOTEBOOKS_DIR = PROJECT_ROOT / 'notebooks'\n",
    "SRC_PATH = str(PROJECT_ROOT / 'src')\n",
    "\n",
    "if SRC_PATH not in sys.path:\n",
    "    sys.path.append(SRC_PATH)\n",
    "    \n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717e7384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet\n",
    "encoder = resnet.resnet18(pretrained=True)\n",
    "encoder.fc = nn.Linear(in_features=512, out_features=512, bias=True)\n",
    "r = encoder.load_state_dict(torch.load('last_encoder403_24')['model'])\n",
    "encoder.eval()\n",
    "encoder.to(device)\n",
    "assert r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa18a032",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def getDataLoaderFromDataset(dataset, shuffle=False, drop_last=True):\n",
    "    \n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last\n",
    "    )\n",
    "    \n",
    "    return loader\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def simpleGetAllEmbeddings(model, dataset, batch_size, dsc=''):\n",
    "    \n",
    "    dataloader = getDataLoaderFromDataset(\n",
    "        dataset,\n",
    "        shuffle=True,\n",
    "        drop_last=False\n",
    "    )\n",
    "    \n",
    "    s, e = 0, 0\n",
    "    pbar = tqdm(\n",
    "        enumerate(dataloader), \n",
    "        total=len(dataloader),\n",
    "        position=0,\n",
    "        leave=False,\n",
    "        desc='Getting all embeddings...' + dsc)\n",
    "    info_arr = []\n",
    "    \n",
    "    add_info_len = None\n",
    "    \n",
    "    for idx, (data, labels, info) in pbar:\n",
    "        data = data.to(device)\n",
    "        \n",
    "        q = model(data)\n",
    "        \n",
    "        if labels.dim() == 1:\n",
    "            labels = labels.unsqueeze(1)\n",
    "        if idx == 0:\n",
    "            labels_ret = torch.zeros(\n",
    "                len(dataloader.dataset),\n",
    "                labels.size(1),\n",
    "                device=device,\n",
    "                dtype=labels.dtype,\n",
    "            )\n",
    "            all_q = torch.zeros(\n",
    "                len(dataloader.dataset),\n",
    "                q.size(1),\n",
    "                device=device,\n",
    "                dtype=q.dtype,\n",
    "            )\n",
    "        \n",
    "        info = np.array(info)\n",
    "        if add_info_len == None:\n",
    "            add_info_len = info.shape[0]\n",
    "        \n",
    "        info_arr.extend(info.T.reshape((-1, add_info_len)))\n",
    "        e = s + q.size(0)\n",
    "        all_q[s:e] = q\n",
    "        labels_ret[s:e] = labels\n",
    "        s = e  \n",
    "    \n",
    "    all_q = torch.nn.functional.normalize(all_q)\n",
    "    return all_q, labels_ret, info_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d951445",
   "metadata": {},
   "source": [
    "### Этап 1.1. Берем RTDS, из него берем *train* как *baseline*. Заменяем *valid* на *test*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33139cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "RTDS_DF = pd.read_csv(DATA_DIR / 'RTDS_DATASET.csv')\n",
    "RTDS_DF['filepath'] = RTDS_DF['filepath'].apply(lambda x: str(DATA_DIR / x))\n",
    "RTDS_DF.drop_duplicates(subset=['filepath'], inplace=True)\n",
    "RTDS_DF['SET'] = RTDS_DF['SET'].apply(lambda x: 'test' if x == 'valid' else x)\n",
    "\n",
    "# print(set(RTDS_DF['SET']))\n",
    "MAKE_ONCE_MERGE_FLAG = True\n",
    "RTDS_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac7c929",
   "metadata": {},
   "source": [
    "### *train* как референс, *valid* - query для валидации.\n",
    "### Этап 1.2. Формируем DataFrame отсутствущих знаков в RTDS.\n",
    "\n",
    "#### Напоминание чего не хватает. Обр. внимание на 3.25. Там для каждоый скорости потенциально.\n",
    "\n",
    "| Знак | Описание | Источник |\n",
    "| ------------- | ------------- | ---- |\n",
    "| 1.6 | Пересечение равнозначных дорог | - |\n",
    "| 1.31 | Туннель | - |\n",
    "| 2.4 | Уступите дорогу | GTSRB Recognition |\n",
    "| 3.21 | Конец запрещения обгона | GTSRB Recognition |\n",
    "| 3.22 | Обгон грузовым автомобилям запрещен | GTSRB Recognition |\n",
    "| 3.23 | Конец запрещения обгона грузовым автомобилям | GTSRB Recognition |\n",
    "| 3.24-90 | Огр 90 | - |\n",
    "| 3.24-100 | Огр 100 | GTSRB Recognition |\n",
    "| 3.24-110 | Огр 110 | - |\n",
    "| 3.24-120 | Огр 120 | GTSRB Recognition |\n",
    "| 3.24-130 | Огр 130 | - |\n",
    "| **3.25** | **Конец огр. максимальной скорости** | **GTSRB Recognition** |\n",
    "| 3.31 | Конец всех ограничений | GTSRB Recognition |\n",
    "| 6.3.2 | Зона для разворота | - |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c19ae9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "additional_DF = pd.DataFrame(columns=RTDS_DF.columns)\n",
    "\n",
    "encode_offset = max(set(RTDS_DF['ENCODED_LABEL'])) + 1\n",
    "files = os.listdir(DATA_DIR / 'additional_sign')\n",
    "\n",
    "FLAG_FIG_3_25 = True\n",
    "\n",
    "for file in files:\n",
    "    sign = file.split('_')[0]\n",
    "    \n",
    "    if FLAG_FIG_3_25 and sign.rsplit('.', 1)[0] == '3.25':\n",
    "        sign = '3.25'\n",
    "        \n",
    "    if FLAG_FIG_3_25 and '3.24' in sign.rsplit('.', 1)[0]:\n",
    "        continue\n",
    "        sign = '3.24'         \n",
    "\n",
    "    row = {'filepath': str(DATA_DIR / 'additional_sign' / file), \n",
    "           'SIGN': sign, \n",
    "           'SET': 'test'\n",
    "          } \n",
    "    additional_DF = additional_DF.append(row, ignore_index=True)\n",
    "\n",
    "additional_DF.ENCODED_LABEL = le.fit_transform(additional_DF.SIGN) + len(set(RTDS_DF.ENCODED_LABEL))\n",
    "# display(additional_DF)\n",
    "\n",
    "if MAKE_ONCE_MERGE_FLAG:\n",
    "    RTDS_DF = RTDS_DF.append(additional_DF, ignore_index=True)\n",
    "    MAKE_ONCE_FLAG = False\n",
    "\n",
    "additional_DF.groupby(['SIGN', 'ENCODED_LABEL']).agg(['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee4401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(RTDS_DF.SIGN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b370e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(RTDS_DF.ENCODED_LABEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc67c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_DICT = dict(zip(RTDS_DF.SIGN, RTDS_DF.ENCODED_LABEL))\n",
    "# LABEL_DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436fe1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(LABEL_DICT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03085880",
   "metadata": {},
   "source": [
    "### Этап 2. Формируем для отсутствующих~=**ДЛЯ ВСЕХ** знаков baseline из образцовых знаков с википедии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2392a47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOCK_SIGNS_CSV_LOCATION = DATA_DIR / 'STOCK_SIGNS.csv'\n",
    "STOCK_SIGNS_DATAFRAME = pd.read_csv(STOCK_SIGNS_CSV_LOCATION)\n",
    "\n",
    "STOCK_SIGNS_DATAFRAME.loc[STOCK_SIGNS_DATAFRAME['SIGN'] == '5.19.2', 'SIGN'] = '5.19.1'\n",
    "if FLAG_FIG_3_25:\n",
    "    STOCK_SIGNS_DATAFRAME['SIGN'] = STOCK_SIGNS_DATAFRAME['SIGN'].apply(\n",
    "        lambda x: '3.25' if x.rsplit('.', 1)[0] == '3.25' else x)\n",
    "\n",
    "## FIXUP для проблем описанных ниже\n",
    "STOCK_SIGNS_DATAFRAME['SIGN'] = STOCK_SIGNS_DATAFRAME['SIGN'].apply(\n",
    "        lambda x: '3.18' if x.rsplit('.', 1)[0] == '3.18' else x)\n",
    "\n",
    "STOCK_SIGNS_DATAFRAME['SIGN'] = STOCK_SIGNS_DATAFRAME['SIGN'].apply(\n",
    "        lambda x: '2.3' if x.rsplit('.', 1)[0] == '2.3' else x)\n",
    "\n",
    "STOCK_SIGNS_DATAFRAME['SIGN'] = STOCK_SIGNS_DATAFRAME['SIGN'].apply(\n",
    "        lambda x: '3.24' if '3.24' in x.rsplit('.', 1)[0] else x)\n",
    "\n",
    "STOCK_SIGNS_DATAFRAME['filepath'] = STOCK_SIGNS_DATAFRAME['filepath'].apply(lambda x: str(DATA_DIR / x))\n",
    "STOCK_SIGNS_DATAFRAME['ENCODED_LABEL'] = [LABEL_DICT[i] for i in STOCK_SIGNS_DATAFRAME['SIGN']]\n",
    "\n",
    "STOCK_SIGNS_DATAFRAME['SET'] = 'train'\n",
    "\n",
    "RTDS_DF = RTDS_DF.append(STOCK_SIGNS_DATAFRAME, ignore_index=True).reset_index()\n",
    "    \n",
    "STOCK_SIGNS_DATAFRAME[::6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6d39b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(STOCK_SIGNS_DATAFRAME['ENCODED_LABEL']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f35dc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(RTDS_DF['SET'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264898f7",
   "metadata": {},
   "source": [
    "## БЕДА, RTDS объеденяет все 3.18 в одну группу. Еще проблемные: 2.3.1. Ну пофиг блин) Смотрим на ошибки."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cdfb44",
   "metadata": {},
   "source": [
    "### Baseline готов, тестовый датасет готов. Че хотим? Хотим получить какие-нибудь метрики."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2692320c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from albumentations.augmentations.transforms import PadIfNeeded\n",
    "from albumentations.augmentations.geometric.resize import LongestMaxSize\n",
    "\n",
    "img_size = 40\n",
    "\n",
    "minimal_transform = A.Compose(\n",
    "        [\n",
    "        LongestMaxSize(\n",
    "            img_size,\n",
    "            interpolation=cv2.INTER_AREA  \n",
    "        ),\n",
    "        PadIfNeeded(\n",
    "            img_size, \n",
    "            img_size, \n",
    "            border_mode=cv2.BORDER_CONSTANT, \n",
    "            value=0\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "class SignDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, set_label=None, hyp=None, transform=None, alpha_color=None):\n",
    "                \n",
    "        self.transform = transform\n",
    "        \n",
    "        if set_label == None:\n",
    "            self.df = df\n",
    "        else:\n",
    "            self.df = df[df['SET']==set_label]\n",
    "        \n",
    "        self.hyp = hyp\n",
    "        self.alpha_color = alpha_color\n",
    "    def __len__(self):\n",
    "        return len(self.df.index)\n",
    "    \n",
    "    def __getitem__(self, index): \n",
    "        label = int(self.df.iloc[index]['ENCODED_LABEL'])\n",
    "        path = str(self.df.iloc[index]['filepath'])\n",
    "        sign = str(self.df.iloc[index]['SIGN'])\n",
    "        img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "        \n",
    "        # check does it contains transparent channel \n",
    "        if img.shape[2] == 4:\n",
    "        # randomize transparent\n",
    "            trans_mask = img[:,:,3] == 0\n",
    "            img[trans_mask] = [self.alpha_color if self.alpha_color else random.randrange(0, 256), \n",
    "                               self.alpha_color if self.alpha_color else random.randrange(0, 256), \n",
    "                               self.alpha_color if self.alpha_color else random.randrange(0, 256), \n",
    "                               255]\n",
    "\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)\n",
    "        # /randomize transparent\n",
    "                \n",
    "        # augment \n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        # /augment\n",
    "        \n",
    "        img = img / 255\n",
    "        return img, label, (path, sign)\n",
    "\n",
    "train_dataset = SignDataset(RTDS_DF, \n",
    "                            set_label='train', \n",
    "                            transform=minimal_transform, \n",
    "                            hyp=None,\n",
    "                            alpha_color=144\n",
    "                           )\n",
    "\n",
    "valid_dataset = SignDataset(RTDS_DF, \n",
    "                            set_label='test',  \n",
    "                            transform=minimal_transform, \n",
    "                            hyp=None,\n",
    "                            alpha_color=144\n",
    "                           )\n",
    "\n",
    "nrows, ncols = 70, 6\n",
    "fig = plt.figure(figsize = (16,200))\n",
    "\n",
    "\n",
    "for idx, (img, encoded_label, (path, sign)) in enumerate(train_dataset):\n",
    "    \n",
    "    if idx > 5:\n",
    "        break\n",
    "    img = torch.Tensor.permute(img, [1, 2, 0]).numpy() \n",
    "    ax = fig.add_subplot(nrows, ncols, idx+1)\n",
    "        \n",
    "    ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), aspect=1)\n",
    "    ax.set_title(str(sign), fontsize=15)\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc729ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 96\n",
    "num_workers = 2 if IN_COLAB else 0\n",
    "\n",
    "encoder.eval()\n",
    "train_embeddings, train_labels, train_additional_info = simpleGetAllEmbeddings(\n",
    "    encoder, train_dataset, batch_size, ' for train'\n",
    ")\n",
    "\n",
    "test_embeddings, test_labels, test_additional_info = simpleGetAllEmbeddings(\n",
    "    encoder, valid_dataset, batch_size, ' for test'\n",
    ")\n",
    "\n",
    "train_labels = train_labels.squeeze(1)\n",
    "test_labels = test_labels.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b41e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_labels.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a987bcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_labels.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f88ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "RTDS_DF.groupby(['SET']).agg('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a6e5b0",
   "metadata": {},
   "source": [
    "### Выше все ок, тестовый набор содержит тест+валид, который сформирован ноутбуков *RTSD-R_MERGED.ipynb*. В валид попало много знаков пешеходного перехода, т.к. их количество значительно превосходило остальные."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22987833",
   "metadata": {},
   "source": [
    "## Get Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d1cbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "INVERSED_LABEL_DICT = {v: k for k, v in LABEL_DICT.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e36b912",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_list = train_labels.cpu().numpy()\n",
    "labels_set = list(set(labels_list))\n",
    "\n",
    "embeddingsListForCentroids = train_embeddings.cpu().numpy()\n",
    "centroidLocationDict = {}\n",
    "\n",
    "for idx, label in enumerate(labels_set):\n",
    "    print('Current label:', label, '[%s]' % INVERSED_LABEL_DICT[label])\n",
    "    mask = labels_list == label\n",
    "    \n",
    "    currentLabelEmbeddingsForCentroids = embeddingsListForCentroids[mask]\n",
    "    zipped = list(zip(*currentLabelEmbeddingsForCentroids))\n",
    "    \n",
    "    singleCoord = []\n",
    "    for coord in zipped:\n",
    "        coord = sum(coord) / len(coord)\n",
    "        singleCoord.append(coord)\n",
    "        # print(coord)\n",
    "        \n",
    "    centroidLocationDict[label] = singleCoord"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adaf66d",
   "metadata": {},
   "source": [
    "### lets construct plot DataFrame\n",
    "Фундаментальный вопрос. fit PCA надо делать на train или на train+test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f1cfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "dim3 = True\n",
    "\n",
    "coords = ['x', 'y'] + (['z'] if dim3 else [])\n",
    "columns = [*coords, 'type', 'size', 'sign', 'filepath', 'color', 'marker']\n",
    "\n",
    "plot_df = pd.DataFrame(columns=columns)\n",
    "# display(plot_df)\n",
    "\n",
    "reducer = PCA(\n",
    "    n_components=3 if dim3 else 2, \n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "train_size = 2\n",
    "train_type = 'train'\n",
    "\n",
    "\n",
    "test_size = 2\n",
    "test_type = 'test'\n",
    "\n",
    "\n",
    "centroid_size = 10\n",
    "centroid_type = 'centroid'\n",
    "\n",
    "\n",
    "## FIT REDUCER\n",
    "train_embeddings_ = reducer.fit_transform(train_embeddings.cpu().numpy())\n",
    "\n",
    "from itertools import cycle\n",
    "import plotly.express as px\n",
    "palette = cycle(\n",
    "        [*px.colors.qualitative.Dark24, \n",
    "         *px.colors.qualitative.Alphabet, \n",
    "         *px.colors.qualitative.Light24]\n",
    "    )\n",
    "colorDict = {}\n",
    "\n",
    "listOfRows = []\n",
    "## CENTROIDS\n",
    "for k, v in centroidLocationDict.items():\n",
    "    coords = reducer.transform(np.array(v).reshape(1, -1)).flatten()\n",
    "    # print([*coords, centroid_type, centroid_size, INVERSED_LABEL_DICT[k]])\n",
    "    path = STOCK_SIGNS_DATAFRAME[STOCK_SIGNS_DATAFRAME['SIGN'] == INVERSED_LABEL_DICT[k]]['filepath'].values[0]\n",
    "    \n",
    "    colorDict[INVERSED_LABEL_DICT[k]] = next(palette)\n",
    "    \n",
    "    row = pd.Series(\n",
    "        [*coords, centroid_type, centroid_size, INVERSED_LABEL_DICT[k], path, \n",
    "         colorDict[INVERSED_LABEL_DICT[k]], 'diamond'],\n",
    "        index=plot_df.columns,\n",
    "    )\n",
    "    listOfRows.append(row)\n",
    "    \n",
    "## TRAIN\n",
    "for idx, (coord, label, info) in tqdm(\n",
    "    enumerate(\n",
    "        zip(train_embeddings_, train_labels, train_additional_info)),\n",
    "    total=len(train_labels)\n",
    "):\n",
    "    label = label.cpu().numpy()\n",
    "    \n",
    "    color = colorDict[INVERSED_LABEL_DICT[int(label)]]\n",
    "    \n",
    "    row = pd.Series(\n",
    "        [*coord, train_type, train_size, INVERSED_LABEL_DICT[int(label)], info[0], \n",
    "         colorDict[INVERSED_LABEL_DICT[k]], 'circle'],\n",
    "        index=plot_df.columns,\n",
    "    )\n",
    "    listOfRows.append(row)\n",
    "\n",
    "\n",
    "    \n",
    "## TEST\n",
    "test_embeddings_ = reducer.transform(test_embeddings.cpu().numpy())\n",
    "\n",
    "for idx, (coord, label, info) in tqdm(\n",
    "    enumerate(\n",
    "        zip(test_embeddings_, test_labels, test_additional_info)),\n",
    "    total=len(test_labels)\n",
    "):\n",
    "    label = label.cpu().numpy()\n",
    "    \n",
    "    row = pd.Series(\n",
    "        [*coord, test_type, test_size, info[1], info[0], \n",
    "        colorDict[INVERSED_LABEL_DICT[k]], 'circle'],\n",
    "        index=plot_df.columns,\n",
    "    )\n",
    "    \n",
    "    listOfRows.append(row)\n",
    "\n",
    "plot_df = plot_df.append(listOfRows)\n",
    "plot_df['x'] = plot_df['x'].astype(float)\n",
    "plot_df['y'] = plot_df['y'].astype(float)\n",
    "if 'z' in plot_df.columns:\n",
    "    plot_df['z'] = plot_df['z'].astype(float)\n",
    "    \n",
    "plot_df['size'] = plot_df['size'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adba312",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(reducer.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dd6372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from plotly import graph_objects as go \n",
    "from itertools import cycle\n",
    "\n",
    "## WERKZEUG FIX\n",
    "import logging\n",
    "log = logging.getLogger('werkzeug')\n",
    "log.setLevel(logging.ERROR)\n",
    "## /WERKZEUG FIX\n",
    "\n",
    "PLOT_CENTROID_AND_TEST_ONLY = True\n",
    "PLOT_LIMIT_FRAC = 0.6\n",
    "\n",
    "if PLOT_CENTROID_AND_TEST_ONLY:\n",
    "    plot_df_ = plot_df[plot_df['type'] != 'train'][::-1]\n",
    "else:\n",
    "    plot_df_ = plot_df[::-1]\n",
    "\n",
    "if PLOT_LIMIT_FRAC:\n",
    "    plot_df_ = plot_df_.groupby(['sign', 'type']).sample(frac=PLOT_LIMIT_FRAC)\n",
    "\n",
    "from jupyter_dash import JupyterDash\n",
    "from dash import dcc, html, Input, Output, no_update\n",
    "import base64\n",
    "\n",
    "\n",
    "app = JupyterDash(__name__)\n",
    "\n",
    "@app.callback(\n",
    "    Output(\"graph-tooltip-5\", \"show\"),\n",
    "    Output(\"graph-tooltip-5\", \"bbox\"),\n",
    "    Output(\"graph-tooltip-5\", \"children\"),\n",
    "    Input(\"graph-5\", \"hoverData\"),\n",
    ")\n",
    "def display_hover(hoverData):\n",
    "\n",
    "    # print(hoverData)\n",
    "    # return False, no_update, no_update\n",
    "    if hoverData is None:\n",
    "        return False, no_update, no_update\n",
    "\n",
    "    hover_data = hoverData[\"points\"][0]\n",
    "    bbox = hover_data[\"bbox\"]\n",
    "    num = hover_data[\"pointNumber\"]\n",
    "    data = hover_data['customdata']\n",
    "    sign = data[0]\n",
    "    rel_img_path = data[1]\n",
    "    \n",
    "    try:\n",
    "        with open(rel_img_path, 'rb') as f:\n",
    "            image = f.read()\n",
    "    except:\n",
    "        print(data)\n",
    "        return False, no_update, no_update\n",
    "    \n",
    "    b64sed_image = 'data:image/png;base64,' + base64.b64encode(image).decode('utf-8')\n",
    "\n",
    "    children = [\n",
    "        html.Div([\n",
    "            html.Img(\n",
    "                src=b64sed_image,\n",
    "                style={\"width\": \"70px\", 'display': 'block', 'margin': '0 auto'},\n",
    "            ),\n",
    "            html.P(sign, style={\"fontSize\": 14, 'text-align':'center'}),\n",
    "            html.P(rel_img_path, style={\"fontSize\": 10}),\n",
    "        ])\n",
    "    ]\n",
    "    return True, bbox, children\n",
    "\n",
    "\n",
    "\n",
    "dim3 = True\n",
    "plot_args = {\n",
    "    'x': 'x',\n",
    "    'y': 'y',\n",
    "    'color': 'sign',\n",
    "    'size': 'size',\n",
    "    'opacity': 0.2 if dim3 else 0.5,\n",
    "    'symbol': 'type',\n",
    "    'hover_name': 'sign',\n",
    "    'hover_data': ['sign', 'filepath', 'type'],\n",
    "    'animation_group': 'type',\n",
    "    'color_discrete_sequence': [\n",
    "        *px.colors.qualitative.Dark24, \n",
    "        # *px.colors.qualitative.Alphabet, \n",
    "        # *px.colors.qualitative.Light24\n",
    "    ]\n",
    "}\n",
    "\n",
    "if dim3:\n",
    "    plotFcn = px.scatter_3d\n",
    "    plot_args.update({'z': 'z'})\n",
    "else:\n",
    "    plotFcn = px.scatter\n",
    "      \n",
    "\n",
    "fig = plotFcn(\n",
    "        plot_df_,\n",
    "        **plot_args,\n",
    "\n",
    "    )\n",
    "\n",
    "fig.update_traces(\n",
    "        hoverinfo=\"none\", \n",
    "        hovertemplate=None,\n",
    "        marker=dict(\n",
    "            line=dict(\n",
    "                width=0)\n",
    "           )\n",
    ")\n",
    "    \n",
    "fig.update_layout(\n",
    "        width=950,\n",
    "        height=950)\n",
    "\n",
    "## FIX ZORDER\n",
    "if True:\n",
    "    sampleData = list(fig.data)\n",
    "    centroidsList = []\n",
    "    for t in list(sampleData[:]):\n",
    "        if (t.ids[0] == 'centroid'):\n",
    "            temp_t = t\n",
    "            sampleData.remove(t)\n",
    "            temp_t['marker']['opacity'] = 1\n",
    "            temp_t['text'] = temp_t['customdata'][0][0]\n",
    "            temp_t['textposition'] = 'top center'\n",
    "            temp_t['mode'] = 'markers+text'\n",
    "            temp_t['marker']['line']['width'] = 40 if dim3 else 2\n",
    "            temp_t['marker']['line']['color'] = 'rgb(0, 0, 0)'\n",
    "            centroidsList.append(temp_t)\n",
    "            \n",
    "    fig.data = tuple(sampleData + centroidsList)\n",
    "    \n",
    "fig.update_layout(font=dict(size=18))\n",
    "\n",
    "app.layout = html.Div(\n",
    "            className=\"container\",\n",
    "            children=[\n",
    "                dcc.Graph(id=\"graph-5\", figure=fig, clear_on_unhover=True),\n",
    "                dcc.Tooltip(id=\"graph-tooltip-5\", direction='bottom'),\n",
    "            ],\n",
    "        )\n",
    "    \n",
    "#fig.show()\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(mode='inline', debug=True, port=2002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1517c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEulerDistance(a, b):\n",
    "    squares = [(p-q) ** 2 for p, q in zip(a, b)]\n",
    "    return sum(squares) ** .5\n",
    "    \n",
    "distancesSign = {}\n",
    "\n",
    "centroidSignList = list(centroidLocationDict.keys())\n",
    "# print(centroidSignList)\n",
    "for idx, ikey in enumerate(centroidSignList):\n",
    "    distancesSign[INVERSED_LABEL_DICT[ikey]] = {}\n",
    "    \n",
    "    \n",
    "    distancesSign[INVERSED_LABEL_DICT[ikey]][INVERSED_LABEL_DICT[ikey]] = np.NaN\n",
    "    for jdx, jkey in enumerate(centroidSignList[idx + 1:]):\n",
    "        dist = getEulerDistance(\n",
    "            centroidLocationDict[ikey],\n",
    "            centroidLocationDict[jkey]\n",
    "        )\n",
    "        distancesSign[INVERSED_LABEL_DICT[ikey]][INVERSED_LABEL_DICT[jkey]] = dist\n",
    "        # distancesSign[INVERSED_LABEL_DICT[jkey]] = {}\n",
    "        # distancesSign[INVERSED_LABEL_DICT[jkey]][INVERSED_LABEL_DICT[ikey]] = dist\n",
    "        # break\n",
    "        \n",
    "# distancesSign\n",
    "df = pd.DataFrame.from_dict(distancesSign)\n",
    "for i in range(len(df)):\n",
    "    df.iloc[i] = df.iloc[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c665018",
   "metadata": {},
   "outputs": [],
   "source": [
    "figH = px.imshow(df)\n",
    "# fig.show()\n",
    "\n",
    "app1 = JupyterDash(__name__)\n",
    "\n",
    "PATH_PREFIX = '../data/STOCK_SIGNS/'\n",
    "PATH_POSTFIX = '.png'\n",
    "\n",
    "@app1.callback(\n",
    "    Output(\"graph-tooltip-5\", \"show\"),\n",
    "    Output(\"graph-tooltip-5\", \"bbox\"),\n",
    "    Output(\"graph-tooltip-5\", \"children\"),\n",
    "    Input(\"graph-5\", \"hoverData\"),\n",
    ")\n",
    "def display_hover(hoverData):\n",
    "    if hoverData is None:\n",
    "        return False, no_update, no_update\n",
    "\n",
    "    hover_data = hoverData[\"points\"][0]\n",
    "    \n",
    "    hover_data['x'] = '2.3.1' if hover_data['x'] == '2.3' else hover_data['x']\n",
    "    hover_data['y'] = '2.3.1' if hover_data['y'] == '2.3' else hover_data['y']        \n",
    "    hover_data['x'] = '3.18.1'  if hover_data['x'] == '3.18' else hover_data['x'] \n",
    "    hover_data['y'] = '3.18.1'  if hover_data['y'] == '3.18' else hover_data['y']\n",
    "        \n",
    "    x_img_path = PATH_PREFIX + hover_data['x'] + PATH_POSTFIX\n",
    "    y_img_path = PATH_PREFIX + hover_data['y'] + PATH_POSTFIX\n",
    "    \n",
    "    bbox = hover_data[\"bbox\"]\n",
    "    \n",
    "    try:\n",
    "        with open(x_img_path, 'rb') as f:\n",
    "            image1 = f.read()\n",
    "        with open(y_img_path, 'rb') as f:\n",
    "            image2 = f.read()\n",
    "    except:\n",
    "        print(hoverData)\n",
    "        return False, no_update, no_update\n",
    "    \n",
    "    img1 = 'data:image/png;base64,' + base64.b64encode(image1).decode('utf-8')\n",
    "    img2 = 'data:image/png;base64,' + base64.b64encode(image2).decode('utf-8')\n",
    "\n",
    "    children = [\n",
    "        html.Div([\n",
    "            html.Img(\n",
    "                src=img1,\n",
    "                style={\"width\": \"70px\",  'margin': '0 auto'},\n",
    "            ),\n",
    "            html.Img(\n",
    "                src=img2,\n",
    "                style={\"width\": \"70px\",  'margin': '0 auto'},\n",
    "            ),\n",
    "            html.P(hover_data['x'] + ':' + hover_data['y'], style={\"fontSize\": 14, 'text-align':'center'}),\n",
    "            html.P(str(hover_data['z']), style={\"fontSize\": 14, 'text-align':'center'}),\n",
    "            \n",
    "        ]),\n",
    "    ]\n",
    "    return True, bbox, children\n",
    "\n",
    "figH.update_traces(\n",
    "        hoverinfo=\"none\", \n",
    "        hovertemplate=None\n",
    ")\n",
    "    \n",
    "figH.update_layout(\n",
    "        width=600,\n",
    "        height=600)\n",
    "\n",
    "app1.layout = html.Div(\n",
    "            className=\"container\",\n",
    "            children=[\n",
    "                dcc.Graph(id=\"graph-5\", figure=figH, clear_on_unhover=True),\n",
    "                dcc.Tooltip(id=\"graph-tooltip-5\", direction='bottom'),\n",
    "            ],\n",
    "        )\n",
    "    \n",
    "#fig.show()\n",
    "if __name__ == '__main__':\n",
    "    app1.run_server(mode='inline', debug=True, port=2003)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7240b4d",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c10859",
   "metadata": {},
   "source": [
    "* train_embeddings, train_labels, train_additional_info - в этих переменных вся инфа о тестовых картинках;\n",
    "* centroidLocationDict - словарь центроидов\n",
    "* getEulerDistance - функция эвклидова расстояния."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd047a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange\n",
    "centroidLocationDictGpu = {}\n",
    "for key, item in centroidLocationDict.items():\n",
    "    centroidLocationDictGpu[key] = torch.Tensor(item).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f80840d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%prun -s cumulative\n",
    "\n",
    "cfDict = {}\n",
    "CHECK_TRAIN = False\n",
    "for i in trange(len(train_embeddings if CHECK_TRAIN else test_embeddings)):\n",
    "    # 1.1 transform embediding to list\n",
    "    emb = train_embeddings[i] if CHECK_TRAIN else test_embeddings[i]\n",
    "    # 1.2. get closer centroid\n",
    "    \n",
    "    minDist = 999999\n",
    "    closerVal = -1\n",
    "    \n",
    "    for key, item in centroidLocationDictGpu.items():\n",
    "        dist = (emb - item).pow(2).sum(0).sqrt()\n",
    "        #print(dist)\n",
    "        # dist = getEulerDistance(emb.tolist(), item.tolist())\n",
    "        #print(dist)\n",
    "        if dist < minDist:\n",
    "            minDist = dist\n",
    "            closerVal = key\n",
    "    \n",
    "    realSign = INVERSED_LABEL_DICT[train_labels[i].item() if CHECK_TRAIN else test_labels[i].item()]\n",
    "    predictedSign =  INVERSED_LABEL_DICT[closerVal]\n",
    "    # print('for', realSign, 'predicted', predictedSign)\n",
    "    \n",
    "    if not realSign in cfDict:\n",
    "        cfDict[realSign] = {} \n",
    "    if not predictedSign in cfDict[realSign]:\n",
    "        cfDict[realSign][predictedSign] = 0\n",
    "        \n",
    "    cfDict[realSign][predictedSign] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800945f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfDict['7.4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db006a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfDf = pd.DataFrame.from_dict(dict(sorted(cfDict.items())))\n",
    "# for i in range(len(cfDf)):\n",
    "#    cfDf.iloc[i] = cfDf.iloc[:, i]\n",
    "cfDf = cfDf.sort_index().T\n",
    "\n",
    "SHOILD_I_NORMOLIZE = False\n",
    "if SHOILD_I_NORMOLIZE:\n",
    "    cfDf = cfDf.apply(lambda x: x / x.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acaf74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "figCf = px.imshow(cfDf.apply(lambda x: x / x.sum(), axis=1), color_continuous_scale=px.colors.sequential.Cividis_r)\n",
    "\n",
    "app2 = JupyterDash(__name__)\n",
    "\n",
    "PATH_PREFIX = '../data/STOCK_SIGNS/'\n",
    "PATH_POSTFIX = '.png'\n",
    "\n",
    "@app2.callback(\n",
    "    Output(\"graph-tooltip-5\", \"show\"),\n",
    "    Output(\"graph-tooltip-5\", \"bbox\"),\n",
    "    Output(\"graph-tooltip-5\", \"children\"),\n",
    "    Input(\"graph-5\", \"hoverData\"),\n",
    ")\n",
    "def display_hover(hoverData):\n",
    "    if hoverData is None:\n",
    "        return False, no_update, no_update\n",
    "\n",
    "    hover_data = hoverData[\"points\"][0]\n",
    "    sum_x = sum(map(int, cfDict[hover_data['y']].values()))\n",
    "    \n",
    "    if not hover_data['z']:\n",
    "        return False, no_update, no_update\n",
    "    \n",
    "    hover_data['x'] = '2.3.1' if hover_data['x'] == '2.3' else hover_data['x']\n",
    "    hover_data['y'] = '2.3.1' if hover_data['y'] == '2.3' else hover_data['y']        \n",
    "    hover_data['x'] = '3.18.1'  if hover_data['x'] == '3.18' else hover_data['x'] \n",
    "    hover_data['y'] = '3.18.1'  if hover_data['y'] == '3.18' else hover_data['y']\n",
    "    hover_data['x'] = '3.25.10' if hover_data['x'] == '3.25' else hover_data['x']\n",
    "    hover_data['y'] = '3.25.10' if hover_data['y'] == '3.25' else hover_data['y']         \n",
    "    hover_data['x'] = '3.24.10' if hover_data['x'] == '3.24' else hover_data['x']\n",
    "    hover_data['y'] = '3.24.10' if hover_data['y'] == '3.24' else hover_data['y']   \n",
    "    \n",
    "    x_img_path = PATH_PREFIX + hover_data['x'] + PATH_POSTFIX\n",
    "    y_img_path = PATH_PREFIX + hover_data['y'] + PATH_POSTFIX\n",
    "    \n",
    "    bbox = hover_data[\"bbox\"]\n",
    "    \n",
    "    try:\n",
    "        with open(x_img_path, 'rb') as f:\n",
    "            image1 = f.read()\n",
    "        with open(y_img_path, 'rb') as f:\n",
    "            image2 = f.read()\n",
    "    except:\n",
    "        # print(hoverData)\n",
    "        return False, no_update, no_update\n",
    "    \n",
    "    img1 = 'data:image/png;base64,' + base64.b64encode(image1).decode('utf-8')\n",
    "    img2 = 'data:image/png;base64,' + base64.b64encode(image2).decode('utf-8')\n",
    "\n",
    "    children = [\n",
    "        html.Div([\n",
    "            html.Img(\n",
    "                src=img1,\n",
    "                style={\"width\": \"70px\",  'margin': '0 auto'},\n",
    "            ),\n",
    "            html.Img(\n",
    "                src=img2,\n",
    "                style={\"width\": \"70px\",  'margin': '0 auto'},\n",
    "            ),\n",
    "            html.P(hover_data['x'] + ':' + hover_data['y'], style={\"fontSize\": 14, 'text-align':'center'}),\n",
    "            html.P(\n",
    "                str(hover_data['z']) \n",
    "                + ': all:' + str(sum_x * hover_data['z']), \n",
    "                style={\"fontSize\": 14, 'text-align':'center'}\n",
    "            ),\n",
    "        ]),\n",
    "    ]\n",
    "    return True, bbox, children\n",
    "\n",
    "figCf.update_traces(\n",
    "        hoverinfo=\"none\", \n",
    "        hovertemplate=None\n",
    ")\n",
    "    \n",
    "figCf.update_layout(\n",
    "        width=950,\n",
    "        height=650)\n",
    "# figCf['layout'].update(plot_bgcolor='green')\n",
    "app2.layout = html.Div(\n",
    "            className=\"container\",\n",
    "            children=[\n",
    "                dcc.Graph(id=\"graph-5\", figure=figCf, clear_on_unhover=True),\n",
    "                dcc.Tooltip(id=\"graph-tooltip-5\", direction='bottom'),\n",
    "            ],\n",
    "        )\n",
    "    \n",
    "#fig.show()\n",
    "if __name__ == '__main__':\n",
    "    app2.run_server(mode='inline', debug=True, port=2003)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9846156",
   "metadata": {},
   "source": [
    "Precision/F1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2d99c5",
   "metadata": {},
   "source": [
    "Значения precision находятся в матрице ниже. В строках - актульные значения, в столбцах точность/вероятность предсказывания соответсвующего знака."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187f39c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0036782f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TPdict = {}\n",
    "FNdict = {}\n",
    "FPdict = {}\n",
    "TNdict = {}\n",
    "\n",
    "cfDf = cfDf.T\n",
    "\n",
    "for i, row in cfDf.iterrows():    \n",
    "    try:\n",
    "        TPdict[i] = cfDf[i][i]\n",
    "        FNdict[i] = cfDf[i].sum() - TPdict[i]\n",
    "        FPdict[i] = cfDf.loc[i].sum() - TPdict[i]\n",
    "        TNdict[i] = cfDf.fillna(0).values.sum() - TPdict[i] - FNdict[i] - FPdict[i]\n",
    "    except:\n",
    "        print('err for key', i)\n",
    "cfDf = cfDf.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08551bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_sign = '7.4'\n",
    "TPdict[_sign], FNdict[_sign], FPdict[_sign], TNdict[_sign]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a56a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PrecisionDict = {}\n",
    "RecallDict = {}\n",
    "F1Dict = {}\n",
    "SupportDict = {}\n",
    "for i in TPdict.keys():\n",
    "    PrecisionDict[i] = TPdict[i] / (TPdict[i] + FPdict[i])\n",
    "    RecallDict[i] = TPdict[i] / (TPdict[i] + FNdict[i])\n",
    "    F1Dict[i] = 2 / (1 / PrecisionDict[i] + 1 / RecallDict[i])\n",
    "    SupportDict[i] = TPdict[i] + FNdict[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d7923b",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Precision', 'Recall', 'F1', 'Support']\n",
    "metrics = {}\n",
    "\n",
    "for i in zip(PrecisionDict.items(), RecallDict.items(), F1Dict.items(), SupportDict.items()):\n",
    "    metrics[i[0][0]] = [i[0][1], i[1][1], i[2][1], i[3][1]]\n",
    "    \n",
    "metricsDf = pd.DataFrame().from_dict(metrics, orient='index')\n",
    "metricsDf.columns = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac529c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "metricsDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c81a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metricsDf.to_excel('metrics3_24.xls', engine='xlsxwriter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04f296e",
   "metadata": {},
   "source": [
    "## [ошибка](https://colab.research.google.com/github/KevinMusgrave/pytorch-metric-learning/blob/master/examples/notebooks/TripletMarginLossMNIST.ipynb)\n",
    "accuracies = accuracy_calculator.get_accuracy"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4bd8b1eb",
   "metadata": {},
   "source": [
    "from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator\n",
    "\n",
    "accuracy_calculator = AccuracyCalculator(\n",
    "    include=('precision_at_1',), k=1,\n",
    "    device=torch.device('cpu'),\n",
    ")\n",
    "\n",
    "accuracies1 = accuracy_calculator.get_accuracy(\n",
    "    test_embeddings,\n",
    "    train_embeddings,    \n",
    "    test_labels, \n",
    "    train_labels, \n",
    "    False\n",
    ")\n",
    "\n",
    "print(accuracies1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "44f64646",
   "metadata": {},
   "source": [
    "accuracy_calculator = AccuracyCalculator(\n",
    "    return_per_class=True,\n",
    "    avg_of_avgs=False,\n",
    "    k=\"max_bin_count\",\n",
    "    device=torch.device('cpu'),\n",
    ")\n",
    "\n",
    "accuracies2 = accuracy_calculator.get_accuracy(\n",
    "    test_embeddings,\n",
    "    train_embeddings,\n",
    "    test_labels, \n",
    "    train_labels,\n",
    "    False\n",
    ")\n",
    "\n",
    "print(accuracies2)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b83b99ba",
   "metadata": {},
   "source": [
    "from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator\n",
    "\n",
    "accuracy_calculator = AccuracyCalculator(\n",
    "    k=1,\n",
    "    return_per_class=True,\n",
    "    device=torch.device('cpu'),\n",
    ")\n",
    "\n",
    "accuracies3 = accuracy_calculator.get_accuracy(\n",
    "    test_embeddings,\n",
    "    train_embeddings,    \n",
    "    test_labels, \n",
    "    train_labels, \n",
    "    False\n",
    ")\n",
    "\n",
    "print(accuracies3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:adas] *",
   "language": "python",
   "name": "conda-env-adas-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
