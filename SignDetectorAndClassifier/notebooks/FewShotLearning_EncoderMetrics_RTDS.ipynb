{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b91b328",
   "metadata": {},
   "source": [
    "### Метрики энкодера на основе Resnet18.  \n",
    "#### Выходной слой: *nn.Linear(in_features=512, out_features=1024, bias=True)*\n",
    "\n",
    "### Визуализация в 3 ГК помимо того что не дает колличественных оценок точности энкодера, так и несет в себе в лучшем случае около 40% информации от выходного вектора длинной 1024. \n",
    "\n",
    "###  Необходимо ознакомится с метриками и оценками модели энкодера. исп.:\n",
    "* kMeans\n",
    "* OneClass SVM\n",
    "* Gaussian Mixture\n",
    "\n",
    "### Конечная цель: оценка целесообразности применения энкодера в рамках *данной* задачи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cc73ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import pathlib\n",
    "import shutil\n",
    "import cv2\n",
    "import PIL\n",
    "import cv2\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "TEXT_COLOR = 'black'\n",
    "# Зафиксируем состояние случайных чисел\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (17,10)\n",
    "\n",
    "USE_COLAB_GPU = False\n",
    "IN_COLAB = False\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    USE_COLAB_GPU = True\n",
    "    from google.colab import drive\n",
    "except:\n",
    "    if IN_COLAB:\n",
    "        print('[!] YOU ARE IN COLAB, BUT DIDNT MOUND A DRIVE. Model wont be synced[!]')\n",
    "\n",
    "        if not os.path.isfile(CURRENT_FILE_NAME):\n",
    "            print(\"FIX ME\")\n",
    "        IN_COLAB = False\n",
    "\n",
    "    else:\n",
    "        print('[!] RUNNING NOT IN COLAB')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f40ac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not IN_COLAB:\n",
    "    PROJECT_ROOT = pathlib.Path(os.path.join(os.curdir, os.pardir))\n",
    "else:\n",
    "    PROJECT_ROOT = pathlib.Path('..')\n",
    "    \n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "NOTEBOOKS_DIR = PROJECT_ROOT / 'notebooks'\n",
    "SRC_PATH = str(PROJECT_ROOT / 'src')\n",
    "\n",
    "if SRC_PATH not in sys.path:\n",
    "    sys.path.append(SRC_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717e7384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet\n",
    "encoder = resnet.resnet18(pretrained=True)\n",
    "encoder.fc = nn.Linear(in_features=512, out_features=1024, bias=True)\n",
    "r = encoder.load_state_dict(torch.load('last_encoder_1024_98')['model'])\n",
    "encoder.eval()\n",
    "assert r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa18a032",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def simpleGetAllEmbeddings(model, dataset, batch_size, dsc=''):\n",
    "    \n",
    "    dataloader = getDataLoaderFromDataset(\n",
    "        dataset,\n",
    "        shuffle=True,\n",
    "        drop_last=False\n",
    "    )\n",
    "    \n",
    "    s, e = 0, 0\n",
    "    pbar = tqdm(\n",
    "        enumerate(dataloader), \n",
    "        total=len(dataloader),\n",
    "        position=0,\n",
    "        leave=False,\n",
    "        desc='Getting all embeddings...' + dsc)\n",
    "    info_arr = []\n",
    "    \n",
    "    add_info_len = None\n",
    "    \n",
    "    for idx, (data, labels, info) in pbar:\n",
    "        data = data.to(device)\n",
    "        \n",
    "        q = model(data)\n",
    "        \n",
    "        if labels.dim() == 1:\n",
    "            labels = labels.unsqueeze(1)\n",
    "        if idx == 0:\n",
    "            labels_ret = torch.zeros(\n",
    "                len(dataloader.dataset),\n",
    "                labels.size(1),\n",
    "                device=device,\n",
    "                dtype=labels.dtype,\n",
    "            )\n",
    "            all_q = torch.zeros(\n",
    "                len(dataloader.dataset),\n",
    "                q.size(1),\n",
    "                device=device,\n",
    "                dtype=q.dtype,\n",
    "            )\n",
    "        \n",
    "        info = np.array(info)\n",
    "        if add_info_len == None:\n",
    "            add_info_len = info.shape[0]\n",
    "        \n",
    "        info_arr.extend(info.T.reshape((-1, add_info_len)))\n",
    "        e = s + q.size(0)\n",
    "        all_q[s:e] = q\n",
    "        labels_ret[s:e] = labels\n",
    "        s = e  \n",
    "    \n",
    "    all_q = torch.nn.functional.normalize(all_q)\n",
    "    return all_q, labels_ret, info_arr\n",
    "\n",
    "### compute accuracy using AccuracyCalculator from pytorch-metric-learning ###\n",
    "def test(train_set, test_set, model, accuracy_calculator, batch_size):\n",
    "    model.eval()\n",
    "    train_embeddings, train_labels, _ = simpleGetAllEmbeddings(model, train_set, batch_size, ' for train')\n",
    "    test_embeddings, test_labels, _ = simpleGetAllEmbeddings(model, test_set, batch_size, ' for test')\n",
    "    train_labels = train_labels.squeeze(1)\n",
    "    test_labels = test_labels.squeeze(1)\n",
    "    accuracies = accuracy_calculator.get_accuracy(\n",
    "        test_embeddings, train_embeddings, test_labels, train_labels, False\n",
    "    )\n",
    "    print(accuracies)\n",
    "    # print(\"Test set accuracy (Precision@1) = {}\".format(accuracies[\"precision_at_1\"]))\n",
    "    return accuracies[\"precision_at_1\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d951445",
   "metadata": {},
   "source": [
    "### Этап 1.1. Берем RTDS, из него берем *train* как *baseline*. Заменяем *valid* на *test*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33139cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "RTDS_DF = pd.read_csv(DATA_DIR / 'RTDS_DATASET.csv')\n",
    "RTDS_DF['filepath'] = RTDS_DF['filepath'].apply(lambda x: str(DATA_DIR / x))\n",
    "RTDS_DF.drop_duplicates(subset=['filepath'], inplace=True)\n",
    "RTDS_DF['SET'] = RTDS_DF['SET'].apply(lambda x: 'test' if x == 'valid' else x)\n",
    "\n",
    "# print(set(RTDS_DF['SET']))\n",
    "MAKE_ONCE_MERGE_FLAG = True\n",
    "RTDS_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac7c929",
   "metadata": {},
   "source": [
    "### Этап 1.2. Формируем DataFrame отсутствущих знаков в RTDS.\n",
    "\n",
    "#### Напоминание чего не хватает. Обр. внимание на 3.25. Там для каждоый скорости потенциально.\n",
    "\n",
    "| Знак | Описание | Источник |\n",
    "| ------------- | ------------- | ---- |\n",
    "| 1.6 | Пересечение равнозначных дорог | - |\n",
    "| 1.31 | Туннель | - |\n",
    "| 2.4 | Уступите дорогу | GTSRB Recognition |\n",
    "| 3.21 | Конец запрещения обгона | GTSRB Recognition |\n",
    "| 3.22 | Обгон грузовым автомобилям запрещен | GTSRB Recognition |\n",
    "| 3.23 | Конец запрещения обгона грузовым автомобилям | GTSRB Recognition |\n",
    "| 3.24-90 | Огр 90 | - |\n",
    "| 3.24-100 | Огр 100 | GTSRB Recognition |\n",
    "| 3.24-110 | Огр 110 | - |\n",
    "| 3.24-120 | Огр 120 | GTSRB Recognition |\n",
    "| 3.24-130 | Огр 130 | - |\n",
    "| **3.25** | **Конец огр. максимальной скорости** | **GTSRB Recognition** |\n",
    "| 3.31 | Конец всех ограничений | GTSRB Recognition |\n",
    "| 6.3.2 | Зона для разворота | - |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c19ae9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_DF = pd.DataFrame(columns=RTDS_DF.columns)\n",
    "\n",
    "encode_offset = max(set(RTDS_DF['ENCODED_LABEL'])) + 1\n",
    "files = os.listdir(DATA_DIR / 'additional_sign')\n",
    "\n",
    "FLAG_FIG_3_25 = True\n",
    "\n",
    "sign_list = list(set([x.split('_')[0] for x in files]))\n",
    "for file in files:\n",
    "    sign = file.split('_')[0]\n",
    "    # print(file.split('_')[1].split('.')[0])\n",
    "    encoded_label = encode_offset + int(sign_list.index(sign))\n",
    "    \n",
    "    if FLAG_FIG_3_25 and sign.rsplit('.', 1)[0] == '3.25':\n",
    "        sign = '3.25'\n",
    "                \n",
    "    # print(sign)\n",
    "    row = {'filepath': str(DATA_DIR / 'additional_sign' / file), \n",
    "           'SIGN': sign, \n",
    "           'ENCODED_LABEL': encoded_label, \n",
    "           'SET': 'test'\n",
    "          } \n",
    "    additional_DF = additional_DF.append(row, ignore_index=True)\n",
    "\n",
    "if 'index' in additional_DF.columns:\n",
    "    additional_DF.drop('index', axis=1, inplace=True)\n",
    "    \n",
    "display(additional_DF)\n",
    "if MAKE_ONCE_MERGE_FLAG:\n",
    "    RTDS_DF = RTDS_DF.append(additional_DF).reset_index()\n",
    "    MAKE_ONCE_FLAG = True\n",
    "    \n",
    "additional_DF.groupby('SIGN').agg(['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddebfae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_DICT = dict(zip(RTDS_DF.SIGN, RTDS_DF.ENCODED_LABEL))\n",
    "LABEL_DICT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7eeda53",
   "metadata": {},
   "source": [
    "### Этап 2. Формируем для отсутствующих знаков baseline из образцовых знаков с википедии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88ffc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOCK_SIGNS_CSV_LOCATION = DATA_DIR / 'STOCK_SIGNS.csv'\n",
    "STOCK_SIGNS_DATAFRAME = pd.read_csv(STOCK_SIGNS_CSV_LOCATION)\n",
    "\n",
    "STOCK_SIGNS_DATAFRAME.loc[STOCK_SIGNS_DATAFRAME['SIGN'] == '5.19.2', 'SIGN'] = '5.19.1'\n",
    "if FLAG_FIG_3_25:\n",
    "    STOCK_SIGNS_DATAFRAME['SIGN'] = STOCK_SIGNS_DATAFRAME['SIGN'].apply(\n",
    "        lambda x: '3.25' if x.rsplit('.', 1)[0] == '3.25' else x)\n",
    "\n",
    "## FIXUP для проблем описанных ниже\n",
    "STOCK_SIGNS_DATAFRAME['SIGN'] = STOCK_SIGNS_DATAFRAME['SIGN'].apply(\n",
    "        lambda x: '3.18' if x.rsplit('.', 1)[0] == '3.18' else x)\n",
    "\n",
    "STOCK_SIGNS_DATAFRAME['SIGN'] = STOCK_SIGNS_DATAFRAME['SIGN'].apply(\n",
    "        lambda x: '2.3' if x.rsplit('.', 1)[0] == '2.3' else x)\n",
    "\n",
    "STOCK_SIGNS_DATAFRAME['filepath'] = STOCK_SIGNS_DATAFRAME['filepath'].apply(lambda x: str(DATA_DIR / x))\n",
    "STOCK_SIGNS_DATAFRAME['ENCODED_LABEL'] = [LABEL_DICT[i] for i in STOCK_SIGNS_DATAFRAME['SIGN']]\n",
    "\n",
    "STOCK_SIGNS_DATAFRAME[::6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6819713d",
   "metadata": {},
   "source": [
    "## БЕДА, RTDS объеденяет все 3.18 в одну группу. Еще проблемные: 2.3.1. Ну пофиг блин) Смотрим на ошибки."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f505b9b0",
   "metadata": {},
   "source": [
    "### Baseline готов, тестовый датасет готов. Че хотим? Хотим получить какие-нибудь метрики."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2692320c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from albumentations.augmentations.transforms import PadIfNeeded\n",
    "from albumentations.augmentations.geometric.resize import LongestMaxSize\n",
    "\n",
    "img_size = 40\n",
    "\n",
    "minimal_transform = A.Compose(\n",
    "        [\n",
    "        LongestMaxSize(\n",
    "            img_size,\n",
    "            # interpolation=cv2.INTER_LANCZOS4 \n",
    "        ),\n",
    "        PadIfNeeded(\n",
    "            img_size, \n",
    "            img_size, \n",
    "            border_mode=cv2.BORDER_CONSTANT, \n",
    "            value=0\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "class SignDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, set_label=None, hyp=None, transform=None, alpha_color=None):\n",
    "                \n",
    "        self.transform = transform\n",
    "        \n",
    "        if set_label == None:\n",
    "            self.df = df\n",
    "        else:\n",
    "            self.df = df[df['SET']==set_label]\n",
    "        \n",
    "        self.hyp = hyp\n",
    "        self.alpha_color = alpha_color\n",
    "    def __len__(self):\n",
    "        return len(self.df.index)\n",
    "    \n",
    "    def __getitem__(self, index): \n",
    "        label = int(self.df.iloc[index]['ENCODED_LABEL'])\n",
    "        path = str(self.df.iloc[index]['filepath'])\n",
    "        sign = str(self.df.iloc[index]['SIGN'])\n",
    "        img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "        \n",
    "        # check does it contains transparent channel \n",
    "        if img.shape[2] == 4:\n",
    "        # randomize transparent\n",
    "            trans_mask = img[:,:,3] == 0\n",
    "            img[trans_mask] = [self.alpha_color if self.alpha_color else random.randrange(0, 256), \n",
    "                               self.alpha_color if self.alpha_color else random.randrange(0, 256), \n",
    "                               self.alpha_color if self.alpha_color else random.randrange(0, 256), \n",
    "                               255]\n",
    "\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)\n",
    "        # /randomize transparent\n",
    "                \n",
    "        # augment \n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        # /augment\n",
    "        \n",
    "        img = img / 255\n",
    "        return img, label, (path, sign)\n",
    "\n",
    "train_dataset = SignDataset(RTDS_DF, \n",
    "                            set_label=None, \n",
    "                            transform=minimal_transform, \n",
    "                            hyp=None)\n",
    "\n",
    "valid_dataset = SignDataset(STOCK_SIGNS_DATAFRAME, \n",
    "                            set_label=None,  \n",
    "                            transform=minimal_transform, \n",
    "                            hyp=None,\n",
    "                            alpha_color=144\n",
    "                           )\n",
    "\n",
    "nrows, ncols = 70, 6\n",
    "fig = plt.figure(figsize = (16,200))\n",
    "\n",
    "for idx, (img, encoded_label, (path, sign)) in enumerate(valid_dataset):\n",
    "    \n",
    "    img = torch.Tensor.permute(img, [1, 2, 0]).numpy() \n",
    "    ax = fig.add_subplot(nrows, ncols, idx+1)\n",
    "        \n",
    "    ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), aspect=1)\n",
    "    ax.set_title(str(sign), fontsize=15)\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988770c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(valid_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:adas] *",
   "language": "python",
   "name": "conda-env-adas-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
