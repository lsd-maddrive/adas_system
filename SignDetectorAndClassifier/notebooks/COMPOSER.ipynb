{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9071b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import pathlib\n",
    "import shutil\n",
    "import cv2\n",
    "import sys\n",
    "from datetime import datetime\n",
    "now = datetime.now\n",
    "\n",
    "%cd adas_system/notebooks\n",
    "\n",
    "try:\n",
    "    USE_TPU = bool(os.environ['COLAB_TPU_ADDR'])\n",
    "except:\n",
    "    USE_TPU = False\n",
    "\n",
    "if USE_TPU:\n",
    "    # !pip uninstall pytorch\n",
    "    # !pip install cloud-tpu-client==0.10 torch==1.10.0\n",
    "    # !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
    "    !pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n",
    "    import torch_xla\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    USE_TPU = True\n",
    "\n",
    "else:\n",
    "    USE_TPU = False\n",
    "\n",
    "IN_COLAB = False\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    if not os.path.isfile('1.0.ClassifierResearch.ipynb'):\n",
    "        print('already exist')\n",
    "        !git clone --branch 9_SignDetector https://github.com/lsd-maddrive/adas_system.git\n",
    "        !gdown --id 1-l3VvU-WtSoXbW_AaTFUreVD-tgXV8Q0\n",
    "        %cd adas_system/notebooks\n",
    "        !unzip -q -o /content/STOCK_SIGNS.zip -d ./../data/\n",
    "\n",
    "except:\n",
    "    if IN_COLAB:\n",
    "        !git clone --branch 9_SignDetector https://github.com/lsd-maddrive/adas_system.git\n",
    "        !gdown --id 1-l3VvU-WtSoXbW_AaTFUreVD-tgXV8Q0\n",
    "        %cd adas_system/notebooks\n",
    "        !mkdir ../data/rtsd-frames\n",
    "        !unzip -q -o /content/STOCK_SIGNS.zip -d ./../data/\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "###\n",
    "import nt_helper\n",
    "from nt_helper.helper_utils import *\n",
    "###\n",
    "\n",
    "TEXT_COLOR = 'black'\n",
    "\n",
    "# Зафиксируем состояние случайных чисел\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (17,10)\n",
    "\n",
    "if USE_TPU:\n",
    "    device = xm.xla_device()\n",
    "else:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "LARGE = False\n",
    "if LARGE:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de296904",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not IN_COLAB:\n",
    "    PROJECT_ROOT = pathlib.Path(os.path.join(os.curdir, os.pardir))\n",
    "else:\n",
    "    PROJECT_ROOT = pathlib.Path('..')\n",
    "    \n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "NOTEBOOKS_DIR = PROJECT_ROOT / 'notebooks'\n",
    "\n",
    "VIDEO_DIR = DATA_DIR / 'reg_videos'\n",
    "video_path = str(VIDEO_DIR / '1.mp4')\n",
    "print(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78e9096",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.yolo import Model\n",
    "from torchvision import models\n",
    "\n",
    "if LARGE:\n",
    "    YOLO_CHECKPOINT_LOCATION = DATA_DIR / 'YoloV5L.pt'\n",
    "    model_cfg_file = DATA_DIR / 'yolov5l_custom_anchors.yaml'\n",
    "else:\n",
    "    YOLO_CHECKPOINT_LOCATION = DATA_DIR / 'YoloV5.pt'\n",
    "    model_cfg_file = DATA_DIR / 'yolov5s_custom_anchors.yaml'   \n",
    "    \n",
    "DETECTOR = Model(cfg=model_cfg_file, ch=3, nc=1)\n",
    "DETECTOR.load_state_dict(torch.load(YOLO_CHECKPOINT_LOCATION, map_location=device))\n",
    "DETECTOR_DETECT_INTEFACE = makeDetectFromModel(DETECTOR)\n",
    "DETECTOR.eval()\n",
    "\n",
    "USE_LAST = True\n",
    "\n",
    "\n",
    "CLASSIFIER = models.resnet18()\n",
    "MODEL_CLASSES = 57\n",
    "CLASSIFIER.fc = nn.Sequential(\n",
    "        nn.Linear(512, MODEL_CLASSES),\n",
    "        nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "if USE_LAST == True:\n",
    "    CLASSIFIER_CHECKPOINT_LOCATION = DATA_DIR / 'classifier_check_point'\n",
    "    ckpt = torch.load(CLASSIFIER_CHECKPOINT_LOCATION)\n",
    "    model_dict = ckpt['model']\n",
    "    CLASSIFIER.load_state_dict(model_dict)\n",
    "else:\n",
    "    CLASSIFIER_CHECKPOINT_LOCATION = DATA_DIR / 'CLASSIFIER_ON_STOCK'\n",
    "    CLASSIFIER.load_state_dict(torch.load(CLASSIFIER_CHECKPOINT_LOCATION, map_location=device))\n",
    "    \n",
    "CLASSIFIER.to(device)\n",
    "CLASSIFIER.eval()\n",
    "\n",
    "DETECTOR_DETECT_INTEFACE.to(device)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "LE_LOCATION = DATA_DIR / 'STOCK_SIGNS_LE.npy'\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.classes_ = np.load(LE_LOCATION, allow_pickle=True)\n",
    "\n",
    "print('[+] SUCCESS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3cf1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT_SUBS = True\n",
    "SUPPRESS_TEXT = True\n",
    "\n",
    "if PLOT_SUBS:\n",
    "    STOCK_SIGNS_CSV_LOCATION = DATA_DIR / 'STOCK_SIGNS.csv'\n",
    "    STOCK_SIGNS_DATAFRAME = pd.read_csv(STOCK_SIGNS_CSV_LOCATION)\n",
    "    STOCK_SIGNS_DATAFRAME['filepath'] = STOCK_SIGNS_DATAFRAME['filepath'].apply(lambda x: str(x).replace('\\\\', '/'))\n",
    "    STOCK_SIGNS_DATAFRAME['filepath'] = STOCK_SIGNS_DATAFRAME['filepath'].apply(lambda x: str(DATA_DIR / x))\n",
    "else:\n",
    "    STOCK_SIGNS = None\n",
    "    \n",
    "STOCK_SIGNS_DATAFRAME[::5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433bfac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "from utils.augmentations import letterbox\n",
    "\n",
    "def FastTransformImgToDetectorInputFormat(frame, trained_img_size=(640, 640)):\n",
    "    # print(frame.shape)\n",
    "    # cv2.imshow('3', frame)\n",
    "    # cv2.waitKey()\n",
    "    frame = letterbox(frame, trained_img_size, auto=False)[0]\n",
    "    # print(frame.shape)\n",
    "    # cv2.imshow('3', frame)\n",
    "    # cv2.waitKey()\n",
    "    frame = frame.transpose((2, 0, 1))[::-1]\n",
    "    frame = np.ascontiguousarray(frame)\n",
    "    frame = torch.from_numpy(frame).float()\n",
    "    frame /= 255\n",
    "    frame = frame[None, ...]\n",
    "    return frame\n",
    "\n",
    "def FastTransformImgToClassifierInputFormat(img, trained_img_size=(64, 64)):\n",
    "    img = cv2.resize(img, trained_img_size, interpolation=cv2.INTER_LANCZOS4)\n",
    "    img_tnsr = torch.Tensor.permute(torch.Tensor(img), [2, 0, 1]).div(255)\n",
    "    img_tnsr = img_tnsr[None, ...]\n",
    "    return img_tnsr\n",
    "\n",
    "def TransformClassifierPred(pred, le):\n",
    "    pred = pred[0]\n",
    "    argmax = np.argmax(pred)\n",
    "    conf = pred[argmax]\n",
    "    return conf, le.inverse_transform([argmax])[0]\n",
    "\n",
    "LABEL_TO_PATH = {}\n",
    "for _, row in STOCK_SIGNS_DATAFRAME.iterrows():\n",
    "    LABEL_TO_PATH[row['SIGN']] = row['filepath']\n",
    "    \n",
    "def insertSignSubframe(frame, label, coords, df):\n",
    "    # instance = df.at[label, 'SIGN'] #df['SIGN'] == label]\n",
    "    # sing_subframe = cv2.imread(instance['filepath'].values[0])\n",
    "    path = LABEL_TO_PATH[label]\n",
    "    sing_subframe = cv2.imread(path)\n",
    "    k = 1\n",
    "\n",
    "    width = int((coords[2] - coords[0]) * k)\n",
    "    height = int((coords[3] - coords[1]) * k) \n",
    "    dim = (width, height) \n",
    "        \n",
    "    sing_subframe = cv2.resize(sing_subframe, dim)\n",
    "    \n",
    "    DELTA_Y = 0 # height\n",
    "    DELTA_X = width\n",
    "    try:\n",
    "        frame[coords[1] + DELTA_Y:coords[1] + sing_subframe.shape[0] + DELTA_Y, \n",
    "              coords[0] + DELTA_X:coords[0] + sing_subframe.shape[1] + DELTA_X, \n",
    "              :] = sing_subframe\n",
    "    except:\n",
    "        pass\n",
    "    return frame\n",
    "\n",
    "video = cv2.VideoCapture(video_path)\n",
    "display_handle1=display(1, display_id=True)\n",
    "\n",
    "if LARGE:\n",
    "    detector_img_size = 416\n",
    "    detector_img_size = (detector_img_size, detector_img_size)\n",
    "else:\n",
    "    detector_img_size = 640\n",
    "    detector_img_size = (detector_img_size, detector_img_size)\n",
    "    \n",
    "classifier_img_size = 40\n",
    "classifier_img_size = (classifier_img_size, classifier_img_size)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print('device', device)\n",
    "i = 0\n",
    "while True:\n",
    "    i += 1\n",
    "    t0 = now()\n",
    "    \n",
    "    _, frame_src = video.read()\n",
    "    \n",
    "    if i % 2 == 0:\n",
    "        for _ in range(8):\n",
    "            _, frame_src = video.read()\n",
    "        continue\n",
    "    \n",
    "    DETECTOR_INPUT = FastTransformImgToDetectorInputFormat(frame_src, trained_img_size=detector_img_size)\n",
    "    DETECTOR_PREDS = DETECTOR_DETECT_INTEFACE(DETECTOR_INPUT.to(device))\n",
    "    \n",
    "    DETECTOR_PREDS_DATA = DETECTOR_DETECT_INTEFACE.translatePreds(DETECTOR_PREDS, \n",
    "                                                                  nn_img_size=detector_img_size, \n",
    "                                                                  source_img_size=frame_src.shape, \n",
    "                                                                  conf_thres=0.3, \n",
    "                                                                  max_det=10)\n",
    "    for i in range(DETECTOR_PREDS_DATA['count']):\n",
    "        COORD_ARR = [DETECTOR_PREDS_DATA['coords'][i][0], \n",
    "                     DETECTOR_PREDS_DATA['coords'][i][1], \n",
    "                     DETECTOR_PREDS_DATA['coords'][i][2], \n",
    "                     DETECTOR_PREDS_DATA['coords'][i][3]\n",
    "                    ]\n",
    "\n",
    "        CROPPED_IMG = frame_src[COORD_ARR[1]:COORD_ARR[3], \n",
    "                        COORD_ARR[0]:COORD_ARR[2]].copy()\n",
    "                \n",
    "        if CROPPED_IMG.shape[0] < 5 or CROPPED_IMG.shape[1] < 5:\n",
    "            continue\n",
    "\n",
    "        CLASSIFIER_INPUT = FastTransformImgToClassifierInputFormat(CROPPED_IMG, classifier_img_size)\n",
    "        CLASSIFIER_PRED = CLASSIFIER(CLASSIFIER_INPUT.to(device)).cpu().detach().numpy()\n",
    "        CLASSIFIER_PRED_CONF, CLASSIFIER_PRED_SIGN = TransformClassifierPred(CLASSIFIER_PRED, le)\n",
    "        \n",
    "        \n",
    "        # CLASSIFIER_PRED_CONF, CLASSIFIER_PRED_SIGN = 1, '1'\n",
    "        \n",
    "        COLOR = (0, 255, 0)\n",
    "        if CLASSIFIER_PRED_CONF < 0.2:\n",
    "            CLASSIFIER_PRED_SIGN = '?'\n",
    "            COLOR = (0, 0, 255)\n",
    "        elif PLOT_SUBS:\n",
    "            frame_src = insertSignSubframe(frame_src, CLASSIFIER_PRED_SIGN, COORD_ARR, STOCK_SIGNS_DATAFRAME)\n",
    "        \n",
    "        if SUPPRESS_TEXT:\n",
    "            CLASSIFIER_PRED_SIGN = ''\n",
    "        else:\n",
    "            CLASSIFIER_PRED_SIGN += ':' + str(int(CLASSIFIER_PRED_CONF * 100)) + ':' + str(int(DETECTOR_PREDS_DATA['confs'][i] * 100))\n",
    "        \n",
    "        frame_src = cv2.rectangle(frame_src, (COORD_ARR[0], COORD_ARR[1]), \n",
    "                        (COORD_ARR[2], COORD_ARR[3]), \n",
    "                        COLOR, \n",
    "                        3)\n",
    "        \n",
    "        DELTA = 15\n",
    "        \n",
    "        frame_src = cv2.putText(frame_src, CLASSIFIER_PRED_SIGN, #str(round(DETECTOR_PREDS_DATA['confs'][i], 3)), \n",
    "                           (COORD_ARR[0], COORD_ARR[3] + DELTA),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                           0.7, (255, 255, 255),\n",
    "                           3, cv2.LINE_AA\n",
    "                          )\n",
    "        \n",
    "        frame_src = cv2.putText(frame_src, CLASSIFIER_PRED_SIGN, #str(round(DETECTOR_PREDS_DATA['confs'][i], 3)), \n",
    "                           (COORD_ARR[0], COORD_ARR[3] + DELTA),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                           0.7, (0, 0, 0),\n",
    "                           1, cv2.LINE_AA\n",
    "                          )\n",
    "        \n",
    "        # cv2.imshow('waitKey', CROPPED_IMG)\n",
    "        # cv2.waitKey(0)\n",
    "        \n",
    "    dt = now() - t0\n",
    "    frame_src = cv2.putText(frame_src, \n",
    "                            'fps:' + str(round(1 / dt.total_seconds() , 2)),\n",
    "                            (0, 70),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                            3, (0, 0, 0),\n",
    "                            3, cv2.LINE_AA\n",
    "                           )\n",
    "    _, frame_src = cv2.imencode('.jpeg', frame_src)\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    display_handle1.update(Image(data=frame_src.tobytes()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d9e6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_src.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e67b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "CROPPED_IMG.shape[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
