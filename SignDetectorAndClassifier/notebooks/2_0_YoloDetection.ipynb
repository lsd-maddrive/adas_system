{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef5ffffc",
   "metadata": {
    "id": "ef5ffffc"
   },
   "source": [
    "Объединенный датасет по [ссылке](https://drive.google.com/file/d/1-naWZnKN3okobAv9FNK-6K1cIzJM991P/view).\n",
    "\n",
    "> *gt_Set_NaN.csv - содержит тот же датасет, но значения колонки Set обнулено*\n",
    "\n",
    "gt - датафрейм содержащий:  \n",
    "* имена файлов - поле filename\n",
    "* класс знака - поле sign_class\n",
    "* координаты знаков\n",
    "* в какой набор включен знак - поле Set $\\in$ $\\{train, valid, test\\}$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6560301",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e6560301",
    "outputId": "53f7c64e-66cb-421f-f2b2-3b28027780d8"
   },
   "outputs": [],
   "source": [
    "# autoreload \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# core imports\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# PROJECT_ROOT\n",
    "try:\n",
    "    PROJECT_ROOT = Path(os.readlink(f'/proc/{os.environ[\"JPY_PARENT_PID\"]}/cwd'))\n",
    "except FileNotFoundError:\n",
    "    __file = %pwd\n",
    "    PROJECT_ROOT = Path(__file).parents[1]\n",
    "    \n",
    "DATA_DIR = PROJECT_ROOT / 'SignDetectorAndClassifier' / 'data'\n",
    "DATASET_DIR = DATA_DIR / 'YOLO_DATASET'\n",
    "# Зафиксируем состояние случайных чисел\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (34,20)\n",
    "\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ec2ac0",
   "metadata": {
    "id": "90ec2ac0"
   },
   "source": [
    "## Init dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8974552a",
   "metadata": {
    "id": "8974552a"
   },
   "outputs": [],
   "source": [
    "FORMATED_GT_PATH = DATASET_DIR / 'USER_FULL_FRAMES.csv'\n",
    "full_gt = pd.read_csv(FORMATED_GT_PATH)\n",
    "full_gt['filepath'] = full_gt['filepath'].apply(lambda x: str(DATASET_DIR / x.replace('\\\\', '/')))\n",
    "FULL_GT_SRC_LEN = len(full_gt.index)\n",
    "\n",
    "FULL_GT_SRC_LEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a582a79b",
   "metadata": {
    "id": "a582a79b"
   },
   "source": [
    "## Init dataset DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edbb41f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "1edbb41f",
    "outputId": "9fe78793-da3c-42b5-8d5a-19ef463e9cd7"
   },
   "outputs": [],
   "source": [
    "if os.path.isfile(FORMATED_GT_PATH):\n",
    "    print(\"FORMATED GT EXIST. LOAD IT\")\n",
    "    \n",
    "    import ast\n",
    "    formated_full_gt_df = full_gt\n",
    "    formated_full_gt_df['coords'].replace({'\\n ':',', ' \\s+': ' ', '\\[ ': '['}, regex=True, inplace=True)\n",
    "    \n",
    "    formated_full_gt_df['coords'] = formated_full_gt_df['coords'].apply(\n",
    "        lambda x: ast.literal_eval(x)\n",
    "    )\n",
    "    \n",
    "    formated_full_gt_df['size'] = formated_full_gt_df['size'].apply(\n",
    "        lambda x: ast.literal_eval(x)\n",
    "    )\n",
    "else:\n",
    "    print(\"FORMATED GT DOESNT EXIST. CREATE IT\")\n",
    "    \n",
    "    # get all original filenames\n",
    "    full_gt_unique_filenames = set(full_gt['filename'])\n",
    "    full_gt_unique_filenames_size = len(full_gt_unique_filenames)\n",
    "    \n",
    "    formated_full_gt_list = []\n",
    "\n",
    "    import imagesize\n",
    "    i = 0\n",
    "    for src_filename_iterator in list(full_gt_unique_filenames):\n",
    "\n",
    "        mask = np.in1d(full_gt['filename'], [src_filename_iterator])\n",
    "        coord_data_arr = full_gt[mask][['x_from', 'y_from', 'width', 'height']].to_numpy()\n",
    "        \n",
    "        filepath = DATA_DIR / \"USER_FULL_FRAMES\" / src_filename_iterator\n",
    "        origW, origH = imagesize.get(filepath)\n",
    "                \n",
    "        rel_coord = []\n",
    "        for coord in coord_data_arr:\n",
    "            # make from x, y, dx, dx -> x1, y1, x2, y2\n",
    "            CV2RectangleCoords = ConvertAbsTLWH2CV2Rectangle(coord)\n",
    "   \n",
    "            # make from x1, y1, x2, y2 -> x, y, w, h\n",
    "            CV2CircleCoords = ConvertCV2Rectangle2CenterXYWH(CV2RectangleCoords)\n",
    "            \n",
    "            # make x, y, w, h -> relative x, y, w, h\n",
    "            rel_instance = MakeRel(CV2CircleCoords, origW, origH)\n",
    "            rel_coord.append(rel_instance)\n",
    "            \n",
    "        if i % 100 == 0:\n",
    "            printProgressEnum(i, full_gt_unique_filenames_size)\n",
    "        i += 1\n",
    "\n",
    "        formated_full_gt_list.append([str(filepath), rel_coord, [origW, origH]])\n",
    "\n",
    "    formated_full_gt_df = pd.DataFrame(formated_full_gt_list, columns=['filepath', 'coords', 'size'])\n",
    "    formated_full_gt_df.to_csv(FORMATED_GT_PATH, index=False)\n",
    "\n",
    "if 'set' in formated_full_gt_df.columns:\n",
    "    print('SET ALREADY EXIST')\n",
    "else:\n",
    "    print('SET DOESNT EXIST. LETS CREATE IT')\n",
    "    formated_full_gt_df_index_count = len(formated_full_gt_df.index)\n",
    "    TRAIN_SIZE = round(0.7 * formated_full_gt_df_index_count)\n",
    "    VALID_SIZE = round(0.2 * formated_full_gt_df_index_count)\n",
    "    TEST_SIZE = round(formated_full_gt_df_index_count - TRAIN_SIZE - VALID_SIZE)\n",
    "        \n",
    "    assert TRAIN_SIZE + VALID_SIZE + TEST_SIZE == formated_full_gt_df_index_count, 'wrong split'\n",
    "    set_series = pd.Series('test', index=range(TEST_SIZE)).append(\n",
    "        pd.Series('train', index=range(TRAIN_SIZE)).append(\n",
    "            pd.Series('valid', index=range(VALID_SIZE))\n",
    "        )\n",
    "    ).sample(frac=1).reset_index(drop=True)\n",
    "    formated_full_gt_df['set'] = set_series\n",
    "    formated_full_gt_df.to_csv(FORMATED_GT_PATH, index=False)\n",
    "    \n",
    "display(formated_full_gt_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33148d2",
   "metadata": {
    "id": "e33148d2"
   },
   "source": [
    "# simple test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2de1281",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 766
    },
    "id": "e2de1281",
    "outputId": "71dc5490-0ab9-4a95-dfa5-7f1342896af8"
   },
   "outputs": [],
   "source": [
    "from maddrive_adas.utils.transforms import (\n",
    "    UnmakeRel, MakeRel,\n",
    "    ConvertCenterXYWH2CV2Rectangle\n",
    ")\n",
    "\n",
    "instance = formated_full_gt_df.iloc[15466]\n",
    "\n",
    "path_ = str(instance['filepath'])\n",
    "print(path_)\n",
    "img = cv2.imread(path_)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "h, w = img.shape[0], img.shape[1]\n",
    "print('Shape:', w, h)\n",
    "\n",
    "for i in instance['coords']:\n",
    "    \n",
    "    xywh = UnmakeRel(i, w, h)\n",
    "    x1y1x2y2 = ConvertCenterXYWH2CV2Rectangle(xywh)\n",
    "    print('+', MakeRel(x1y1x2y2, w, h))\n",
    "    print('xywh', xywh)\n",
    "    print('x1y1x2y2', x1y1x2y2)\n",
    "    \n",
    "    \n",
    "    img = cv2.rectangle(img, (x1y1x2y2[0], x1y1x2y2[1]), \n",
    "                        (x1y1x2y2[2], x1y1x2y2[3]), \n",
    "                        (255, 0, 0), \n",
    "                        3)\n",
    "    \n",
    "    img = cv2.circle(img, \n",
    "                     (xywh[0], xywh[1]), \n",
    "                     xywh[2] // 2, \n",
    "                     (255, 255, 0), \n",
    "                     3)\n",
    "\n",
    "plt.figure(figsize = (20, 20))  \n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a67aff",
   "metadata": {
    "id": "98a67aff"
   },
   "source": [
    "# Now we have pd.DataFrame that contains filenames, list of relative coordinates, corresponding photo resoulutions and marks for set. \n",
    "## createDataLoaderAndDataSet function in utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f8ff5c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "e1f8ff5c",
    "outputId": "952c4349-3111-4c3b-d128-7c9b1304cc18"
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "from maddrive_adas.utils.datasets import create_dataloader_and_dataset_for_yolo\n",
    "hyps_file = DATA_DIR / \"hyp.scratch.yaml\"\n",
    "with open(hyps_file, errors='ignore') as f:\n",
    "    hyp = yaml.safe_load(f)\n",
    "\n",
    "\n",
    "IMG_SIZE = 640\n",
    "batch_size = 1\n",
    "train_loader, train_dataset = create_dataloader_and_dataset_for_yolo(\n",
    "    formated_full_gt_df, \n",
    "    'train',\n",
    "    hyp_arg=hyp,\n",
    "    imgsz=IMG_SIZE, \n",
    "    batch_size=batch_size, \n",
    "    augment=True\n",
    ")\n",
    "\n",
    "test_loader, test_dataset = create_dataloader_and_dataset_for_yolo(\n",
    "    formated_full_gt_df, \n",
    "    'test',\n",
    "    hyp_arg=hyp,\n",
    "    imgsz=IMG_SIZE, \n",
    "    batch_size=batch_size, \n",
    "    augment=True\n",
    ")\n",
    "\n",
    "del test_loader, train_loader, test_dataset\n",
    "\n",
    "img, labels_out, filepath, shapes = train_dataset[98]\n",
    "# img_, labels_out_, filepath_, shapes_ = test_dataset[random.randrange(0, len(test_dataset))]\n",
    "\n",
    "imgNT = img.numpy().transpose(1, 2, 0).astype(np.uint8).copy() #, cv2.COLOR_BGR2RGB)\n",
    "# print(labels_out)\n",
    "# print(filepath)\n",
    "for coord in labels_out[:, 2:]:\n",
    "    # print(coord)\n",
    "    h, w = shapes[0]\n",
    "    xywh = UnmakeRel(coord, IMG_SIZE, IMG_SIZE)\n",
    "    x1y1x2y2 = ConvertCenterXYWH2CV2Rectangle(xywh)\n",
    "    # print(x1y1x2y2)\n",
    "    imgNT = cv2.rectangle(imgNT, (x1y1x2y2[0], x1y1x2y2[1]), \n",
    "                        (x1y1x2y2[2], x1y1x2y2[3]), \n",
    "                        (255, 0, 0), \n",
    "                        3)\n",
    "\n",
    "\n",
    "plt.figure(figsize = (80, 80))  \n",
    "%matplotlib inline\n",
    "plt.imshow(imgNT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XGelWDPIcxGl",
   "metadata": {
    "id": "XGelWDPIcxGl"
   },
   "source": [
    "<b>ROAD SIGN ANCHORS:</b> 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326 [Source](https://grechka.family/dmitry/blog/2019/09/yolo-v3-anchors-for-traffic-sign-detection/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cf049f",
   "metadata": {
    "id": "92cf049f"
   },
   "source": [
    "# Train?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b909113",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0b909113",
    "outputId": "c19aa788-e0c7-4947-e4f1-570eea72c3ae"
   },
   "outputs": [],
   "source": [
    "from torch.optim import SGD, lr_scheduler\n",
    "from torch.cuda import amp\n",
    "from maddrive_adas.utils.general import one_cycle, LOGGER\n",
    "from maddrive_adas.utils.loss import ComputeLoss\n",
    "from maddrive_adas.utils.torch_utils import ModelEMA, de_parallel\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from maddrive_adas.models.yolo import Model\n",
    "import yaml\n",
    "\n",
    "\n",
    "hyps_file = DATA_DIR / \"hyp.scratch.yaml\"\n",
    "with open(hyps_file, errors='ignore') as f:\n",
    "    hyp = yaml.safe_load(f)\n",
    "\n",
    "\n",
    "def train(epochs, model, train_loader, valid_loader, device, opt=None, imgsz=640):\n",
    "       \n",
    "    ###\n",
    "    start_epoch = 0\n",
    "    nc = 1\n",
    "    model.float()\n",
    "    # print(device.type)\n",
    "    cuda = device.type == 'cuda'\n",
    "    # print(cuda)\n",
    "    nb = len(train_loader)\n",
    "    nw = max(round(hyp['warmup_epochs'] * nb), 1000)\n",
    "    nbs = 64  # nominal batch size\n",
    "    batch_size = train_loader.batch_size\n",
    "    last_opt_step = -1\n",
    "    ###\n",
    "        \n",
    "    g0, g1, g2 = [], [], []  # optimizer parameter groups\n",
    "    for v in model.modules():\n",
    "        if hasattr(v, 'bias') and isinstance(v.bias, torch.nn.Parameter):  # bias\n",
    "            g2.append(v.bias)\n",
    "        if isinstance(v, torch.nn.BatchNorm2d):  # weight (no decay)\n",
    "            g0.append(v.weight)\n",
    "        elif hasattr(v, 'weight') and isinstance(v.weight, torch.nn.Parameter):  # weight (with decay)\n",
    "            g1.append(v.weight)\n",
    "    \n",
    "    optimizer = SGD(g0, lr=hyp['lr0'], momentum=hyp['momentum'], nesterov=True)\n",
    "    \n",
    "    optimizer.add_param_group({'params': g1, 'weight_decay': hyp['weight_decay']})  # add g1 with weight_decay\n",
    "    optimizer.add_param_group({'params': g2})  # add g2 (biases)\n",
    "    del g0, g1, g2\n",
    "    \n",
    "\n",
    "    lf = one_cycle(1, hyp['lrf'], epochs)  # cosine 1->hyp['lrf']\n",
    "    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)\n",
    "    \n",
    "    ema = ModelEMA(model)\n",
    "    \n",
    "    nl = de_parallel(model).model[-1].nl  # number of detection layers (to scale hyps)\n",
    "    hyp['box'] *= 3 / nl  # scale to layers\n",
    "    hyp['cls'] *= nc / 80 * 3 / nl  # scale to classes and layers\n",
    "    hyp['obj'] *= (imgsz / 640) ** 2 * 3 / nl  # scale to image size and layers\n",
    "    hyp['label_smoothing'] = opt.label_smoothing if opt else 0.\n",
    "    \n",
    "    model.nc = nc  # attach number of classes to model\n",
    "    model.hyp = hyp  # attach hyperparameters to model\n",
    "    model.names = ['sign']\n",
    "    \n",
    "    scaler = amp.GradScaler(enabled=cuda)\n",
    "    compute_loss = ComputeLoss(model)\n",
    "    \n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        model.train()\n",
    "        mloss = torch.zeros(3, device=device)\n",
    "        \n",
    "        pbar = enumerate(train_loader)\n",
    "        LOGGER.info(('\\n' + '%10s' * 7) % ('Epoch', 'gpu_mem', 'box', 'obj', 'cls', 'labels', 'img_size'))\n",
    "        pbar = tqdm(pbar, total=nb, bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}')  # progress bar\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        for i, (imgs, targets, paths, _) in pbar:\n",
    "            # sleep(1)\n",
    "            ni = i + nb * epoch  # number integrated batches (since train start)\n",
    "            imgs = imgs.to(device, non_blocking=True).float() / 255  # uint8 to float32, 0-255 to 0.0-1.0\n",
    "            \n",
    "            # Warmup\n",
    "            if ni <= nw:\n",
    "                xi = [0, nw]  # x interp\n",
    "                # compute_loss.gr = np.interp(ni, xi, [0.0, 1.0])  # iou loss ratio (obj_loss = 1.0 or iou)\n",
    "                accumulate = max(1, np.interp(ni, xi, [1, nbs / batch_size]).round())\n",
    "                for j, x in enumerate(optimizer.param_groups):\n",
    "                    # bias lr falls from 0.1 to lr0, all other lrs rise from 0.0 to lr0\n",
    "                    x['lr'] = np.interp(ni, xi, [hyp['warmup_bias_lr'] if j == 2 else 0.0, x['initial_lr'] * lf(epoch)])\n",
    "                    if 'momentum' in x:\n",
    "                        x['momentum'] = np.interp(ni, xi, [hyp['warmup_momentum'], hyp['momentum']])\n",
    "\n",
    "            # Forward\n",
    "            with amp.autocast(enabled=cuda):\n",
    "                pred = model(imgs)  # forward\n",
    "                loss, loss_items = compute_loss(pred, targets.to(device))  # loss scaled by batch_size\n",
    "                \n",
    "            # Backward\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # Optimize\n",
    "            if ni - last_opt_step >= accumulate:\n",
    "                scaler.step(optimizer)  # optimizer.step\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                if ema:\n",
    "                    ema.update(model)\n",
    "                last_opt_step = ni\n",
    "            \n",
    "            if True:\n",
    "                mloss = (mloss * i + loss_items) / (i + 1)  # update mean losses\n",
    "                mem = f'{torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0:.3g}G'  # (GB)\n",
    "                pbar.set_description(('%10s' * 2 + '%10.4g' * 5) % (\n",
    "                    f'{epoch}/{epochs - 1}', mem, *mloss, targets.shape[0], imgs.shape[-1]))\n",
    "        \n",
    "        ###\n",
    "        # every 5 epochs check mAP\n",
    "        if False and (epoch + 1) % 5 == 0:\n",
    "            ema.update_attr(model, include=['yaml', 'nc', 'hyp', 'names', 'stride', 'class_weights'])\n",
    "            map = valid_epoch()\n",
    "\n",
    "        ###\n",
    "\n",
    "        # Scheduler\n",
    "        lr = [x['lr'] for x in optimizer.param_groups]  # for loggers\n",
    "        scheduler.step()\n",
    "                \n",
    "        now = datetime.now()\n",
    "        model_save_name = DATA_DIR / 'YoloV5_{}_lbox{:.4f}_lobj{:.4f}.pt'.format(\n",
    "            now.strftime(\"%d.%m_%H.%M\"),\n",
    "            mloss[0], mloss[1]\n",
    "        )\n",
    "        \n",
    "        torch.save(model.state_dict(), model_save_name)\n",
    "    \n",
    "    print('TRAIN END')\n",
    "    return 1\n",
    "\n",
    "\n",
    "num_workers = 8\n",
    "\n",
    "restore= DATA_DIR / 'YoloV5_27.06_17.48_lbox0.0312_lobj0.0063.pt'\n",
    "model_cfg_file = DATA_DIR / 'yolov5l_custom_anchors.yaml'\n",
    "IMG_SIZE = 640\n",
    "batch_size = 18\n",
    "\n",
    "train_loader, train_dataset = create_dataloader_and_dataset_for_yolo(\n",
    "    formated_full_gt_df, \n",
    "    'train',\n",
    "    hyp_arg=hyp,\n",
    "    imgsz=IMG_SIZE, \n",
    "    batch_size=batch_size, \n",
    "    augment=True,\n",
    "    nw=num_workers\n",
    ")\n",
    "\n",
    "valid_loader, valid_dataset = create_dataloader_and_dataset_for_yolo(\n",
    "    formated_full_gt_df, \n",
    "    'valid',\n",
    "    hyp_arg=hyp,\n",
    "    imgsz=IMG_SIZE, \n",
    "    batch_size=batch_size * 3, \n",
    "    augment=False,\n",
    "    nw=num_workers\n",
    ")\n",
    "\n",
    "SHOULD_I_TRAIN = True\n",
    "IN_COLAB = True\n",
    "\n",
    "model = Model(cfg=model_cfg_file, ch=3, nc=1)\n",
    "try:\n",
    "    model.load_state_dict(torch.load(restore, map_location=device))\n",
    "    model.eval()\n",
    "    print('Model successfully loaded!')\n",
    "except FileNotFoundError as exc_obj:\n",
    "    print(f'[!] File [{restore}] not found')\n",
    "\n",
    "hyp['lr0'] = 0.001\n",
    "model.hyp = hyp  # attach hyperparameters to model\n",
    "model.to(device)\n",
    "\n",
    "if SHOULD_I_TRAIN:\n",
    "    pass\n",
    "    \n",
    "else:\n",
    "    pass\n",
    "\n",
    "model.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07225ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521b903d",
   "metadata": {},
   "source": [
    "## Core cell for training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qMmgen5Ocw8j",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 972
    },
    "id": "qMmgen5Ocw8j",
    "outputId": "916df554-5b25-4427-f8cc-8770c7c2cec5"
   },
   "outputs": [],
   "source": [
    "model = train(50, model, train_loader, valid_loader, device, imgsz=IMG_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "K9J_DjhatvaU",
   "metadata": {
    "id": "K9J_DjhatvaU"
   },
   "outputs": [],
   "source": [
    "from maddrive_adas.utils.metrics import ConfusionMatrix, ap_per_class\n",
    "from maddrive_adas.utils.general import (\n",
    "    box_iou, check_dataset, check_img_size, check_requirements, check_yaml,\n",
    "    increment_path, non_max_suppression, print_args,\n",
    "    scale_coords, xywh2xyxy, xyxy2xywh\n",
    ")\n",
    "\n",
    "def process_batch(detections, labels, iouv):\n",
    "    \"\"\"\n",
    "    Return correct predictions matrix. Both sets of boxes are in (x1, y1, x2, y2) format.\n",
    "    Arguments:\n",
    "        detections (Array[N, 6]), x1, y1, x2, y2, conf, class\n",
    "        labels (Array[M, 5]), class, x1, y1, x2, y2\n",
    "    Returns:\n",
    "        correct (Array[N, 10]), for 10 IoU levels\n",
    "    \"\"\"\n",
    "    correct = torch.zeros(detections.shape[0], iouv.shape[0], dtype=torch.bool, device=iouv.device)\n",
    "    iou = box_iou(labels[:, 1:], detections[:, :4])\n",
    "    x = torch.where((iou >= iouv[0]) & (labels[:, 0:1] == detections[:, 5]))  # IoU above threshold and classes match\n",
    "    if x[0].shape[0]:\n",
    "        matches = torch.cat((torch.stack(x, 1), iou[x[0], x[1]][:, None]), 1).cpu().detach().numpy()  # [label, detection, iou]\n",
    "        if x[0].shape[0] > 1:\n",
    "            matches = matches[matches[:, 2].argsort()[::-1]]\n",
    "            matches = matches[np.unique(matches[:, 1], return_index=True)[1]]\n",
    "            # matches = matches[matches[:, 2].argsort()[::-1]]\n",
    "            matches = matches[np.unique(matches[:, 0], return_index=True)[1]]\n",
    "        matches = torch.Tensor(matches).to(iouv.device)\n",
    "        correct[matches[:, 1].long()] = matches[:, 2:3] >= iouv\n",
    "    return correct\n",
    "\n",
    "@torch.no_grad()\n",
    "def valid_epoch(model, dataloader, imgsz=640, iou_thres=0.5, conf_thres=0.01, nc=1, compute_loss=None, half=True):\n",
    "\n",
    "    model.eval()\n",
    "    map = 0\n",
    "    \n",
    "    single_cls = True if nc==1 else False\n",
    "\n",
    "    device, pt, jit, engine = next(model.parameters()).device, True, False, False  # get model device, PyTorch model\n",
    "\n",
    "    half &= device.type != 'cpu'  # half precision only supported on CUDA\n",
    "    model.half() if half else model.float()\n",
    "\n",
    "    iouv = torch.linspace(0.5, 0.95, 10).to(device)  # iou vector for mAP@0.5:0.95\n",
    "    niou = iouv.numel()\n",
    "\n",
    "    seen = 0\n",
    "    confusion_matrix = ConfusionMatrix(nc=nc)\n",
    "    names = {k: v for k, v in enumerate(model.names if hasattr(model, 'names') else model.module.names)}\n",
    "\n",
    "    s = ('%20s' + '%11s' * 6) % ('Class', 'Images', 'Labels', 'P', 'R', 'mAP@.5', 'mAP@.5:.95')\n",
    "    dt, p, r, f1, mp, mr, map50, map = [0.0, 0.0, 0.0], 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "    loss = torch.zeros(3, device=device)\n",
    "    jdict, stats, ap, ap_class = [], [], [], []\n",
    "    pbar = tqdm(dataloader, total=len(dataloader), desc=s, bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}')  # progress bar\n",
    "    for batch_i, (im, targets, paths, shapes) in enumerate(pbar):\n",
    "\n",
    "        im = im.to(device) #, non_blocking=True)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        im = im.half() if half else im.float()\n",
    "        im /= 255\n",
    "        nb, _, height, width = im.shape\n",
    "\n",
    "        # Inference\n",
    "        out, train_out = model(im)\n",
    "        if compute_loss:\n",
    "            loss += compute_loss([x.float() for x in train_out], targets)[1]  # box, obj, cls\n",
    "\n",
    "        # NMS\n",
    "        targets[:, 2:] *= torch.Tensor([width, height, width, height]).to(device)  # to pixels\n",
    "        lb = []  # for autolabelling\n",
    "        out = non_max_suppression(out, conf_thres, iou_thres, labels=lb, multi_label=True, agnostic=single_cls)\n",
    "        \n",
    "        # Metrics\n",
    "        for si, pred in enumerate(out):\n",
    "            labels = targets[targets[:, 0] == si, 1:]\n",
    "            nl = len(labels)\n",
    "            tcls = labels[:, 0].tolist() if nl else []  # target class\n",
    "            shape = shapes[si][0]\n",
    "            seen += 1\n",
    "            \n",
    "            if len(pred) == 0:\n",
    "                if nl:\n",
    "                    stats.append((torch.zeros(0, niou, dtype=torch.bool), torch.Tensor(), torch.Tensor(), tcls))\n",
    "                continue\n",
    "\n",
    "            # Predictions\n",
    "            if single_cls:\n",
    "                pred[:, 5] = 0\n",
    "            predn = pred.clone()\n",
    "            scale_coords(im[si].shape[1:], predn[:, :4], shape, shapes[si][1])  # native-space pred\n",
    "\n",
    "            # Evaluate\n",
    "            if nl:\n",
    "                tbox = xywh2xyxy(labels[:, 1:5])  # target boxes\n",
    "                scale_coords(im[si].shape[1:], tbox, shape, shapes[si][1])  # native-space labels\n",
    "                labelsn = torch.cat((labels[:, 0:1], tbox), 1)  # native-space labels\n",
    "                correct = process_batch(predn, labelsn, iouv)\n",
    "            else:\n",
    "                correct = torch.zeros(pred.shape[0], niou, dtype=torch.bool)\n",
    "            stats.append((correct.cpu(), pred[:, 4].cpu(), pred[:, 5].cpu(), tcls))  # (correct, conf, pcls, tcls)\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Compute metrics\n",
    "    stats = [np.concatenate(x, 0) for x in zip(*stats)]  # to numpy\n",
    "    # print(*stats)\n",
    "    # print(stats)\n",
    "    if len(stats) and stats[0].any():\n",
    "        tp, fp, p, r, f1, ap, ap_class = ap_per_class(*stats, plot=False, save_dir='.', names=names)\n",
    "        ap50, ap = ap[:, 0], ap.mean(1)  # AP@0.5, AP@0.5:0.95\n",
    "        mp, mr, map50, map = p.mean(), r.mean(), ap50.mean(), ap.mean()\n",
    "        nt = np.bincount(stats[3].astype(np.int64), minlength=nc)  # number of targets per class\n",
    "    else:\n",
    "        nt = torch.zeros(1)\n",
    "\n",
    "    maps = np.zeros(nc) + map\n",
    "    for i, c in enumerate(ap_class):\n",
    "        maps[c] = ap[i]\n",
    "\n",
    "    pf = '%20s' + '%11i' * 2 + '%11.3g' * 4  # print format\n",
    "    LOGGER.info(pf % ('all', seen, nt.sum(), mp, mr, map50, map))\n",
    "    return (mp, mr, map50, map, *(loss.cpu() / len(dataloader)).tolist()), maps\n",
    "\n",
    "res = valid_epoch(model, valid_loader, imgsz=640, compute_loss=ComputeLoss(model))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fc11e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_detector_model(\n",
    "    model: torch.nn.Module, \n",
    "    img_size: int,\n",
    "    model_config: dict, \n",
    "    output_file_path: str | Path = 'exported_detector'\n",
    "):\n",
    "    assert isinstance(model_config, dict), 'Cannot save: wrong model config type. Dict is required'\n",
    "    \n",
    "    torch.save({\n",
    "        'model': model.state_dict(),\n",
    "        'input_image_size': img_size,\n",
    "        'model_config': model_config,\n",
    "    }, output_file_path)\n",
    "    print('Model successfuly saved!')\n",
    "    \n",
    "save_detector_model(\n",
    "    model,\n",
    "    IMG_SIZE,\n",
    "    model.yaml\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "2_0_YoloDetection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
