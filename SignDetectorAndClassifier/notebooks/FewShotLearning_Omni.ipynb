{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdc13461",
   "metadata": {},
   "source": [
    "## Цель ноутбука: изучение метода Few Shots Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefeb9ca",
   "metadata": {},
   "source": [
    "#### В RTSD не хватает 14 знаков:\n",
    "\n",
    "| Знак | Описание | Источник |\n",
    "| ------------- | ------------- | ---- |\n",
    "| 1.6 | Пересечение равнозначных дорог | - |\n",
    "| 1.31 | Туннель | - |\n",
    "| 2.4 | Уступите дорогу | GTSRB Recognition |\n",
    "| 3.21 | Конец запрещения обгона | GTSRB Recognition |\n",
    "| 3.22 | Обгон грузовым автомобилям запрещен | GTSRB Recognition |\n",
    "| 3.23 | Конец запрещения обгона грузовым автомобилям | GTSRB Recognition |\n",
    "| 3.24-90 | Огр 90 | - |\n",
    "| 3.24-100 | Огр 100 | GTSRB Recognition |\n",
    "| 3.24-110 | Огр 110 | - |\n",
    "| 3.24-120 | Огр 120 | GTSRB Recognition |\n",
    "| 3.24-130 | Огр 130 | - |\n",
    "| 3.25 | Конец огр. максимальной скорости | GTSRB Recognition |\n",
    "| 3.31 | Конец всех ограничений | GTSRB Recognition |\n",
    "| 6.3.2 | Зона для разворота | - |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5a296e",
   "metadata": {},
   "source": [
    "Инициализация библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b694f2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "if A.__version__ != '1.0.3':\n",
    "    !pip install albumentations==1.0.3\n",
    "    !pip install opencv-python-headless==4.5.2.52\n",
    "    assert False, 'restart runtime pls'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import pathlib\n",
    "import shutil\n",
    "import cv2\n",
    "import PIL\n",
    "import cv2\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "TEXT_COLOR = 'black'\n",
    "# Зафиксируем состояние случайных чисел\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (17,10)\n",
    "\n",
    "IN_COLAB = False\n",
    "USE_COLAB_GPU = False\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    USE_COLAB_GPU = True\n",
    "    from google.colab import drive\n",
    "except:\n",
    "    if IN_COLAB:\n",
    "        print('[!]YOU ARE IN COLAB, BUT DIDNT MOUND A DRIVE. Model wont be synced[!]')\n",
    "\n",
    "        if not os.path.isfile(CURRENT_FILE_NAME):\n",
    "            print(\"FIX ME\")\n",
    "        IN_COLAB = False\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19a8a95",
   "metadata": {},
   "source": [
    "Инициализация основных путей и папки src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7d8136",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not IN_COLAB:\n",
    "    PROJECT_ROOT = pathlib.Path(os.path.join(os.curdir, os.pardir))\n",
    "else:\n",
    "    PROJECT_ROOT = pathlib.Path('..')\n",
    "    \n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "NOTEBOOKS_DIR = PROJECT_ROOT / 'notebooks'\n",
    "\n",
    "SRC_DIR = PROJECT_ROOT / 'src'\n",
    "sys.path.append(SRC_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe33e5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "        \n",
    "model = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2bfc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322120d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "from pytorch_metric_learning import distances, losses, miners, reducers, testers\n",
    "from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eb98e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    ")\n",
    "\n",
    "dataset1 = datasets.MNIST(\".\", train=True, download=True, transform=transform)\n",
    "dataset2 = datasets.MNIST(\".\", train=False, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset1, batch_size=256, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f029a048",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader.num_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e2f955",
   "metadata": {},
   "outputs": [],
   "source": [
    "### convenient function from pytorch-metric-learning ###\n",
    "def get_all_embeddings(dataset, model):\n",
    "    tester = testers.BaseTester(dataloader_num_workers=0)\n",
    "    return tester.get_all_embeddings(dataset, model)\n",
    "\n",
    "\n",
    "### MNIST code originally from https://github.com/pytorch/examples/blob/master/mnist/main.py ###\n",
    "def train(model, loss_func, mining_func, device, train_loader, optimizer, epoch):\n",
    "    print('train was called')\n",
    "    model.train()\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        # print(data)\n",
    "        # print(labels)\n",
    "        # input()\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        embeddings = model(data)\n",
    "        indices_tuple = mining_func(embeddings, labels)\n",
    "        # print(indices_tuple)\n",
    "        loss = loss_func(embeddings, labels, indices_tuple)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 20 == 0:\n",
    "            print(\n",
    "                \"Epoch {} Iteration {}: Loss = {}, Number of mined triplets = {}\".format(\n",
    "                    epoch, batch_idx, loss, mining_func.num_triplets\n",
    "                )\n",
    "            )\n",
    "\n",
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.uniform_(m.weight)\n",
    "        \n",
    "# model.apply(init_normal)\n",
    "\n",
    "### compute accuracy using AccuracyCalculator from pytorch-metric-learning ###\n",
    "def test(train_set, test_set, model, accuracy_calculator):\n",
    "    train_embeddings, train_labels = get_all_embeddings(train_set, model)\n",
    "    test_embeddings, test_labels = get_all_embeddings(test_set, model)\n",
    "    train_labels = train_labels.squeeze(1)\n",
    "    test_labels = test_labels.squeeze(1)\n",
    "    print(\"Computing accuracy\")\n",
    "    accuracies = accuracy_calculator.get_accuracy(\n",
    "        test_embeddings, train_embeddings, test_labels, train_labels, False\n",
    "    )\n",
    "    print(\"Test set accuracy (Precision@1) = {}\".format(accuracies[\"precision_at_1\"]))\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "num_epochs = 1\n",
    "\n",
    "\n",
    "### pytorch-metric-learning stuff ###\n",
    "distance = distances.CosineSimilarity()\n",
    "reducer = reducers.ThresholdReducer(low=0)\n",
    "loss_func = losses.TripletMarginLoss(margin=0.2, distance=distance, reducer=reducer)\n",
    "mining_func = miners.TripletMarginMiner(\n",
    "    margin=0.2, distance=distance, type_of_triplets=\"semihard\"\n",
    ")\n",
    "accuracy_calculator = AccuracyCalculator(include=(\"precision_at_1\",))\n",
    "### pytorch-metric-learning stuff ###\n",
    "\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # train(model, loss_func, mining_func, device, train_loader, optimizer, epoch)\n",
    "    test(dataset1, dataset2, model, accuracy_calculator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c3699d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c68b3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange, tqdm\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from umap import UMAP\n",
    "import plotly.express as px\n",
    "\n",
    "@torch.no_grad()\n",
    "def plotSmth(model, loader, device='cpu', dim3=False, fcn='PCA'):\n",
    "    model.eval()\n",
    "    \n",
    "    fig = plt.figure(figsize=(16, 16))    \n",
    "    # clean the figure\n",
    "    fig.clf()\n",
    "    \n",
    "    n_components = 3 if dim3 else 2\n",
    "    \n",
    "    fcn = fcn.upper()\n",
    "    \n",
    "    if fcn == 'PCA':\n",
    "        reducer = PCA(n_components=n_components, random_state=RANDOM_STATE) \n",
    "    elif fcn == 'TSNE':\n",
    "        reducer = TSNE(n_components=n_components, init='random', random_state=RANDOM_STATE)\n",
    "    elif fcn == 'UMAP':\n",
    "        reducer = UMAP(n_components=n_components, init='random', random_state=RANDOM_STATE)\n",
    "    else:\n",
    "        assert False, \"wrong fcn arg\"\n",
    "            \n",
    "    # colormap=plt.cm.Paired\n",
    "    \n",
    "    pbar = tqdm(enumerate(loader),\n",
    "                    total=len(loader), \n",
    "                    position=0,\n",
    "                    leave=False)\n",
    "\n",
    "    MODEL_ARRAY_OUT_SIZE = (0, 128)\n",
    "    model_out_arr = np.empty(MODEL_ARRAY_OUT_SIZE, dtype=np.float32)\n",
    "    target_arr = np.empty((0, 1), dtype=np.int32)\n",
    "\n",
    "    for idx, (data, target) in pbar:\n",
    "\n",
    "        data = data.to(device)\n",
    "        out = model(data).detach().cpu().numpy()\n",
    "        # print(out.shape)\n",
    "        model_out_arr = np.append(model_out_arr, out, axis=0)\n",
    "        target = target.detach().cpu().numpy()\n",
    "        # print(target)\n",
    "        target_arr = np.append(target_arr, target)\n",
    "\n",
    "        if idx > 1:\n",
    "            break\n",
    "\n",
    "    # print(len(target_arr)) \n",
    "    # print(target_arr)\n",
    "    X_embedded = reducer.fit_transform(model_out_arr)\n",
    "    target_arr = np.char.mod('%d', target_arr)\n",
    "    # print(target_arr)\n",
    "    \n",
    "    if not dim3:\n",
    "        fig = px.scatter(X_embedded, x=0, y=1, color=target_arr)\n",
    "    else:\n",
    "        fig = px.scatter_3d(X_embedded, x=0, y=1, z=2, color=target_arr)    \n",
    "    fig.show()\n",
    "    \n",
    "    \n",
    "test_loader = torch.utils.data.DataLoader(dataset2, batch_size=256)\n",
    "\n",
    "plotSmth(model, test_loader, device=device, dim3=False, fcn='pca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d4fd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d735ee14",
   "metadata": {},
   "outputs": [],
   "source": [
    "enumerate(train_loader)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42c4a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(dataset1[1][0][None, ...].to(device)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf03f67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = get_all_embeddings(dataset1, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515f130e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f132c771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ed2e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad(dataset1[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fe80de",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    This class works under the directory structure of the Omniglot Dataset\n",
    "    It creates the pairs of images for inputs, same character label = 1, vice versa\n",
    "'''\n",
    "class OmniglotDataset(Dataset):\n",
    "    '''\n",
    "        categories is the list of different alphabets (folders)\n",
    "        root_dir is the root directory leading to the alphabet files, could be /images_background or /images_evaluation\n",
    "        setSize is the size of the train set and the validation set combined\n",
    "        transform is any image transformations\n",
    "    '''\n",
    "    def __init__(self, categories, root_dir, setSize, transform=None):\n",
    "        self.categories = categories\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.setSize = setSize\n",
    "    def __len__(self):\n",
    "        return self.setSize\n",
    "    def __getitem__(self, idx):\n",
    "        img1 = None\n",
    "        img2 = None\n",
    "        label = None\n",
    "        if idx % 2 == 0: # select the same character for both images\n",
    "            category = random.choice(categories)\n",
    "            character = random.choice(category[1])\n",
    "            imgDir = root_dir + category[0] + '/' + character\n",
    "            img1Name = random.choice(os.listdir(imgDir))\n",
    "            img2Name = random.choice(os.listdir(imgDir))\n",
    "            img1 = Image.open(imgDir + '/' + img1Name)\n",
    "            img2 = Image.open(imgDir + '/' + img2Name)\n",
    "            label = 1.0\n",
    "        else: # select a different character for both images\n",
    "            category1, category2 = random.choice(categories), random.choice(categories)\n",
    "            category1, category2 = random.choice(categories), random.choice(categories)\n",
    "            character1, character2 = random.choice(category1[1]), random.choice(category2[1])\n",
    "            imgDir1, imgDir2 = root_dir + category1[0] + '/' + character1, root_dir + category2[0] + '/' + character2\n",
    "            img1Name = random.choice(os.listdir(imgDir1))\n",
    "            img2Name = random.choice(os.listdir(imgDir2))\n",
    "            while img1Name == img2Name:\n",
    "                img2Name = random.choice(os.listdir(imgDir2))\n",
    "            label = 0.0\n",
    "            img1 = Image.open(imgDir1 + '/' + img1Name)\n",
    "            img2 = Image.open(imgDir2 + '/' + img2Name)\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "        return img1, img2, torch.from_numpy(np.array([label], dtype=np.float32))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f2d2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOCK_SIGNS_CSV_LOCATION = DATA_DIR / 'STOCK_SIGNS.csv'\n",
    "STOCK_SIGNS_DATAFRAME = pd.read_csv(STOCK_SIGNS_CSV_LOCATION)\n",
    "\n",
    "# Исправляем пешеходов в разные стороны\n",
    "STOCK_SIGNS_DATAFRAME.loc[STOCK_SIGNS_DATAFRAME['SIGN'] == '5.19.2', 'SIGN'] = '5.19.1'\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# кодируем знаки, добавляем еще колонку\n",
    "LE_LOCATION = DATA_DIR / 'STOCK_SIGNS_LE.npy'\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "if os.path.isfile(LE_LOCATION):\n",
    "    le.classes_ = np.load(LE_LOCATION, allow_pickle=True)\n",
    "    print('[+] label encoder was restored')\n",
    "else:\n",
    "    le.fit_transform(STOCK_SIGNS_DATAFRAME['SIGN'])\n",
    "    np.save(LE_LOCATION, le.classes_)\n",
    "    print('[!] label encoder was created')\n",
    "    \n",
    "STOCK_SIGNS_DATAFRAME['ENCODED_LABELS'] = le.transform(STOCK_SIGNS_DATAFRAME['SIGN'])\n",
    "STOCK_SIGNS_DATAFRAME['filepath'] = STOCK_SIGNS_DATAFRAME['filepath'].apply(lambda x: str(x).replace('\\\\', '/'))\n",
    "STOCK_SIGNS_DATAFRAME['filepath'] = STOCK_SIGNS_DATAFRAME['filepath'].apply(lambda x: DATA_DIR / x)\n",
    "STOCK_SIGNS_DATAFRAME[::6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca31f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIGN_LIST = list(STOCK_SIGNS_DATAFRAME['SIGN'])\n",
    "\n",
    "nrows, ncols = 20, 6\n",
    "fig = plt.figure(figsize = (16,50))\n",
    "\n",
    "for idx, row in enumerate(STOCK_SIGNS_DATAFRAME.iterrows()):\n",
    "    instance = row[1]\n",
    "    path = str(instance['filepath'])\n",
    "    sign = instance['SIGN']\n",
    "    encoded_label = instance['ENCODED_LABELS']\n",
    "    \n",
    "    img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "    trans_mask = img[:,:,3] == 0\n",
    "    img[trans_mask] = [random.randrange(0, 256), random.randrange(0, 256), random.randrange(0, 256), 255]\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)\n",
    "    ax = fig.add_subplot(nrows, ncols, idx+1)\n",
    "    \n",
    "    ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), aspect=1)\n",
    "    ax.set_title('ENCODED: ' + str(encoded_label) \n",
    "                 + '\\nSIGN: ' + str(sign)\n",
    "                )\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdccee0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:adas] *",
   "language": "python",
   "name": "conda-env-adas-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
