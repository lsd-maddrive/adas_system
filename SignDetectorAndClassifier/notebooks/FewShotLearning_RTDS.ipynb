{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdc13461",
   "metadata": {
    "id": "fdc13461"
   },
   "source": [
    "## Цель ноутбука: изучение метода Few Shots Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b37903a",
   "metadata": {},
   "source": [
    "### Ниже код, специфичный для запуска на платформе Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EGa0W9FRKyaX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EGa0W9FRKyaX",
    "outputId": "dad803c2-8b80-436d-f236-cbc8149dee76"
   },
   "outputs": [],
   "source": [
    "#!git clone --branch 11_ShotLearning_Encoder https://github.com/lsd-maddrive/adas_system.git\n",
    "#!gdown --id 1-K3ee1NbMmx_0T5uwMesStmKnZO_6mWi\n",
    "#%cd adas_system\n",
    "#!pip install -r requirements.txt\n",
    "#!pip install faiss-cpu faiss\n",
    "#!pip install --upgrade tbb\n",
    "#%cd SignDetectorAndClassifier/notebooks\n",
    "#!unzip -q -o /content/R_MERGED.zip -d ./../data/\n",
    "\n",
    "%cd adas_system/SignDetectorAndClassifier/notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefeb9ca",
   "metadata": {
    "id": "eefeb9ca"
   },
   "source": [
    "#### В RTSD не хватает 14 знаков:\n",
    "\n",
    "| Знак | Описание | Источник |\n",
    "| ------------- | ------------- | ---- |\n",
    "| 1.6 | Пересечение равнозначных дорог | - |\n",
    "| 1.31 | Туннель | - |\n",
    "| 2.4 | Уступите дорогу | GTSRB Recognition |\n",
    "| 3.21 | Конец запрещения обгона | GTSRB Recognition |\n",
    "| 3.22 | Обгон грузовым автомобилям запрещен | GTSRB Recognition |\n",
    "| 3.23 | Конец запрещения обгона грузовым автомобилям | GTSRB Recognition |\n",
    "| 3.24-90 | Огр 90 | - |\n",
    "| 3.24-100 | Огр 100 | GTSRB Recognition |\n",
    "| 3.24-110 | Огр 110 | - |\n",
    "| 3.24-120 | Огр 120 | GTSRB Recognition |\n",
    "| 3.24-130 | Огр 130 | - |\n",
    "| 3.25 | Конец огр. максимальной скорости | GTSRB Recognition |\n",
    "| 3.31 | Конец всех ограничений | GTSRB Recognition |\n",
    "| 6.3.2 | Зона для разворота | - |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5a296e",
   "metadata": {
    "id": "1a5a296e"
   },
   "source": [
    "Инициализация библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b694f2b3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b694f2b3",
    "outputId": "a70cff22-aa1e-404b-a582-99057acc2231"
   },
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "if A.__version__ != '1.0.3':\n",
    "    !pip install albumentations==1.0.3\n",
    "    !pip install opencv-python-headless==4.5.2.52\n",
    "    assert False, 'restart runtime pls'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import pathlib\n",
    "import shutil\n",
    "import cv2\n",
    "import PIL\n",
    "import cv2\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "TEXT_COLOR = 'black'\n",
    "# Зафиксируем состояние случайных чисел\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (17,10)\n",
    "\n",
    "USE_COLAB_GPU = False\n",
    "IN_COLAB = False\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    USE_COLAB_GPU = True\n",
    "    from google.colab import drive\n",
    "except:\n",
    "    if IN_COLAB:\n",
    "        print('[!] YOU ARE IN COLAB, BUT DIDNT MOUND A DRIVE. Model wont be synced[!]')\n",
    "\n",
    "        if not os.path.isfile(CURRENT_FILE_NAME):\n",
    "            print(\"FIX ME\")\n",
    "        IN_COLAB = False\n",
    "\n",
    "    else:\n",
    "        print('[!] RUNNING NOT IN COLAB')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19a8a95",
   "metadata": {
    "id": "b19a8a95"
   },
   "source": [
    "Инициализация основных путей и папки src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7d8136",
   "metadata": {
    "id": "8b7d8136"
   },
   "outputs": [],
   "source": [
    "if not IN_COLAB:\n",
    "    PROJECT_ROOT = pathlib.Path(os.path.join(os.curdir, os.pardir))\n",
    "else:\n",
    "    PROJECT_ROOT = pathlib.Path('..')\n",
    "    \n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "NOTEBOOKS_DIR = PROJECT_ROOT / 'notebooks'\n",
    "SRC_PATH = str(PROJECT_ROOT / 'src')\n",
    "\n",
    "if SRC_PATH not in sys.path:\n",
    "    sys.path.append(SRC_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d99cb4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "f3d99cb4",
    "outputId": "1d1f48a4-4a0b-496b-ee8f-3e4abf275c4a"
   },
   "outputs": [],
   "source": [
    "RTDS_DF = pd.read_csv(DATA_DIR / 'RTDS_DATASET.csv')\n",
    "RTDS_DF['filepath'] = RTDS_DF['filepath'].apply(lambda x: str(DATA_DIR / x))\n",
    "\n",
    "# UNFIX TRAIN\n",
    "# SIMPLE_FIX = True\n",
    "# JUST_FIX = False\n",
    "RTDS_DF.drop_duplicates(subset=['filepath'], inplace=True)\n",
    "# RTDS_DF.drop_duplicates(subset=['SET', 'SIGN'], inplace=True)\n",
    "\n",
    "RTDS_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97019252",
   "metadata": {
    "id": "97019252"
   },
   "outputs": [],
   "source": [
    "# SAMPLE_NUMBER = 13 # min(RTDS_DF.groupby(['SIGN', 'SET']).size())\n",
    "# RTDS_DF = RTDS_DF.groupby(['SIGN', 'SET']).apply(lambda x: x.sample(frac=0.1))# sample(SAMPLE_NUMBER, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "# LEARN_RTDS_DF.groupby(['SIGN']).sample(SAMPLE_NUMBER, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "RTDS_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b46b42e",
   "metadata": {
    "id": "3b46b42e"
   },
   "outputs": [],
   "source": [
    "class SignDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, set_label=None, hyp=None, transform=None, le=None):\n",
    "                \n",
    "        self.transform = transform\n",
    "        \n",
    "        if set_label == None:\n",
    "            self.df = df\n",
    "        else:\n",
    "            self.df = df[df['SET']==set_label]\n",
    "        \n",
    "        self.hyp = hyp\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df.index)\n",
    "    \n",
    "    def __getitem__(self, index): \n",
    "        label = int(self.df.iloc[index]['ENCODED_LABEL'])\n",
    "        path = str(self.df.iloc[index]['filepath'])\n",
    "        sign = str(self.df.iloc[index]['SIGN'])\n",
    "        img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "        \n",
    "        # check does it contains transparent channel \n",
    "        if img.shape[2] == 4:\n",
    "        # randomize transparent\n",
    "            trans_mask = img[:,:,3] == 0\n",
    "            img[trans_mask] = [random.randrange(0, 256), \n",
    "                               random.randrange(0, 256), \n",
    "                               random.randrange(0, 256), \n",
    "                               255]\n",
    "\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)\n",
    "        # /randomize transparent\n",
    "                \n",
    "        # augment \n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        # /augment\n",
    "        \n",
    "        img = img / 255\n",
    "        return img, label, (path, sign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbabf73",
   "metadata": {
    "id": "7bbabf73"
   },
   "outputs": [],
   "source": [
    "from albumentations.augmentations.geometric.transforms import Perspective, ShiftScaleRotate\n",
    "from albumentations.core.transforms_interface import ImageOnlyTransform\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from albumentations.augmentations.transforms import PadIfNeeded\n",
    "from albumentations.augmentations.geometric.resize import LongestMaxSize\n",
    "\n",
    "img_size = 40\n",
    "\n",
    "\n",
    "minimal_transform = A.Compose(\n",
    "        [\n",
    "        LongestMaxSize(img_size),\n",
    "        PadIfNeeded(\n",
    "            img_size, \n",
    "            img_size, \n",
    "            border_mode=cv2.BORDER_CONSTANT, \n",
    "            value=0\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "transform = A.Compose(\n",
    "        [\n",
    "        A.Blur(blur_limit=2),\n",
    "        A.CLAHE(p=0.5),\n",
    "        A.Perspective(scale=(0.01, 0.1), p=0.5), \n",
    "        A.ShiftScaleRotate(shift_limit=0.05,\n",
    "                           scale_limit=0.05,\n",
    "                           interpolation=cv2.INTER_LANCZOS4, \n",
    "                           border_mode=cv2.BORDER_CONSTANT, \n",
    "                           value=(0,0,0),\n",
    "                           rotate_limit=6, p=0.5),\n",
    "        A.RandomGamma(\n",
    "            gamma_limit=(50, 130), \n",
    "            p=1\n",
    "        ),\n",
    "        A.ImageCompression(quality_lower=80, p=0.5),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.5, \n",
    "                                   contrast_limit=0.3, \n",
    "                                   brightness_by_max=False, \n",
    "                                   p=0.5),\n",
    "        A.CoarseDropout(max_height=3, \n",
    "                        max_width=3, \n",
    "                        min_holes=1, \n",
    "                        max_holes=3, \n",
    "                        p=0.5),\n",
    "        LongestMaxSize(img_size),\n",
    "        PadIfNeeded(\n",
    "            img_size, \n",
    "            img_size, \n",
    "            border_mode=cv2.BORDER_CONSTANT, \n",
    "            value=0\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "train_dataset = SignDataset(RTDS_DF, \n",
    "                            set_label='train',  \n",
    "                            transform=transform, \n",
    "                            hyp=None)\n",
    "\n",
    "valid_dataset = SignDataset(RTDS_DF, \n",
    "                            set_label='valid',  \n",
    "                            transform=minimal_transform, \n",
    "                            hyp=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487a7cb7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 842
    },
    "id": "487a7cb7",
    "outputId": "b0ba54df-1fcf-4b69-da9c-f7df8c79bc8d"
   },
   "outputs": [],
   "source": [
    "def getNSamplesFromDataSet(ds, N):\n",
    "    random_index = random.sample(range(0, len(ds)), N)\n",
    "    ret = []\n",
    "    for index in random_index:\n",
    "        ret.append(ds[index])\n",
    "    return ret\n",
    "\n",
    "IMG_COUNT = 18\n",
    "nrows, ncols = 70, 6\n",
    "fig = plt.figure(figsize = (16,200))\n",
    "\n",
    "PLOT_SOFT_LIMIT = 20\n",
    "\n",
    "TEMP_DS = getNSamplesFromDataSet(train_dataset, 20)\n",
    "# TEMP_DS = train_dataset.sort_values(['SIGN'], axis=1)\n",
    "# TEMP_DS = train_dataset\n",
    "for idx, (img, encoded_label, info) in enumerate(TEMP_DS):\n",
    "    \n",
    "    img = torch.Tensor.permute(img, [1, 2, 0]).numpy() \n",
    "    ax = fig.add_subplot(nrows, ncols, idx+1)\n",
    "        \n",
    "    ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), aspect=1)\n",
    "    \n",
    "    title = str(info[1])\n",
    "    \n",
    "    ax.set_title(title, fontsize=15)\n",
    "    \n",
    "    if idx > PLOT_SOFT_LIMIT:\n",
    "        print('[!] plot soft limit reached. Breaking.')\n",
    "        break\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6fc60c",
   "metadata": {
    "id": "8d6fc60c"
   },
   "outputs": [],
   "source": [
    "batch_size = 896 if IN_COLAB else 56\n",
    "num_workers = 2 if IN_COLAB else 0\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def getDataLoaderFromDataset(dataset, shuffle=False, drop_last=True):\n",
    "    \n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last\n",
    "    )\n",
    "    \n",
    "    return loader\n",
    "\n",
    "\n",
    "train_loader = getDataLoaderFromDataset(\n",
    "    train_dataset,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c840de4",
   "metadata": {
    "id": "7c840de4"
   },
   "outputs": [],
   "source": [
    "def saveCheckpoint(model, scheduler, optimizer, epoch, filename):\n",
    "    torch.save({\n",
    "        'epoch': epoch if epoch else None,\n",
    "        'model': model.state_dict() if model else None,\n",
    "        'optimizer': optimizer.state_dict() if optimizer else None,\n",
    "        'scheduler': scheduler.state_dict() if scheduler else None\n",
    "    }, filename)\n",
    "\n",
    "def loadCheckpoint(model, scheduler, optimizer, filename):\n",
    "    checkpoint = torch.load(filename)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    return model, optimizer, scheduler, epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c67bfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet\n",
    "\n",
    "def create_encoder(emb_dim):\n",
    "    model = resnet.resnet18(pretrained=True)\n",
    "    model.fc = nn.Linear(in_features=512, out_features=emb_dim, bias=True)\n",
    "\n",
    "    return model\n",
    "\n",
    "encoder = create_encoder(1024)\n",
    "# encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be43bf4b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 553,
     "referenced_widgets": [
      "884ba4d9b4da4f6ea38b5f42eb9c82f0",
      "6511c5c959084662a03a935f53c3ac80",
      "1f3b07440bb945a9b4f8577f9035bbd4",
      "b85b5a67e6ac421086d910cc2eea8702",
      "14196cf414984cf6ac7deb105157b22d",
      "a7ab6c12864940428613f15d9de71792",
      "02e64db32ce14e4c8e0e87cc6a88ead4",
      "bd85823aa3324d1084c7ad4c9b9f6f75",
      "fd5ac201bdd7423dba2bf4a11359cf46",
      "16082e8a38294a8990d7cc6f8d1e428b",
      "9ce8304ad70243f3803a360ecec4990c",
      "c6fd127ae5074f6aa1a6ac2090d2c78e",
      "d676d9c8a96244e19df84ff8cd23d9d6",
      "088bfdb186944b9cae13f45ee8866cfc",
      "040a332d46b24b7886f47ea8ab1c996b",
      "6229f877414548a4b81bf12e9f867c37",
      "6f903f0791d44040a8e526a4460d7ece",
      "53fb1197e472477a83d2e4f70bb9f0a9",
      "87c2dfa6a1834f1488b5af8aeb58d993",
      "cd3536e6aae7417cab2f845e464b4eaa",
      "627336cc294e495a8c1050388093210f",
      "fa058285a68e42448e1c811e9bb3daaa",
      "748340dd41ad4574867c56e4c209bb16",
      "248a599ad4c04b459fbf3085d4a0e905",
      "59eb169b38d344d696a779eec38d4cfb",
      "16948c743ae744fbad17e86237a850fc",
      "81af14bee21a4d1aac0dbd80e909a950",
      "3c07edeef448427e82bf4b3ce22628ab",
      "706e43c3039341d1976d17ccd0b8051f",
      "17fab1f02c8c40a2a60354e60cbd052b",
      "f6f29f8e4019419094ddebc1a6221e9a",
      "30bfad83fbe04aa5b7f78269de51abbd",
      "7eae6e014dc34438aa01877eac6356dd",
      "002a2513d2b24253933906fadf5fbb0f",
      "12aaa7daa21e42da92df774f66b64171",
      "52da5918eb984231bf7d68af159ab6f0",
      "b21e94d058cb4bf0acf61d0a93112077",
      "bf3b492612c54b4d95d92201aa8477be",
      "1efce39b6d7c427da3204a26016d9a8c",
      "7b6f65bc6f3d410ba01aedf820c89448",
      "429bf5246e4f48a4b14c334cf163d44d",
      "1e7fcec0c1dd4fdc98db126ab77a1fda",
      "d2dfce75702e49d49a3823c2e48d3b05",
      "d0ad7b190f444ed99a00932ba52be8d2",
      "7c7ee49fcde04748a9acdbebb7754d8b",
      "6476ad581a814d4aa2629bcc2fa566c3",
      "f5b771ac175c4b2da4efaefce8a6f60c",
      "2b46a8cb5051498f80e41b67aebe9e02",
      "0be8cc08ee024c968a8e5d4ee8fbd5c5",
      "a34eced2588e41a58b0ee5476d0669b8",
      "3bcf609218e74ee2aade8587795aaa57",
      "802a5c0ac05a4a11ae63cc7b879ed24f",
      "db7f14b7643647329e9efd38fd264e94",
      "b1b682f108c74fe49cc10d9e6456c712",
      "6b039929b95f43a984eacdda4fab6a65",
      "90621fc5f9194802809f4e38112972d8",
      "b11817028fce49a4a77c4bba686cb831",
      "4e0df4759e3248e294679565721d77a0",
      "4127a6d237884b15a116850ca88dac3c",
      "a88dda357e1a4def9e2f6a55281dcb0a",
      "b0fb6b674e624c458a69411be3f0ba0c",
      "c9e63506613548e68f0018cd4972e974",
      "dd6e2a3c43d74a1585c06b1533dd2704",
      "b7bb0d0e85ec481fa313c241c5dbd677",
      "dbb0fa5c50e549e8b909a3edef068e6b",
      "d89d7d66960249fd81c7c46866e2d60f",
      "21cc13c81325458487fac95626512323",
      "53f4d66997d54ceaa1abbcc71534fd10",
      "12fa207f86744963b9aa097ee061a276",
      "319d32aad8b8491fa27ad283db60a1d6",
      "2e68042fcbc14122b122d89c062cfdb1",
      "536b1be6a26b4aab8958346aa47c6cac",
      "460af53089fb4d4f8a0af6db0c4a2c61",
      "4e16bf75d82e4324ba7b3170b4c872b7",
      "7f29ea1787824049ae611331fb4d2e5b",
      "589a08ab3eb04552ac31ae67435e62a2",
      "00563ca6149c4893922585dbbb064237",
      "5a5f5245e486447080ddcc5acb444784",
      "360d4d0130eb436998597d8fd11fd923",
      "be786180681b4d40a49fe842e99f8768",
      "3d06dd79e7e6485095c9118e51eee269",
      "583195950dfb49fba656ae8e7c3e4ceb",
      "f4652bb13a6c4e268a4385b866110386",
      "a36d28cc3c6d41f4b871775400bf574f",
      "c41c63ad3193469e945e53c7d51d9f57",
      "6688ff7dff234b719f1be17cde7a3650",
      "428cd2f2b04a45688140fed0ab3fb356",
      "0653a0f1074c4793be1adb8a38503c09",
      "32381d422e7244918697a46a283aff04",
      "64ae04c1cb9a4c43809f71c7c9f83ac1",
      "8d0ff762e62d46b589d31099b60b51be",
      "d22ec449a99a4d9e972a1795733a67f0",
      "cc18b25a93174c76b255c4b29e825e9b",
      "1f8bdf6e0a374d109cedae1c64fa80af",
      "0d49e92cec85435ead68679637721aed",
      "e5028e9788684a779a40a7874bde6ae7",
      "f95080b2e1d448cdb1dd2496d2ef8309",
      "7689c5d143b04c75a5195ae3b5d8bf6c",
      "47c7b5aaf75441f4a0945e17e580278e",
      "47f7613a625847a1b00a6bc0ff5a2807",
      "68cca4002d994fa297ee098b15bc8b40",
      "7ee611b3b6254572ac73354e7ba85337",
      "4ef5eb52337342c193dad88ab43261c1",
      "d9bb239fd6f345c78f44328613ce7cea",
      "48bdc72412e2427e81fcd6134045700a",
      "9fdcab27f94c41a4bb2c8819c5ea84b3",
      "8b4ea8a2d4274f949e8fca3fa9431173",
      "b5bdf5edff3c4094a367faa28a1bb649",
      "5144060990a546509954527ad3a6cac1",
      "8f1e4034be4847bc82c6d1e64be5b58f",
      "5dd0b990af7b423ebc46db078f95fed4",
      "a2f7a015983f49d282755fb12be2c32e",
      "42c97f71979b412fb54342770d7a1455",
      "dd1453fcbd954d1eac530e8da406f654",
      "306a8ba4b33a4293962961025f602911",
      "c286d57619bb4e618100c339e056c7fa",
      "c2e2faf12ecd4402b3f70fe27ba758fd",
      "75b25015440e49c4b7b1af8eec002444",
      "184e57dc8b9d484f8305d1b12b0cd80f",
      "92ab35da1dfc4e7183f034bfbca394ed",
      "2ba34fac331a41dd8e7373dbd9c53860"
     ]
    },
    "id": "be43bf4b",
    "outputId": "21230855-e730-4c58-809e-3ebfea619d5e"
   },
   "outputs": [],
   "source": [
    "from pytorch_metric_learning import distances, losses, miners, reducers, testers\n",
    "from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator\n",
    "\n",
    "from pytorch_metric_learning.utils import common_functions as c_f\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "config = {\n",
    "    'lr': 0.1,\n",
    "    'epochs': 500,\n",
    "    'momentum':  0.937,\n",
    "    'margin': 0\n",
    "}\n",
    "\n",
    "optimizer = torch.optim.SGD(encoder.parameters(), lr=config['lr'], momentum=config['momentum'], nesterov=True)\n",
    "# encoder.to('cpu')\n",
    "# optimizer = torch.optim.Adam(encoder.parameters(), config['lr'])\n",
    "# scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, \n",
    "#                                              base_lr=0.00001, \n",
    "#                                              max_lr=config['lr'],\n",
    "#                                              step_size_up=50,\n",
    "#                                              step_size_down=20,\n",
    "#                                              mode=\"exp_range\",\n",
    "#                                              gamma=0.9,\n",
    "#                                              cycle_momentum=False\n",
    "#                                            )\n",
    "distance = distances.LpDistance()\n",
    "reducer = reducers.AvgNonZeroReducer()\n",
    "loss_func = losses.TripletMarginLoss(margin=config['margin'], distance=distance, reducer=reducer)\n",
    "\n",
    "# mining_func = miners.MultiSimilarityMiner(epsilon=0.1)\n",
    "mining_func = miners.TripletMarginMiner(margin=config['margin'], distance=distance, type_of_triplets=\"hard\")\n",
    "\n",
    "accuracy_calculator = AccuracyCalculator(k=5)\n",
    "    # include=(\"precision_at_1\",\n",
    "    #          \"mean_average_precision_at_r\"), k=1)\n",
    "\n",
    "try:\n",
    "    # encoder, optimizer, scheduler, started_epoch = loadCheckpoint(encoder, scheduler, optimizer, 'sample')\n",
    "    started_epoch\n",
    "    print('[+] check point loaded')\n",
    "except:\n",
    "    started_epoch = 0\n",
    "    print('[!] check point doesnt exist')\n",
    "\n",
    "encoder.to(device)    \n",
    "\n",
    "\n",
    "### convenient function from pytorch-metric-learning ###\n",
    "@torch.no_grad()\n",
    "def simpleGetAllEmbeddings(model, dataset, batch_size, dsc=''):\n",
    "    \n",
    "    dataloader = getDataLoaderFromDataset(\n",
    "        dataset,\n",
    "        shuffle=True,\n",
    "        drop_last=False\n",
    "    )\n",
    "    \n",
    "    s, e = 0, 0\n",
    "    pbar = tqdm(\n",
    "        enumerate(dataloader), \n",
    "        total=len(dataloader),\n",
    "        position=0,\n",
    "        leave=False,\n",
    "        desc='Getting all embeddings...' + dsc)\n",
    "    info_arr = []\n",
    "    \n",
    "    add_info_len = None\n",
    "    \n",
    "    for idx, (data, labels, info) in pbar:\n",
    "        data = data.to(device)\n",
    "        \n",
    "        q = model(data)\n",
    "        \n",
    "        if labels.dim() == 1:\n",
    "            labels = labels.unsqueeze(1)\n",
    "        if idx == 0:\n",
    "            labels_ret = torch.zeros(\n",
    "                len(dataloader.dataset),\n",
    "                labels.size(1),\n",
    "                device=device,\n",
    "                dtype=labels.dtype,\n",
    "            )\n",
    "            all_q = torch.zeros(\n",
    "                len(dataloader.dataset),\n",
    "                q.size(1),\n",
    "                device=device,\n",
    "                dtype=q.dtype,\n",
    "            )\n",
    "        \n",
    "        info = np.array(info)\n",
    "        if add_info_len == None:\n",
    "            add_info_len = info.shape[0]\n",
    "        \n",
    "        info_arr.extend(info.T.reshape((-1, add_info_len)))\n",
    "        e = s + q.size(0)\n",
    "        all_q[s:e] = q\n",
    "        labels_ret[s:e] = labels\n",
    "        s = e  \n",
    "    \n",
    "    all_q = torch.nn.functional.normalize(all_q)\n",
    "    return all_q, labels_ret, info_arr\n",
    "\n",
    "### compute accuracy using AccuracyCalculator from pytorch-metric-learning ###\n",
    "def test(train_set, test_set, model, accuracy_calculator, batch_size):\n",
    "    model.eval()\n",
    "    train_embeddings, train_labels, _ = simpleGetAllEmbeddings(model, train_set, batch_size, ' for train')\n",
    "    test_embeddings, test_labels, _ = simpleGetAllEmbeddings(model, test_set, batch_size, ' for test')\n",
    "    \n",
    "    train_labels = train_labels.squeeze(1)\n",
    "    test_labels = test_labels.squeeze(1)\n",
    "    # print(\"Computing accuracy\")\n",
    "    accuracies = accuracy_calculator.get_accuracy(\n",
    "        test_embeddings, train_embeddings, test_labels, train_labels, False\n",
    "    )\n",
    "    print(accuracies)\n",
    "    # print(\"Test set accuracy (Precision@1) = {}\".format(accuracies[\"precision_at_1\"]))\n",
    "    return accuracies[\"precision_at_1\"]\n",
    "    \n",
    "    \n",
    "### MNIST code originally from https://github.com/pytorch/examples/blob/master/mnist/main.py ###\n",
    "def train(model, loss_func, mining_func, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    \n",
    "    pbar = tqdm(\n",
    "        enumerate(train_loader), \n",
    "        total=len(train_loader),\n",
    "        position=0,\n",
    "        leave=False,\n",
    "        desc='WAITING...')\n",
    "    \n",
    "    USING_CentroidTripletLoss_FLAG = False\n",
    "    USING_MultiSimilarityMiner_FLAG = False\n",
    "    if isinstance(loss_func, losses.CentroidTripletLoss):\n",
    "        USING_CentroidTripletLoss_FLAG = True\n",
    "    if isinstance(mining_func, miners.MultiSimilarityMiner):\n",
    "        USING_MultiSimilarityMiner_FLAG = True\n",
    "        \n",
    "    for batch_idx, (data, labels, _) in pbar:\n",
    "        \n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        embeddings = model(data)\n",
    "\n",
    "        if USING_CentroidTripletLoss_FLAG:\n",
    "            embeddings = torch.tensor(\n",
    "                [c_f.angle_to_coord(a) for a in embeddings],\n",
    "                requires_grad=True,\n",
    "                dtype=dtype,\n",
    "            ).to(\n",
    "                device\n",
    "            )\n",
    "            print(embeddings.shape)\n",
    "            print(labels.shape)\n",
    "            loss = loss_func(embeddings, labels)\n",
    "        else:\n",
    "            indices_tuple = mining_func(embeddings, labels)\n",
    "            loss = loss_func(embeddings, labels, indices_tuple)\n",
    "\n",
    "        instant_loss = loss.item()\n",
    "        loss_sum += instant_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if USING_CentroidTripletLoss_FLAG or USING_MultiSimilarityMiner_FLAG:\n",
    "            pbar.set_description(\"TRAIN: INSTANT MEAN LOSS %f\" % \n",
    "                             (round(instant_loss / len(labels), 3))\n",
    "                            )            \n",
    "        else:\n",
    "            pbar.set_description(\"TRAIN: INSTANT MEAN LOSS %f, MINED TRIPLET: %d\" % \n",
    "                             (round(instant_loss / len(labels), 3),\n",
    "                             mining_func.num_triplets)\n",
    "                            )\n",
    "        # if batch_idx >= 0:\n",
    "        #     break\n",
    "            \n",
    "    return loss_sum / (train_loader.batch_size * len(train_loader))\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "pbar = trange(\n",
    "        started_epoch, \n",
    "        config['epochs'], \n",
    "        initial=started_epoch, \n",
    "        total=config['epochs'],\n",
    "        leave=True,\n",
    "        desc='WAITING FOR FIRST EPOCH END...')\n",
    "\n",
    "mean_acc = -1\n",
    "\n",
    "for epoch in pbar:\n",
    "    \n",
    "    # plotSmth(encoder, train_dataset, device=device, dim3=False, fcn='umap')\n",
    "    train_loss = train(encoder, loss_func, mining_func, device, train_loader, optimizer, epoch)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        mean_acc = test(train_dataset, valid_dataset, encoder, accuracy_calculator, batch_size)\n",
    "    \n",
    "    # print(lr_val)\n",
    "    # lr_val = scheduler.get_last_lr()[0]\n",
    "    # saveCheckpoint(encoder, scheduler, optimizer, epoch, 'sample')\n",
    "    # plotSmth(encoder, CONST_MINIMAL_DATASET, device=device, dim3=False, fcn='umap')\n",
    "    # scheduler.step()\n",
    "    \n",
    "    mean_train_acc = mean_valid_acc = 0\n",
    "    lr_val = 1\n",
    "    saveCheckpoint(encoder, None, optimizer, epoch, 'last_encoder')\n",
    "    pbar.set_description(\"PER %d EPOCH: TRAIN LOSS: %.1e; VALID ACCUR: %.4f, LR %.1e\" % (\n",
    "        epoch + 1,\n",
    "        train_loss, \n",
    "        mean_acc,\n",
    "        lr_val)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa9dc77",
   "metadata": {
    "id": "3fa9dc77"
   },
   "outputs": [],
   "source": [
    "additional_DF = pd.DataFrame(columns=RTDS_DF.columns)\n",
    "\n",
    "encode_offset = max(set(RTDS_DF['ENCODED_LABEL'])) + 1\n",
    "files = os.listdir(DATA_DIR / 'additional_sign')\n",
    "\n",
    "sign_list = list(set([x.split('_')[0] for x in files]))\n",
    "for file in files:\n",
    "    sign = file.split('_')[0]\n",
    "    # print(file.split('_')[1].split('.')[0])\n",
    "    encoded_label = encode_offset + int(sign_list.index(sign))\n",
    "    \n",
    "    # print(sign)\n",
    "    row = {'filepath': str(DATA_DIR / 'additional_sign' / file), 'SIGN':sign, 'ENCODED_LABEL':encoded_label, 'SET':'valid'} \n",
    "    additional_DF = additional_DF.append(row, ignore_index=True)\n",
    "\n",
    "display(additional_DF)    \n",
    "additional_dataset = SignDataset(\n",
    "    additional_DF,\n",
    "    transform=minimal_transform\n",
    ")\n",
    "\n",
    "add_dataset_dict = dict(zip(additional_DF.ENCODED_LABEL, additional_DF.SIGN))\n",
    "\n",
    "IMG_COUNT = 18\n",
    "nrows, ncols = 70, 6\n",
    "fig = plt.figure(figsize = (16,200))\n",
    "\n",
    "PLOT_SOFT_LIMIT = 20\n",
    "\n",
    "for idx, (img, encoded_label, info) in enumerate(additional_dataset):\n",
    "    \n",
    "    img = torch.Tensor.permute(img, [1, 2, 0]).numpy() \n",
    "    ax = fig.add_subplot(nrows, ncols, idx+1)\n",
    "        \n",
    "    ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), aspect=1)\n",
    "    \n",
    "    title = str(info[1])\n",
    "    \n",
    "    ax.set_title(title, fontsize=15)\n",
    "    \n",
    "    if idx > PLOT_SOFT_LIMIT:\n",
    "        print('[!] plot soft limit reached. Breaking.')\n",
    "        break\n",
    "plt.tight_layout()\n",
    "\n",
    "label_dict = dict(zip(RTDS_DF.ENCODED_LABEL, RTDS_DF.SIGN))\n",
    "label_dict.update(add_dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ed7216",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def getInfoForFig(\n",
    "    model, \n",
    "    dataset, \n",
    "    batch_size, \n",
    "    additional_dataset,\n",
    "    main_dataset_marker_size=10,\n",
    "    additional_dataset_marker_size=20,\n",
    "    dot_limit=1000):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    if len(dataset) > dot_limit:\n",
    "        print(\"[!] Dot limit! Random choice\", dot_limit, '\\nSrc len', len(dataset))\n",
    "        indicies = np.random.choice(len(dataset), dot_limit, replace=False)\n",
    "        dataset = torch.utils.data.Subset(dataset, indicies)\n",
    "        \n",
    "    embeddings, labels, info = simpleGetAllEmbeddings(model, dataset, batch_size, dsc='for main dataset')\n",
    "    embeddings = embeddings.cpu().numpy()\n",
    "    labels = labels.cpu().numpy().flatten()[:, None]\n",
    "    size = np.ones(labels.shape) * main_dataset_marker_size\n",
    "    \n",
    "    if additional_dataset:\n",
    "        embeddings_addon, labels_addon, info_addon = simpleGetAllEmbeddings(\n",
    "            model, \n",
    "            additional_dataset, \n",
    "            batch_size, \n",
    "            dsc='for addon')\n",
    "        \n",
    "        embeddings_addon = embeddings_addon.cpu().numpy()\n",
    "        labels_addon = labels_addon.cpu().numpy().flatten()[:, None]\n",
    "        \n",
    "        size_addon = np.ones(labels_addon.shape) * additional_dataset_marker_size\n",
    "        \n",
    "        size = np.concatenate((size, size_addon))\n",
    "        embeddings = np.concatenate((embeddings, embeddings_addon))\n",
    "        labels = np.concatenate((labels, labels_addon))\n",
    "        info.extend(info_addon)\n",
    "        del embeddings_addon, labels_addon, size_addon, info_addon\n",
    "        \n",
    "    return embeddings, labels, info, size\n",
    "\n",
    "embeddings, labels, info, size = getInfoForFig(\n",
    "    encoder,\n",
    "    train_dataset,\n",
    "    batch_size,\n",
    "    additional_dataset,\n",
    "    dot_limit=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf3f64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from itertools import cycle\n",
    "\n",
    "def getFigForModelAndDataset(\n",
    "    embeddings,\n",
    "    labels,\n",
    "    info,\n",
    "    size, \n",
    "    reducer, \n",
    "    dsc='', \n",
    "    label_dict=None,\n",
    "    FORCE_USE_WO_FIT=False):\n",
    "        \n",
    "    palette = cycle(\n",
    "        [*px.colors.qualitative.Dark24, \n",
    "         *px.colors.qualitative.Alphabet, \n",
    "         *px.colors.qualitative.Light24]\n",
    "    )\n",
    "    \n",
    "    dim3 = True if reducer.n_components==3 else False\n",
    "   \n",
    "    if FORCE_USE_WO_FIT:\n",
    "        X_embedded = reducer.transform(embeddings)\n",
    "    else:\n",
    "        X_embedded = reducer.fit_transform(embeddings)\n",
    "\n",
    "    if label_dict:\n",
    "        try:\n",
    "            group = [label_dict[int(x)] for x in labels][:, None]\n",
    "        except Exception as e:\n",
    "            print('label dict broken', e)\n",
    "            group = labels\n",
    "    else:\n",
    "        group = labels\n",
    "        \n",
    "    group = np.array(group)\n",
    "    # print(len(info))\n",
    "    hover_data = np.array([x[1] + ':' + x[0] for x in info])[:, None]\n",
    "    \n",
    "    # now embeedings, labels, info, size are concatenated. Let's build dataframe from it\n",
    "    # print(X_embedded.shape)\n",
    "    # print(group.shape)\n",
    "    # print(size.shape)\n",
    "    # print(hover_data.shape)\n",
    "    \n",
    "    plot_df_data = np.concatenate([X_embedded, group, size, hover_data], axis=1)\n",
    "    if dim3:\n",
    "        columns = columns=['x', 'y', 'z', 'group', 'size', 'hover_data']\n",
    "    else:\n",
    "        columns = columns=['x', 'y', 'group', 'size', 'hover_data']\n",
    "        \n",
    "    plot_df = pd.DataFrame(plot_df_data, columns=columns)\n",
    "    plot_df['size'] = plot_df['size'].apply(pd.to_numeric)\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    groups = plot_df['group'].unique()\n",
    "    groups.sort()\n",
    "    main_dataset_marker_size = min(plot_df['size'])\n",
    "    \n",
    "    if dim3:\n",
    "        plot_df[['x', 'y', 'z']] = plot_df[['x', 'y', 'z']].apply(pd.to_numeric)\n",
    "        for group in groups: \n",
    "            df = plot_df.loc[plot_df['group'] == group]\n",
    "            group_size = df['size'].iloc[0]\n",
    "            symbol = 'circle' if group_size == main_dataset_marker_size else 'diamond'\n",
    "            line_width = 1 if group_size == main_dataset_marker_size else 2\n",
    "            marker_color=next(palette)\n",
    "            \n",
    "            fig.add_trace(go.Scatter3d(\n",
    "                x=df['x'],\n",
    "                y=df['y'],\n",
    "                z=df['z'],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=df['size'],\n",
    "                    opacity=1,\n",
    "                    symbol=symbol,\n",
    "                    line=dict(\n",
    "                        color='black',\n",
    "                        width=line_width,\n",
    "                    ),\n",
    "                ),\n",
    "                opacity=1,\n",
    "                text=df['hover_data'],\n",
    "                name=group,\n",
    "                marker_color=marker_color\n",
    "            ))\n",
    "    else:\n",
    "        plot_df[['x', 'y']] = plot_df[['x', 'y']].apply(pd.to_numeric)\n",
    "        for group in groups: \n",
    "            df = plot_df.loc[plot_df['group'] == group]\n",
    "            group_size = df['size'].iloc[0]\n",
    "            symbol = 'circle' if group_size == main_dataset_marker_size else 'diamond'\n",
    "            line_width = 1 if group_size == main_dataset_marker_size else 2\n",
    "            marker_color=next(palette)\n",
    "            \n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=df['x'],\n",
    "                y=df['y'],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=df['size'],\n",
    "                    opacity=1,\n",
    "                    symbol=symbol,\n",
    "                    line=dict(\n",
    "                        color='black',\n",
    "                        width=line_width\n",
    "                    ),\n",
    "                    \n",
    "                ),\n",
    "                text=df['hover_data'],\n",
    "                name=group,\n",
    "                marker_color=marker_color\n",
    "            ))\n",
    "            \n",
    "    return fig, reducer, plot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889d6465",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "d2ad1f3419c2435c9088051b15535029",
      "d6a207b8736841078c5dadd4909bb59c",
      "cbdaf90a52774138b8e2759375681c7f",
      "10fb1238586742d696c7355e86ded8a9",
      "32088b7903ca43cebc7ce23a5fb6eeb8",
      "79eed37ad67046e7aa6365e60a424b8b",
      "bd22ec92741043b9b52100d00ab5beac",
      "39a976f2aed346a89792dcde0c06f4aa",
      "bf38156d170a4ce680fbc551f21fc39b",
      "4b79ae865dfe4b3089a270d0d7f7e2d6",
      "d8ccd8539ce1447485df6daf7f007bae"
     ]
    },
    "id": "889d6465",
    "outputId": "cb26c76e-5b97-48d2-8922-3cbb9d42e7ac"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from umap import UMAP\n",
    "\n",
    "dim3=True\n",
    "\n",
    "reducer = PCA(n_components=3 if dim3 else 2, \n",
    "               # init='random', \n",
    "               random_state=RANDOM_STATE)\n",
    "\n",
    "fig, reducer, plot_df = getFigForModelAndDataset(\n",
    "    embeddings, \n",
    "    labels, \n",
    "    info, \n",
    "    size,\n",
    "    reducer=reducer,\n",
    "    label_dict=label_dict\n",
    ")\n",
    "\n",
    "fig.update_traces(\n",
    "    hoverinfo=\"none\", \n",
    "    hovertemplate=None,\n",
    ")\n",
    "fig.update_layout(\n",
    "    width=900,\n",
    "    height=800,\n",
    ")\n",
    "\n",
    "from jupyter_dash import JupyterDash\n",
    "from dash import dcc, html, Input, Output, no_update\n",
    "import base64\n",
    "\n",
    "def b64_image(image_filename):\n",
    "    with open(image_filename, 'rb') as f:\n",
    "        image = f.read()\n",
    "    return 'data:image/png;base64,' + base64.b64encode(image).decode('utf-8')\n",
    "\n",
    "app = JupyterDash(__name__, assets_folder='sdd')\n",
    "@app.callback(\n",
    "    Output(\"graph-tooltip-5\", \"show\"),\n",
    "    Output(\"graph-tooltip-5\", \"bbox\"),\n",
    "    Output(\"graph-tooltip-5\", \"children\"),\n",
    "    Input(\"graph-5\", \"hoverData\"),\n",
    ")\n",
    "def display_hover(hoverData):\n",
    "    if hoverData is None:\n",
    "        return False, no_update, no_update\n",
    "\n",
    "    hover_data = hoverData[\"points\"][0]\n",
    "    bbox = hover_data[\"bbox\"]\n",
    "    num = hover_data[\"pointNumber\"]\n",
    "    sign = hover_data['text'].split(':')[0]\n",
    "    rel_img_path = hover_data['text'].split(':')[1]\n",
    "    b64sed_image = b64_image(rel_img_path)\n",
    "\n",
    "    children = [\n",
    "        html.Div([\n",
    "            html.Img(\n",
    "                src=b64sed_image,\n",
    "                style={\"width\": \"70px\", 'display': 'block', 'margin': '0 auto'},\n",
    "            ),\n",
    "            html.P(sign, style={\"fontSize\": 14, 'text-align':'center'}),\n",
    "            html.P(rel_img_path, style={\"fontSize\": 10}),\n",
    "        ])\n",
    "    ]\n",
    "    return True, bbox, children\n",
    "\n",
    "app.layout = html.Div(\n",
    "        className=\"container\",\n",
    "        children=[\n",
    "            dcc.Graph(id=\"graph-5\", figure=fig, clear_on_unhover=True),\n",
    "            dcc.Tooltip(id=\"graph-tooltip-5\", direction='bottom'),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "app.run_server(mode='inline', debug=True, port=2002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef5a644",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings, labels, info = data[1:]\n",
    "reducer = UMAP(n_components=n_components, init='random', metric=\"cosine\", random_state=RANDOM_STATE)\n",
    "X_embedded = reducer.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f685949",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2f4d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = ['2.1', '5.5', '3.24.40', '2.5', '3.20', '4.1.1', '5.19.1', '3.24.30', '1.33',\n",
    " '3.24.50', '5.16', '3.24.20', '1.22', '3.18']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cdb4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.sort()\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c97e3ca",
   "metadata": {
    "id": "4c97e3ca"
   },
   "outputs": [],
   "source": [
    "additional_DF = pd.DataFrame(columns=RTDS_DF.columns)\n",
    "# display(additional_DF)\n",
    "encode_offset = max(set(RTDS_DF['ENCODED_LABEL'])) + 1\n",
    "\n",
    "files = os.listdir(DATA_DIR / 'additional_sign')\n",
    "\n",
    "for file in files:\n",
    "    sign = file.split('_')[0]\n",
    "    encoded_label = encode_offset\n",
    "    # print(sign)\n",
    "    row = {'filepath': str(DATA_DIR / 'additional_sign' / file), 'SIGN':sign, 'ENCODED_LABEL':encoded_label, 'SET':'valid'} \n",
    "    additional_DF = additional_DF.append(row, ignore_index=True)\n",
    "display(additional_DF)\n",
    "\n",
    "additional_dataset = SignDataset(\n",
    "    additional_DF,\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473d6261",
   "metadata": {
    "id": "473d6261"
   },
   "outputs": [],
   "source": [
    "nrows, ncols = 70, 6\n",
    "fig = plt.figure(figsize = (16,200))\n",
    "\n",
    "PLOT_SOFT_LIMIT = 80\n",
    "\n",
    "TEMP_DS = additional_dataset\n",
    "for idx, (img, encoded_label, info) in enumerate(TEMP_DS):\n",
    "    \n",
    "    img = torch.Tensor.permute(img, [1, 2, 0]).numpy() \n",
    "    ax = fig.add_subplot(nrows, ncols, idx+1)\n",
    "        \n",
    "    ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), aspect=1)\n",
    "    \n",
    "    title = str(info[1])\n",
    "    \n",
    "    ax.set_title(title, fontsize=15)\n",
    "    \n",
    "    if idx > PLOT_SOFT_LIMIT:\n",
    "        print('[!] plot soft limit reached. Breaking.')\n",
    "        break\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FewShotLearning_RTDS.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:adas] *",
   "language": "python",
   "name": "conda-env-adas-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
