{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdc13461",
   "metadata": {},
   "source": [
    "## Цель ноутбука: изучение метода Few Shots Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefeb9ca",
   "metadata": {},
   "source": [
    "#### В RTSD не хватает 14 знаков:\n",
    "\n",
    "| Знак | Описание | Источник |\n",
    "| ------------- | ------------- | ---- |\n",
    "| 1.6 | Пересечение равнозначных дорог | - |\n",
    "| 1.31 | Туннель | - |\n",
    "| 2.4 | Уступите дорогу | GTSRB Recognition |\n",
    "| 3.21 | Конец запрещения обгона | GTSRB Recognition |\n",
    "| 3.22 | Обгон грузовым автомобилям запрещен | GTSRB Recognition |\n",
    "| 3.23 | Конец запрещения обгона грузовым автомобилям | GTSRB Recognition |\n",
    "| 3.24-90 | Огр 90 | - |\n",
    "| 3.24-100 | Огр 100 | GTSRB Recognition |\n",
    "| 3.24-110 | Огр 110 | - |\n",
    "| 3.24-120 | Огр 120 | GTSRB Recognition |\n",
    "| 3.24-130 | Огр 130 | - |\n",
    "| 3.25 | Конец огр. максимальной скорости | GTSRB Recognition |\n",
    "| 3.31 | Конец всех ограничений | GTSRB Recognition |\n",
    "| 6.3.2 | Зона для разворота | - |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5a296e",
   "metadata": {},
   "source": [
    "Инициализация библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b694f2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "if A.__version__ != '1.0.3':\n",
    "    !pip install albumentations==1.0.3\n",
    "    !pip install opencv-python-headless==4.5.2.52\n",
    "    assert False, 'restart runtime pls'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import pathlib\n",
    "import shutil\n",
    "import cv2\n",
    "import PIL\n",
    "import cv2\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "TEXT_COLOR = 'black'\n",
    "# Зафиксируем состояние случайных чисел\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (17,10)\n",
    "\n",
    "IN_COLAB = False\n",
    "USE_COLAB_GPU = False\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    USE_COLAB_GPU = True\n",
    "    from google.colab import drive\n",
    "except:\n",
    "    if IN_COLAB:\n",
    "        print('[!]YOU ARE IN COLAB, BUT DIDNT MOUND A DRIVE. Model wont be synced[!]')\n",
    "\n",
    "        if not os.path.isfile(CURRENT_FILE_NAME):\n",
    "            print(\"FIX ME\")\n",
    "        IN_COLAB = False\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19a8a95",
   "metadata": {},
   "source": [
    "Инициализация основных путей и папки src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7d8136",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not IN_COLAB:\n",
    "    PROJECT_ROOT = pathlib.Path(os.path.join(os.curdir, os.pardir))\n",
    "else:\n",
    "    PROJECT_ROOT = pathlib.Path('..')\n",
    "    \n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "NOTEBOOKS_DIR = PROJECT_ROOT / 'notebooks'\n",
    "SRC_PATH = str(PROJECT_ROOT / 'src')\n",
    "\n",
    "if SRC_PATH not in sys.path:\n",
    "    sys.path.append(SRC_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe33e5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from torchvision.models import resnet\n",
    "\n",
    "class SplitBatchNorm(torch.nn.BatchNorm2d):\n",
    "    def __init__(self, num_features, num_splits, **kw):\n",
    "        super().__init__(num_features, **kw)\n",
    "        self.num_splits = num_splits\n",
    "\n",
    "    def forward(self, input):\n",
    "        N, C, H, W = input.shape\n",
    "        if self.training or not self.track_running_stats:\n",
    "            running_mean_split = self.running_mean.repeat(self.num_splits)\n",
    "            running_var_split = self.running_var.repeat(self.num_splits)\n",
    "            outcome = torch.nn.functional.batch_norm(\n",
    "                input.view(-1, C * self.num_splits, H, W),\n",
    "                running_mean_split,\n",
    "                running_var_split,\n",
    "                self.weight.repeat(self.num_splits),\n",
    "                self.bias.repeat(self.num_splits),\n",
    "                True,\n",
    "                self.momentum,\n",
    "                self.eps,\n",
    "            ).view(N, C, H, W)\n",
    "            self.running_mean.data.copy_(\n",
    "                running_mean_split.view(self.num_splits, C).mean(dim=0)\n",
    "            )\n",
    "            self.running_var.data.copy_(\n",
    "                running_var_split.view(self.num_splits, C).mean(dim=0)\n",
    "            )\n",
    "            return outcome\n",
    "        else:\n",
    "            return torch.nn.functional.batch_norm(\n",
    "                input,\n",
    "                self.running_mean,\n",
    "                self.running_var,\n",
    "                self.weight,\n",
    "                self.bias,\n",
    "                False,\n",
    "                self.momentum,\n",
    "                self.eps,\n",
    "            )\n",
    "        \n",
    "class ModelBase(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Common CIFAR ResNet recipe.\n",
    "    Comparing with ImageNet ResNet recipe, it:\n",
    "    (i) replaces conv1 with kernel=3, str=1\n",
    "    (ii) removes pool1\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_dim=128, arch=\"resnet18\", bn_splits=8):\n",
    "        super(ModelBase, self).__init__()\n",
    "\n",
    "        # use split batchnorm\n",
    "        norm_layer = (\n",
    "            partial(SplitBatchNorm, num_splits=bn_splits)\n",
    "            if bn_splits > 1\n",
    "            else torch.nn.BatchNorm2d\n",
    "        )\n",
    "        # print(norm_layer)\n",
    "        resnet_arch = getattr(resnet, arch)\n",
    "        # print(resnet_arch)\n",
    "        net = resnet_arch(num_classes=feature_dim, norm_layer=norm_layer)\n",
    "\n",
    "        self.net = []\n",
    "        for name, module in net.named_children():\n",
    "            print(name)\n",
    "            if name == \"conv1\":\n",
    "                module = torch.nn.Conv2d(\n",
    "                    3, 64, kernel_size=3, stride=1, padding=1, bias=False\n",
    "                )\n",
    "            if isinstance(module, torch.nn.MaxPool2d):\n",
    "                continue\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                self.net.append(torch.nn.Flatten(1))\n",
    "            self.net.append(module)\n",
    "\n",
    "        self.net = torch.nn.Sequential(*self.net)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        # note: not normalized here\n",
    "        return x\n",
    "\n",
    "def create_encoder(emb_dim):\n",
    "    model = ModelBase(emb_dim)\n",
    "    # model = torch.nn.DataParallel(model)\n",
    "    # model.to(device)\n",
    "    return model\n",
    "\n",
    "encoder = create_encoder(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972c5493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d99cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "RTDS_DF = pd.read_csv(DATA_DIR / 'RTDS_DATASET.csv')\n",
    "RTDS_DF['filepath'] = RTDS_DF['filepath'].apply(lambda x: str(DATA_DIR / x))\n",
    "\n",
    "SIGN_TO_NUMBER = pd.read_csv(DATA_DIR / 'sign_to_number.csv', index_col=0).T.to_dict('records')[0]\n",
    "NUMBER_TO_SIGN = pd.read_csv(DATA_DIR / 'number_to_sign.csv', index_col=0).T.to_dict('records')[0]\n",
    "\n",
    "RTDS_DF['ENCODED_LABELS'] = RTDS_DF['SIGN']\n",
    "RTDS_DF['SIGN'] = RTDS_DF['SIGN'].apply(lambda x: str(NUMBER_TO_SIGN[x]).replace('_', '.').replace('n', ''))\n",
    "\n",
    "# UNFIX TRAIN\n",
    "# SIMPLE_FIX = True\n",
    "# JUST_FIX = False\n",
    "RTDS_DF.drop_duplicates(subset=['filepath'], inplace=True)\n",
    "# RTDS_DF.drop_duplicates(subset=['SET', 'SIGN'], inplace=True)\n",
    "\n",
    "RTDS_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012f4d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "min(RTDS_DF.groupby(['SIGN', 'SET']).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a073b129",
   "metadata": {},
   "outputs": [],
   "source": [
    "RTDS_DF = RTDS_DF.groupby(['SIGN', 'SET']).sample(13, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "RTDS_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64400440",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(RTDS_DF['SIGN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b46b42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, set_label=None, hyp=None, transform=None, le=None):\n",
    "                \n",
    "        self.transform = transform\n",
    "        \n",
    "        if set_label == None:\n",
    "            self.df = df\n",
    "        else:\n",
    "            self.df = df[df['SET']==set_label]\n",
    "        \n",
    "        self.hyp = hyp\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df.index)\n",
    "    \n",
    "    def __getitem__(self, index): \n",
    "        label = int(self.df.iloc[index]['ENCODED_LABELS'])\n",
    "        path = str(self.df.iloc[index]['filepath'])\n",
    "        \n",
    "        img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "        \n",
    "        # check does it contains transparent channel \n",
    "        if img.shape[2] == 4:\n",
    "        # randomize transparent\n",
    "            trans_mask = img[:,:,3] == 0\n",
    "            img[trans_mask] = [random.randrange(0, 256), \n",
    "                               random.randrange(0, 256), \n",
    "                               random.randrange(0, 256), \n",
    "                               255]\n",
    "\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)\n",
    "        # /randomize transparent\n",
    "                \n",
    "        # augment\n",
    "        if self.hyp and self.transform:\n",
    "            img, _ =  random_perspective(img, \n",
    "                                      (),\n",
    "                                      degrees=self.hyp['degrees'],\n",
    "                                      translate=self.hyp['translate'],\n",
    "                                      scale=self.hyp['scale'],\n",
    "                                      shear=self.hyp['shear'],\n",
    "                                      perspective=self.hyp['perspective'],\n",
    "                                      border=self.hyp['border'])   \n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        # /augment\n",
    "        img = img / 255\n",
    "        return img, label #, path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbabf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations.augmentations.geometric.transforms import Perspective, ShiftScaleRotate\n",
    "from albumentations.core.transforms_interface import ImageOnlyTransform\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from albumentations.augmentations.transforms import PadIfNeeded\n",
    "from albumentations.augmentations.geometric.resize import LongestMaxSize\n",
    "\n",
    "# import torchvision.transforms.functional as F\n",
    "\n",
    "img_size = 40\n",
    "\n",
    "MINIMAL_TRANSFORM = True\n",
    "\n",
    "if MINIMAL_TRANSFORM:\n",
    "    transform = A.Compose(\n",
    "        [\n",
    "        LongestMaxSize(img_size),\n",
    "        PadIfNeeded(\n",
    "            img_size, \n",
    "            img_size, \n",
    "            border_mode=cv2.BORDER_CONSTANT, \n",
    "            value=0\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    "else:\n",
    "    transform = A.Compose(\n",
    "        [\n",
    "        A.Blur(blur_limit=2),\n",
    "        A.CLAHE(p=1),\n",
    "        A.Perspective(scale=(0.01, 0.1), p=1), \n",
    "        A.ShiftScaleRotate(shift_limit=0.05,\n",
    "                           scale_limit=0.05,\n",
    "                           interpolation=cv2.INTER_LANCZOS4, \n",
    "                           border_mode=cv2.BORDER_CONSTANT, \n",
    "                           value=(0,0,0),\n",
    "                           rotate_limit=6, p=1),\n",
    "        A.RandomGamma(\n",
    "            gamma_limit=(50, 130), \n",
    "            p=1\n",
    "        ),\n",
    "        A.ImageCompression(quality_lower=80, p=1),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.5, \n",
    "                                   contrast_limit=0.3, \n",
    "                                   brightness_by_max=False, \n",
    "                                   p=1),\n",
    "        A.CoarseDropout(max_height=3, \n",
    "                        max_width=3, \n",
    "                        min_holes=1, \n",
    "                        max_holes=3, \n",
    "                        p=0.8),\n",
    "        LongestMaxSize(img_size),\n",
    "        PadIfNeeded(\n",
    "            img_size, \n",
    "            img_size, \n",
    "            border_mode=cv2.BORDER_CONSTANT, \n",
    "            value=0\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "train_dataset = SignDataset(RTDS_DF, \n",
    "                            set_label='train',  \n",
    "                            transform=transform, \n",
    "                            hyp=None)\n",
    "\n",
    "valid_dataset = SignDataset(RTDS_DF, \n",
    "                            set_label='valid',  \n",
    "                            transform=transform, \n",
    "                            hyp=None)\n",
    "# valid_dataset = train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487a7cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNSamplesFromDataSet(ds, N):\n",
    "    random_index = random.sample(range(0, len(ds)), N)\n",
    "    ret = []\n",
    "    for index in random_index:\n",
    "        ret.append(ds[index])\n",
    "    return ret\n",
    "\n",
    "IMG_COUNT = 18\n",
    "nrows, ncols = 70, 6\n",
    "fig = plt.figure(figsize = (16,200))\n",
    "\n",
    "PLOT_SOFT_LIMIT = 20\n",
    "\n",
    "# TEMP_DS = getNSamplesFromDataSet(train_dataset, len(train_dataset))\n",
    "# TEMP_DS = train_dataset.sort_values(['SIGN'], axis=1)\n",
    "TEMP_DS = train_dataset\n",
    "for idx, (img, encoded_label) in enumerate(TEMP_DS):\n",
    "    # print(img.shape)\n",
    "    img = torch.Tensor.permute(img, [1, 2, 0]).numpy() \n",
    "    ax = fig.add_subplot(nrows, ncols, idx+1)\n",
    "        \n",
    "    ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), aspect=1)\n",
    "    \n",
    "    title = str(NUMBER_TO_SIGN[encoded_label]) + ':' + str(encoded_label)\n",
    "    # if idx % 2 == 1:\n",
    "    #     title += \"\\n\" + _\n",
    "    \n",
    "    ax.set_title(title, fontsize=15)\n",
    "    \n",
    "    if idx > PLOT_SOFT_LIMIT:\n",
    "        print('[!] plot soft limit reached. Breaking.')\n",
    "        break\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6fc60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 40\n",
    "num_workers = 2 if IN_COLAB else 0\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def getDataLoaderFromDataset(dataset, shuffle=False, drop_last=True):\n",
    "    \n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last\n",
    "    )\n",
    "    \n",
    "    return loader\n",
    "\n",
    "\n",
    "train_loader = getDataLoaderFromDataset(\n",
    "    train_dataset,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f756782",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(encoder.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f17e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img, sample_label = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a3f36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sample_img)\n",
    "with torch.no_grad():\n",
    "    encoder.eval()\n",
    "    out = encoder(sample_img).detach().cpu()\n",
    "    \n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9824041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(encoder.state_dict(), 'last_encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c840de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveCheckpoint(model, scheduler, optimizer, epoch, filename):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'scheduler': scheduler.state_dict()\n",
    "    }, filename)\n",
    "\n",
    "def loadCheckpoint(model, scheduler, optimizer, filename):\n",
    "    checkpoint = torch.load(filename)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    return model, optimizer, scheduler, epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876f1cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder, optimizer, scheduler, started_epoch = loadCheckpoint(encoder, scheduler, optimizer, 'sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9872b253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saveCheckpoint(encoder, scheduler, optimizer, 0, 'sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be43bf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_metric_learning import distances, losses, miners, reducers, testers\n",
    "from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "config = {\n",
    "    'lr': 0.1,\n",
    "    'epochs': 120,\n",
    "    'momentum':  0.937,\n",
    "    'margin': 0.0\n",
    "}\n",
    "\n",
    "optimizer = torch.optim.SGD(encoder.parameters(), lr=config['lr'], momentum=config['momentum'], nesterov=True)\n",
    "# encoder.to('cpu')\n",
    "# optimizer = torch.optim.Adam(encoder.parameters(), lr=0.1)\n",
    "# scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, \n",
    "#                                              base_lr=0.00001, \n",
    "#                                              max_lr=config['lr'],\n",
    "#                                              step_size_up=50,\n",
    "#                                              step_size_down=20,\n",
    "#                                              mode=\"exp_range\",\n",
    "#                                              gamma=0.9,\n",
    "#                                              cycle_momentum=False\n",
    "#                                            )\n",
    "distance = distances.CosineSimilarity()\n",
    "reducer = reducers.AvgNonZeroReducer()\n",
    "loss_func = losses.TripletMarginLoss(margin=config['margin'], distance=distance, reducer=reducer)\n",
    "mining_func = miners.TripletMarginMiner(\n",
    "    margin=config['margin'], distance=distance, type_of_triplets=\"all\"\n",
    ")\n",
    "\n",
    "accuracy_calculator = AccuracyCalculator(include=(\"precision_at_1\",), k=1)\n",
    "\n",
    "try:\n",
    "    # encoder, optimizer, scheduler, started_epoch = loadCheckpoint(encoder, scheduler, optimizer, 'sample')\n",
    "    started_epoch\n",
    "    print('[+] check point loaded')\n",
    "except:\n",
    "    started_epoch = 0\n",
    "    print('[!] check point doesnt exist')\n",
    "\n",
    "encoder.to(device)    \n",
    "# assert False, '+'\n",
    "### convenient function from pytorch-metric-learning ###\n",
    "def get_all_embeddings(dataset, model):\n",
    "    tester = testers.BaseTester(dataloader_num_workers=0)\n",
    "    return tester.get_all_embeddings(dataset, model)\n",
    "\n",
    "### compute accuracy using AccuracyCalculator from pytorch-metric-learning ###\n",
    "def test(train_set, test_set, model, accuracy_calculator):\n",
    "    train_embeddings, train_labels = get_all_embeddings(train_set, model)\n",
    "    test_embeddings, test_labels = get_all_embeddings(test_set, model)\n",
    "    train_labels = train_labels.squeeze(1)\n",
    "    test_labels = test_labels.squeeze(1)\n",
    "    # print(\"Computing accuracy\")\n",
    "    accuracies = accuracy_calculator.get_accuracy(\n",
    "        test_embeddings, train_embeddings, test_labels, train_labels, False\n",
    "    )\n",
    "    # print(\"Test set accuracy (Precision@1) = {}\".format(accuracies[\"precision_at_1\"]))\n",
    "    return accuracies[\"precision_at_1\"]\n",
    "    \n",
    "    \n",
    "### MNIST code originally from https://github.com/pytorch/examples/blob/master/mnist/main.py ###\n",
    "def train(model, loss_func, mining_func, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    \n",
    "    pbar = tqdm(\n",
    "        enumerate(train_loader), \n",
    "        total=len(train_loader),\n",
    "        position=0,\n",
    "        leave=False,\n",
    "        desc='WAITING...')\n",
    "    \n",
    "    for batch_idx, (data, labels) in pbar:\n",
    "        \n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        embeddings = model(data)\n",
    "        # print('e', embeddings)\n",
    "        # print('l', labels)\n",
    "        indices_tuple = mining_func(embeddings, labels)\n",
    "        # print('it', indices_tuple)\n",
    "        loss = loss_func(embeddings, labels, indices_tuple)\n",
    "        # print(loss.item())\n",
    "        instant_loss = loss.item()\n",
    "        loss_sum += instant_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        pbar.set_description(\"TRAIN: INSTANT MEAN LOSS %f, MINED TRIPLET: %d\" % \n",
    "                             (round(instant_loss / len(labels), 3),\n",
    "                             mining_func.num_triplets)\n",
    "                            )\n",
    "        # if batch_idx >= 0:\n",
    "        #     break\n",
    "            \n",
    "    return loss_sum / len(train_loader.dataset)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "pbar = trange(\n",
    "        started_epoch, \n",
    "        config['epochs'], \n",
    "        initial=started_epoch, \n",
    "        total=config['epochs'],\n",
    "        leave=True,\n",
    "        desc='WAITING FOR FIRST EPOCH END...')\n",
    "\n",
    "for epoch in pbar:\n",
    "    \n",
    "    # plotSmth(encoder, train_dataset, device=device, dim3=False, fcn='umap')\n",
    "    train_loss = train(encoder, loss_func, mining_func, device, train_loader, optimizer, epoch)\n",
    "    mean_acc = test(train_dataset, valid_dataset, encoder, accuracy_calculator)\n",
    "    \n",
    "    # print(lr_val)\n",
    "    # lr_val = scheduler.get_last_lr()[0]\n",
    "    # saveCheckpoint(encoder, scheduler, optimizer, epoch, 'sample')\n",
    "    # plotSmth(encoder, CONST_MINIMAL_DATASET, device=device, dim3=False, fcn='umap')\n",
    "    # scheduler.step()\n",
    "    \n",
    "    mean_train_acc = mean_valid_acc = 0\n",
    "    lr_val = 1\n",
    "    pbar.set_description(\"PER EPOCH: TRAIN LOSS: %.4f; TRAIN ACCUR %.4f; VALID ACCUR: %.4f, LR %.2e\" % (train_loss, \n",
    "                                                                                                           mean_acc,\n",
    "                                                                                                           mean_valid_acc,\n",
    "                                                                                                           lr_val)\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bff8148",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange, tqdm\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from umap import UMAP\n",
    "import plotly.express as px\n",
    "\n",
    "@torch.no_grad()\n",
    "def plotSmth(model, data, device='cpu', dim3=False, fcn='PCA', reducer_arg=None):\n",
    "    model.eval()\n",
    "    print(type(data))\n",
    "    if isinstance(data, DataLoader):\n",
    "        print('data is DataLoader')\n",
    "        loader = data\n",
    "    else:\n",
    "        print('data is not DataLoader, assume its DataSet')\n",
    "        loader = DataLoader(data)\n",
    "        \n",
    "    plt.figure(figsize=(32, 16))    \n",
    "    # clean the figure\n",
    "    plt.clf()\n",
    "    \n",
    "    n_components = 3 if dim3 else 2\n",
    "    \n",
    "    fcn = fcn.upper()\n",
    "    \n",
    "    if reducer_arg == None:\n",
    "        if fcn == 'PCA':\n",
    "            reducer = PCA(n_components=n_components, random_state=RANDOM_STATE) \n",
    "        elif fcn == 'TSNE':\n",
    "            reducer = TSNE(n_components=n_components, init='random', random_state=RANDOM_STATE)\n",
    "        elif fcn == 'UMAP':\n",
    "            reducer = UMAP(n_components=n_components, init='random', random_state=RANDOM_STATE)\n",
    "        else:\n",
    "            assert False, \"wrong fcn arg\"        \n",
    "    else:\n",
    "        reducer = reducer_arg\n",
    "        \n",
    "    pbar = tqdm(enumerate(loader),\n",
    "                    total=len(loader), \n",
    "                    position=0,\n",
    "                    leave=False)\n",
    "\n",
    "    MODEL_ARRAY_OUT_SIZE = (0, 128)\n",
    "    model_out_arr = np.empty(MODEL_ARRAY_OUT_SIZE, dtype=np.float32)\n",
    "    target_arr = np.empty((0, 1), dtype=np.int32)\n",
    "\n",
    "    for idx, (data, target) in pbar:\n",
    "\n",
    "        data = data.to(device)\n",
    "        out = model(data).detach().cpu().numpy()\n",
    "        # print(out.shape)\n",
    "        model_out_arr = np.append(model_out_arr, out, axis=0)\n",
    "        target = target.detach().cpu().numpy()\n",
    "        # print(target)\n",
    "        target_arr = np.append(target_arr, target)\n",
    "\n",
    "        # if idx > 80:\n",
    "        #     break\n",
    "\n",
    "    # print(len(target_arr)) \n",
    "    # print(target_arr)\n",
    "    if reducer_arg == None:\n",
    "        X_embedded = reducer.fit_transform(model_out_arr)\n",
    "    else:\n",
    "        X_embedded = reducer.transform(model_out_arr)\n",
    "    # print(X_embedded)\n",
    "    # target_arr = np.char.mod('%d', target_arr)\n",
    "    target_arr_color = [NUMBER_TO_SIGN[x] for x in target_arr]\n",
    "    # print(target_arr)\n",
    "    \n",
    "    if not dim3:\n",
    "        fig = px.scatter(X_embedded, x=0, y=1, color=target_arr_color)\n",
    "    else:\n",
    "        fig = px.scatter_3d(X_embedded, x=0, y=1, z=2, color=target_arr_color)    \n",
    "    fig.show()\n",
    "    \n",
    "    return reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f813883",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = plotSmth(encoder, valid_dataset, device=device, dim3=False, fcn='umap')\n",
    "plotSmth(encoder, train_dataset, device=device, dim3=False, reducer_arg=r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fdc750",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# \n",
    "\n",
    "# display(CONST_MINIMAL_DF)\n",
    "# TEMP_DS = getNSamplesFromDataSet(CONST_MINIMAL_DF, 1)\n",
    "# print(TEMP_DS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7b2b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONST_MINIMAL_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbfe7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mining_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2bfc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "model = resnet18()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c68b3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange, tqdm\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from umap import UMAP\n",
    "import plotly.express as px\n",
    "\n",
    "@torch.no_grad()\n",
    "def plotSmth(model, loader, device='cpu', dim3=False, fcn='PCA'):\n",
    "    model.eval()\n",
    "    \n",
    "    plt.figure(figsize=(32, 16))    \n",
    "    # clean the figure\n",
    "    plt.clf()\n",
    "    \n",
    "    n_components = 3 if dim3 else 2\n",
    "    \n",
    "    fcn = fcn.upper()\n",
    "    \n",
    "    if fcn == 'PCA':\n",
    "        reducer = PCA(n_components=n_components, random_state=RANDOM_STATE) \n",
    "    elif fcn == 'TSNE':\n",
    "        reducer = TSNE(n_components=n_components, init='random', random_state=RANDOM_STATE)\n",
    "    elif fcn == 'UMAP':\n",
    "        reducer = UMAP(n_components=n_components, init='random', random_state=RANDOM_STATE)\n",
    "    else:\n",
    "        assert False, \"wrong fcn arg\"\n",
    "    \n",
    "    pbar = tqdm(enumerate(loader),\n",
    "                    total=len(loader), \n",
    "                    position=0,\n",
    "                    leave=False)\n",
    "\n",
    "    MODEL_ARRAY_OUT_SIZE = (0, 128)\n",
    "    model_out_arr = np.empty(MODEL_ARRAY_OUT_SIZE, dtype=np.float32)\n",
    "    target_arr = np.empty((0, 1), dtype=np.int32)\n",
    "\n",
    "    for idx, (data, target) in pbar:\n",
    "\n",
    "        data = data.to(device)\n",
    "        out = model(data).detach().cpu().numpy()\n",
    "        # print(out.shape)\n",
    "        model_out_arr = np.append(model_out_arr, out, axis=0)\n",
    "        target = target.detach().cpu().numpy()\n",
    "        # print(target)\n",
    "        target_arr = np.append(target_arr, target)\n",
    "\n",
    "        # if idx > 1:\n",
    "        #     break\n",
    "\n",
    "    # print(len(target_arr)) \n",
    "    # print(target_arr)\n",
    "    X_embedded = reducer.fit_transform(model_out_arr)\n",
    "    target_arr = np.char.mod('%d', target_arr)\n",
    "    # print(target_arr)\n",
    "    \n",
    "    if not dim3:\n",
    "        fig = px.scatter(X_embedded, x=0, y=1, color=target_arr)\n",
    "    else:\n",
    "        fig = px.scatter_3d(X_embedded, x=0, y=1, z=2, color=target_arr)    \n",
    "    fig.show()\n",
    "    \n",
    "    \n",
    "test_loader = torch.utils.data.DataLoader(dataset2, batch_size=256)\n",
    "# model.apply(init_normal)\n",
    "plotSmth(model, test_loader, device=device, dim3=True, fcn='pca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113da1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388bc89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "enumerate(train_loader)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42c4a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(dataset1[1][0][None, ...].to(device)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf03f67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = get_all_embeddings(dataset1, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515f130e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f132c771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ed2e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad(dataset1[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdccee0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:adas] *",
   "language": "python",
   "name": "conda-env-adas-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
