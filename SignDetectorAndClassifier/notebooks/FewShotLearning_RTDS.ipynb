{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdc13461",
   "metadata": {},
   "source": [
    "## Цель ноутбука: изучение метода Few Shots Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefeb9ca",
   "metadata": {},
   "source": [
    "#### В RTSD не хватает 14 знаков:\n",
    "\n",
    "| Знак | Описание | Источник |\n",
    "| ------------- | ------------- | ---- |\n",
    "| 1.6 | Пересечение равнозначных дорог | - |\n",
    "| 1.31 | Туннель | - |\n",
    "| 2.4 | Уступите дорогу | GTSRB Recognition |\n",
    "| 3.21 | Конец запрещения обгона | GTSRB Recognition |\n",
    "| 3.22 | Обгон грузовым автомобилям запрещен | GTSRB Recognition |\n",
    "| 3.23 | Конец запрещения обгона грузовым автомобилям | GTSRB Recognition |\n",
    "| 3.24-90 | Огр 90 | - |\n",
    "| 3.24-100 | Огр 100 | GTSRB Recognition |\n",
    "| 3.24-110 | Огр 110 | - |\n",
    "| 3.24-120 | Огр 120 | GTSRB Recognition |\n",
    "| 3.24-130 | Огр 130 | - |\n",
    "| 3.25 | Конец огр. максимальной скорости | GTSRB Recognition |\n",
    "| 3.31 | Конец всех ограничений | GTSRB Recognition |\n",
    "| 6.3.2 | Зона для разворота | - |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5a296e",
   "metadata": {},
   "source": [
    "Инициализация библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b694f2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "if A.__version__ != '1.0.3':\n",
    "    !pip install albumentations==1.0.3\n",
    "    !pip install opencv-python-headless==4.5.2.52\n",
    "    assert False, 'restart runtime pls'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import pathlib\n",
    "import shutil\n",
    "import cv2\n",
    "import PIL\n",
    "import cv2\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "TEXT_COLOR = 'black'\n",
    "# Зафиксируем состояние случайных чисел\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (17,10)\n",
    "\n",
    "IN_COLAB = False\n",
    "USE_COLAB_GPU = False\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    USE_COLAB_GPU = True\n",
    "    from google.colab import drive\n",
    "except:\n",
    "    if IN_COLAB:\n",
    "        print('[!]YOU ARE IN COLAB, BUT DIDNT MOUND A DRIVE. Model wont be synced[!]')\n",
    "\n",
    "        if not os.path.isfile(CURRENT_FILE_NAME):\n",
    "            print(\"FIX ME\")\n",
    "        IN_COLAB = False\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19a8a95",
   "metadata": {},
   "source": [
    "Инициализация основных путей и папки src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7d8136",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not IN_COLAB:\n",
    "    PROJECT_ROOT = pathlib.Path(os.path.join(os.curdir, os.pardir))\n",
    "else:\n",
    "    PROJECT_ROOT = pathlib.Path('..')\n",
    "    \n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "NOTEBOOKS_DIR = PROJECT_ROOT / 'notebooks'\n",
    "SRC_PATH = str(PROJECT_ROOT / 'src')\n",
    "\n",
    "if SRC_PATH not in sys.path:\n",
    "    sys.path.append(SRC_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe33e5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from torchvision.models import resnet\n",
    "\n",
    "class SplitBatchNorm(torch.nn.BatchNorm2d):\n",
    "    def __init__(self, num_features, num_splits, **kw):\n",
    "        super().__init__(num_features, **kw)\n",
    "        self.num_splits = num_splits\n",
    "\n",
    "    def forward(self, input):\n",
    "        N, C, H, W = input.shape\n",
    "        if self.training or not self.track_running_stats:\n",
    "            running_mean_split = self.running_mean.repeat(self.num_splits)\n",
    "            running_var_split = self.running_var.repeat(self.num_splits)\n",
    "            outcome = torch.nn.functional.batch_norm(\n",
    "                input.view(-1, C * self.num_splits, H, W),\n",
    "                running_mean_split,\n",
    "                running_var_split,\n",
    "                self.weight.repeat(self.num_splits),\n",
    "                self.bias.repeat(self.num_splits),\n",
    "                True,\n",
    "                self.momentum,\n",
    "                self.eps,\n",
    "            ).view(N, C, H, W)\n",
    "            self.running_mean.data.copy_(\n",
    "                running_mean_split.view(self.num_splits, C).mean(dim=0)\n",
    "            )\n",
    "            self.running_var.data.copy_(\n",
    "                running_var_split.view(self.num_splits, C).mean(dim=0)\n",
    "            )\n",
    "            return outcome\n",
    "        else:\n",
    "            return torch.nn.functional.batch_norm(\n",
    "                input,\n",
    "                self.running_mean,\n",
    "                self.running_var,\n",
    "                self.weight,\n",
    "                self.bias,\n",
    "                False,\n",
    "                self.momentum,\n",
    "                self.eps,\n",
    "            )\n",
    "        \n",
    "class ModelBase(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Common CIFAR ResNet recipe.\n",
    "    Comparing with ImageNet ResNet recipe, it:\n",
    "    (i) replaces conv1 with kernel=3, str=1\n",
    "    (ii) removes pool1\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_dim=128, arch=\"resnet18\", bn_splits=8):\n",
    "        super(ModelBase, self).__init__()\n",
    "\n",
    "        # use split batchnorm\n",
    "        norm_layer = (\n",
    "            partial(SplitBatchNorm, num_splits=bn_splits)\n",
    "            if bn_splits > 1\n",
    "            else torch.nn.BatchNorm2d\n",
    "        )\n",
    "        # print(norm_layer)\n",
    "        resnet_arch = getattr(resnet, arch)\n",
    "        # print(resnet_arch)\n",
    "        net = resnet_arch(num_classes=feature_dim, norm_layer=norm_layer)\n",
    "\n",
    "        self.net = []\n",
    "        for name, module in net.named_children():\n",
    "            print(name)\n",
    "            if name == \"conv1\":\n",
    "                module = torch.nn.Conv2d(\n",
    "                    3, 64, kernel_size=3, stride=1, padding=1, bias=False\n",
    "                )\n",
    "            if isinstance(module, torch.nn.MaxPool2d):\n",
    "                continue\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                self.net.append(torch.nn.Flatten(1))\n",
    "            self.net.append(module)\n",
    "\n",
    "        self.net = torch.nn.Sequential(*self.net)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        # note: not normalized here\n",
    "        return x\n",
    "\n",
    "def create_encoder(emb_dim):\n",
    "    model = ModelBase(emb_dim)\n",
    "    # model = torch.nn.DataParallel(model)\n",
    "    # model.to(device)\n",
    "    return model\n",
    "\n",
    "encoder = create_encoder(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d99cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "RTDS_DF = pd.read_csv(DATA_DIR / 'RTDS_DATASET.csv')\n",
    "RTDS_DF['filepath'] = RTDS_DF['filepath'].apply(lambda x: str(DATA_DIR / x))\n",
    "\n",
    "# UNFIX TRAIN\n",
    "# SIMPLE_FIX = True\n",
    "# JUST_FIX = False\n",
    "RTDS_DF.drop_duplicates(subset=['filepath'], inplace=True)\n",
    "# RTDS_DF.drop_duplicates(subset=['SET', 'SIGN'], inplace=True)\n",
    "\n",
    "RTDS_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e74bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "RTDS_DF.groupby(['SIGN', 'SET']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97019252",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_NUMBER = 13\n",
    "\n",
    "# RTDS_DF = RTDS_DF.groupby(['SIGN', 'SET']).sample(SAMPLE_NUMBER, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "# LEARN_RTDS_DF.groupby(['SIGN']).sample(SAMPLE_NUMBER, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "RTDS_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64400440",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(RTDS_DF['SIGN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b46b42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, set_label=None, hyp=None, transform=None, le=None):\n",
    "                \n",
    "        self.transform = transform\n",
    "        \n",
    "        if set_label == None:\n",
    "            self.df = df\n",
    "        else:\n",
    "            self.df = df[df['SET']==set_label]\n",
    "        \n",
    "        self.hyp = hyp\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df.index)\n",
    "    \n",
    "    def __getitem__(self, index): \n",
    "        label = int(self.df.iloc[index]['ENCODED_LABEL'])\n",
    "        path = str(self.df.iloc[index]['filepath'])\n",
    "        sign = str(self.df.iloc[index]['SIGN'])\n",
    "        img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "        \n",
    "        # check does it contains transparent channel \n",
    "        if img.shape[2] == 4:\n",
    "        # randomize transparent\n",
    "            trans_mask = img[:,:,3] == 0\n",
    "            img[trans_mask] = [random.randrange(0, 256), \n",
    "                               random.randrange(0, 256), \n",
    "                               random.randrange(0, 256), \n",
    "                               255]\n",
    "\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)\n",
    "        # /randomize transparent\n",
    "                \n",
    "        # augment\n",
    "        if self.hyp and self.transform:\n",
    "            img, _ =  random_perspective(img, \n",
    "                                      (),\n",
    "                                      degrees=self.hyp['degrees'],\n",
    "                                      translate=self.hyp['translate'],\n",
    "                                      scale=self.hyp['scale'],\n",
    "                                      shear=self.hyp['shear'],\n",
    "                                      perspective=self.hyp['perspective'],\n",
    "                                      border=self.hyp['border'])   \n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        # /augment\n",
    "        \n",
    "        img = img / 255\n",
    "        return img, label, (path, sign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbabf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations.augmentations.geometric.transforms import Perspective, ShiftScaleRotate\n",
    "from albumentations.core.transforms_interface import ImageOnlyTransform\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from albumentations.augmentations.transforms import PadIfNeeded\n",
    "from albumentations.augmentations.geometric.resize import LongestMaxSize\n",
    "\n",
    "# import torchvision.transforms.functional as F\n",
    "\n",
    "img_size = 40\n",
    "\n",
    "MINIMAL_TRANSFORM = False\n",
    "\n",
    "if MINIMAL_TRANSFORM:\n",
    "    transform = A.Compose(\n",
    "        [\n",
    "        LongestMaxSize(img_size),\n",
    "        PadIfNeeded(\n",
    "            img_size, \n",
    "            img_size, \n",
    "            border_mode=cv2.BORDER_CONSTANT, \n",
    "            value=0\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    "else:\n",
    "    transform = A.Compose(\n",
    "        [\n",
    "        A.Blur(blur_limit=2),\n",
    "        A.CLAHE(p=0.5),\n",
    "        A.Perspective(scale=(0.01, 0.1), p=0.5), \n",
    "        A.ShiftScaleRotate(shift_limit=0.05,\n",
    "                           scale_limit=0.05,\n",
    "                           interpolation=cv2.INTER_LANCZOS4, \n",
    "                           border_mode=cv2.BORDER_CONSTANT, \n",
    "                           value=(0,0,0),\n",
    "                           rotate_limit=6, p=0.5),\n",
    "        A.RandomGamma(\n",
    "            gamma_limit=(50, 130), \n",
    "            p=1\n",
    "        ),\n",
    "        A.ImageCompression(quality_lower=80, p=0.5),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.5, \n",
    "                                   contrast_limit=0.3, \n",
    "                                   brightness_by_max=False, \n",
    "                                   p=0.5),\n",
    "        A.CoarseDropout(max_height=3, \n",
    "                        max_width=3, \n",
    "                        min_holes=1, \n",
    "                        max_holes=3, \n",
    "                        p=0.5),\n",
    "        LongestMaxSize(img_size),\n",
    "        PadIfNeeded(\n",
    "            img_size, \n",
    "            img_size, \n",
    "            border_mode=cv2.BORDER_CONSTANT, \n",
    "            value=0\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "train_dataset = SignDataset(RTDS_DF, \n",
    "                            set_label='train',  \n",
    "                            transform=transform, \n",
    "                            hyp=None)\n",
    "\n",
    "valid_dataset = SignDataset(RTDS_DF, \n",
    "                            set_label='valid',  \n",
    "                            transform=transform, \n",
    "                            hyp=None)\n",
    "\n",
    "# valid_dataset = train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6c31f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487a7cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNSamplesFromDataSet(ds, N):\n",
    "    random_index = random.sample(range(0, len(ds)), N)\n",
    "    ret = []\n",
    "    for index in random_index:\n",
    "        ret.append(ds[index])\n",
    "    return ret\n",
    "\n",
    "IMG_COUNT = 18\n",
    "nrows, ncols = 70, 6\n",
    "fig = plt.figure(figsize = (16,200))\n",
    "\n",
    "PLOT_SOFT_LIMIT = 20\n",
    "\n",
    "TEMP_DS = getNSamplesFromDataSet(train_dataset, 20)\n",
    "# TEMP_DS = train_dataset.sort_values(['SIGN'], axis=1)\n",
    "# TEMP_DS = train_dataset\n",
    "for idx, (img, encoded_label, info) in enumerate(TEMP_DS):\n",
    "    \n",
    "    img = torch.Tensor.permute(img, [1, 2, 0]).numpy() \n",
    "    ax = fig.add_subplot(nrows, ncols, idx+1)\n",
    "        \n",
    "    ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), aspect=1)\n",
    "    \n",
    "    title = str(info[1])\n",
    "    \n",
    "    ax.set_title(title, fontsize=15)\n",
    "    \n",
    "    if idx > PLOT_SOFT_LIMIT:\n",
    "        print('[!] plot soft limit reached. Breaking.')\n",
    "        break\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6fc60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 56\n",
    "num_workers = 2 if IN_COLAB else 0\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def getDataLoaderFromDataset(dataset, shuffle=False, drop_last=True):\n",
    "    \n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last\n",
    "    )\n",
    "    \n",
    "    return loader\n",
    "\n",
    "\n",
    "train_loader = getDataLoaderFromDataset(\n",
    "    train_dataset,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f756782",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(encoder.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f17e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_img, sample_label, _ = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a3f36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sample_img)\n",
    "# with torch.no_grad():\n",
    "#     encoder.eval()\n",
    "#     out = encoder(sample_img).detach().cpu()\n",
    "    \n",
    "# out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9824041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(encoder.state_dict(), 'last_encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c840de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveCheckpoint(model, scheduler, optimizer, epoch, filename):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'scheduler': scheduler.state_dict()\n",
    "    }, filename)\n",
    "\n",
    "def loadCheckpoint(model, scheduler, optimizer, filename):\n",
    "    checkpoint = torch.load(filename)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    return model, optimizer, scheduler, epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876f1cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder, optimizer, scheduler, started_epoch = loadCheckpoint(encoder, scheduler, optimizer, 'sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9872b253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saveCheckpoint(encoder, encoder, optimizer, 0, 'sample')\n",
    "# encoder.load_state_dict(torch.load('sample')['model'])\n",
    "# encoder.eval()\n",
    "assert True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be43bf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_metric_learning import distances, losses, miners, reducers, testers\n",
    "from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator\n",
    "\n",
    "from pytorch_metric_learning.utils import common_functions as c_f\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "config = {\n",
    "    'lr': 0.1,\n",
    "    'epochs': 17,\n",
    "    'momentum':  0.937,\n",
    "    'margin': 0\n",
    "}\n",
    "\n",
    "# optimizer = torch.optim.SGD(encoder.parameters(), lr=config['lr'], momentum=config['momentum'], nesterov=True)\n",
    "# encoder.to('cpu')\n",
    "optimizer = torch.optim.Adam(encoder.parameters(), lr=0.1)\n",
    "# scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, \n",
    "#                                              base_lr=0.00001, \n",
    "#                                              max_lr=config['lr'],\n",
    "#                                              step_size_up=50,\n",
    "#                                              step_size_down=20,\n",
    "#                                              mode=\"exp_range\",\n",
    "#                                              gamma=0.9,\n",
    "#                                              cycle_momentum=False\n",
    "#                                            )\n",
    "distance = distances.LpDistance()\n",
    "reducer = reducers.AvgNonZeroReducer()\n",
    "loss_func = losses.TripletMarginLoss(margin=config['margin'], distance=distance, reducer=reducer)\n",
    "\n",
    "# mining_func = miners.MultiSimilarityMiner(epsilon=0.1)\n",
    "mining_func = miners.TripletMarginMiner(margin=config['margin'], distance=distance, type_of_triplets=\"all\")\n",
    "\n",
    "accuracy_calculator = AccuracyCalculator(\n",
    "    include=(\"precision_at_1\",\n",
    "             \"mean_average_precision_at_r\"), k=1)\n",
    "\n",
    "try:\n",
    "    # encoder, optimizer, scheduler, started_epoch = loadCheckpoint(encoder, scheduler, optimizer, 'sample')\n",
    "    started_epoch\n",
    "    print('[+] check point loaded')\n",
    "except:\n",
    "    started_epoch = 0\n",
    "    print('[!] check point doesnt exist')\n",
    "\n",
    "encoder.to(device)    \n",
    "\n",
    "def get_all_embeddings(dataset, model):\n",
    "    tester = testers.BaseTester(dataloader_num_workers=0)\n",
    "    res =  tester.get_all_embeddings(dataset, model)\n",
    "    return res\n",
    "\n",
    "### convenient function from pytorch-metric-learning ###\n",
    "@torch.no_grad()\n",
    "def simpleGetAllEmbeddings(model, dataset, batch_size, dsc=''):\n",
    "    \n",
    "    dataloader = getDataLoaderFromDataset(\n",
    "        dataset,\n",
    "        shuffle=True,\n",
    "        drop_last=False\n",
    "    )\n",
    "    \n",
    "    s, e = 0, 0\n",
    "    \n",
    "    pbar = tqdm(\n",
    "        enumerate(dataloader), \n",
    "        total=len(dataloader),\n",
    "        position=0,\n",
    "        leave=False,\n",
    "        desc='Getting all embeddings...' + dsc)\n",
    "    info_arr = []\n",
    "    \n",
    "    add_info_len = None\n",
    "    \n",
    "    for idx, (data, labels, info) in pbar:\n",
    "        data = data.to(device)\n",
    "        \n",
    "        q = model(data)\n",
    "        \n",
    "        if labels.dim() == 1:\n",
    "            labels = labels.unsqueeze(1)\n",
    "        if idx == 0:\n",
    "            labels_ret = torch.zeros(\n",
    "                len(dataloader.dataset),\n",
    "                labels.size(1),\n",
    "                device=device,\n",
    "                dtype=labels.dtype,\n",
    "            )\n",
    "            all_q = torch.zeros(\n",
    "                len(dataloader.dataset),\n",
    "                q.size(1),\n",
    "                device=device,\n",
    "                dtype=q.dtype,\n",
    "            )\n",
    "        \n",
    "        info = np.array(info)\n",
    "        if add_info_len == None:\n",
    "            add_info_len = info.shape[0]\n",
    "        \n",
    "        info_arr.extend(info.T.reshape((-1, add_info_len)))\n",
    "        # return info_arr\n",
    "        # print(info)\n",
    "        # input()\n",
    "        # print(len(info))\n",
    "        e = s + q.size(0)\n",
    "        all_q[s:e] = q\n",
    "        labels_ret[s:e] = labels\n",
    "        s = e  \n",
    "    \n",
    "    all_q = torch.nn.functional.normalize(all_q)\n",
    "    # print(info)\n",
    "    return all_q, labels_ret, info_arr\n",
    "\n",
    "### compute accuracy using AccuracyCalculator from pytorch-metric-learning ###\n",
    "def test(train_set, test_set, model, accuracy_calculator, batch_size):\n",
    "    model.eval()\n",
    "    train_embeddings, train_labels, _ = simpleGetAllEmbeddings(model, train_set, batch_size, ' for train')\n",
    "    test_embeddings, test_labels, _ = simpleGetAllEmbeddings(model, test_set, batch_size, ' for test')\n",
    "    \n",
    "    train_labels = train_labels.squeeze(1)\n",
    "    test_labels = test_labels.squeeze(1)\n",
    "    # print(\"Computing accuracy\")\n",
    "    accuracies = accuracy_calculator.get_accuracy(\n",
    "        test_embeddings, train_embeddings, test_labels, train_labels, False\n",
    "    )\n",
    "    print(accuracies)\n",
    "    # print(\"Test set accuracy (Precision@1) = {}\".format(accuracies[\"precision_at_1\"]))\n",
    "    return accuracies[\"precision_at_1\"]\n",
    "    \n",
    "    \n",
    "### MNIST code originally from https://github.com/pytorch/examples/blob/master/mnist/main.py ###\n",
    "def train(model, loss_func, mining_func, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    \n",
    "    pbar = tqdm(\n",
    "        enumerate(train_loader), \n",
    "        total=len(train_loader),\n",
    "        position=0,\n",
    "        leave=False,\n",
    "        desc='WAITING...')\n",
    "    \n",
    "    USING_CentroidTripletLoss_FLAG = False\n",
    "    USING_MultiSimilarityMiner_FLAG = False\n",
    "    if isinstance(loss_func, losses.CentroidTripletLoss):\n",
    "        USING_CentroidTripletLoss_FLAG = True\n",
    "    if isinstance(mining_func, miners.MultiSimilarityMiner):\n",
    "        USING_MultiSimilarityMiner_FLAG = True\n",
    "        \n",
    "    for batch_idx, (data, labels, _) in pbar:\n",
    "        \n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        embeddings = model(data)\n",
    "\n",
    "        if USING_CentroidTripletLoss_FLAG:\n",
    "            embeddings = torch.tensor(\n",
    "                [c_f.angle_to_coord(a) for a in embeddings],\n",
    "                requires_grad=True,\n",
    "                dtype=dtype,\n",
    "            ).to(\n",
    "                device\n",
    "            )\n",
    "            print(embeddings.shape)\n",
    "            print(labels.shape)\n",
    "            loss = loss_func(embeddings, labels)\n",
    "        else:\n",
    "            indices_tuple = mining_func(embeddings, labels)\n",
    "            loss = loss_func(embeddings, labels, indices_tuple)\n",
    "\n",
    "        instant_loss = loss.item()\n",
    "        loss_sum += instant_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if USING_CentroidTripletLoss_FLAG or USING_MultiSimilarityMiner_FLAG:\n",
    "            pbar.set_description(\"TRAIN: INSTANT MEAN LOSS %f\" % \n",
    "                             (round(instant_loss / len(labels), 3))\n",
    "                            )            \n",
    "        else:\n",
    "            pbar.set_description(\"TRAIN: INSTANT MEAN LOSS %f, MINED TRIPLET: %d\" % \n",
    "                             (round(instant_loss / len(labels), 3),\n",
    "                             mining_func.num_triplets)\n",
    "                            )\n",
    "        # if batch_idx >= 0:\n",
    "        #     break\n",
    "            \n",
    "    return loss_sum / (train_loader.batch_size * len(train_loader))\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "pbar = trange(\n",
    "        started_epoch, \n",
    "        config['epochs'], \n",
    "        initial=started_epoch, \n",
    "        total=config['epochs'],\n",
    "        leave=True,\n",
    "        desc='WAITING FOR FIRST EPOCH END...')\n",
    "\n",
    "mean_acc = -1\n",
    "\n",
    "# assert False\n",
    "for epoch in pbar:\n",
    "    \n",
    "    # plotSmth(encoder, train_dataset, device=device, dim3=False, fcn='umap')\n",
    "    train_loss = train(encoder, loss_func, mining_func, device, train_loader, optimizer, epoch)\n",
    "    \n",
    "    if (epoch + 1) % 8 == 0:\n",
    "        mean_acc = test(train_dataset, valid_dataset, encoder, accuracy_calculator, batch_size)\n",
    "    \n",
    "    # print(lr_val)\n",
    "    # lr_val = scheduler.get_last_lr()[0]\n",
    "    # saveCheckpoint(encoder, scheduler, optimizer, epoch, 'sample')\n",
    "    # plotSmth(encoder, CONST_MINIMAL_DATASET, device=device, dim3=False, fcn='umap')\n",
    "    # scheduler.step()\n",
    "    \n",
    "    mean_train_acc = mean_valid_acc = 0\n",
    "    lr_val = 1\n",
    "    pbar.set_description(\"PER %d EPOCH: TRAIN LOSS: %.4f; VALID ACCUR: %.4f, LR %.2e\" % (\n",
    "        epoch,\n",
    "        train_loss, \n",
    "        mean_acc,\n",
    "        lr_val)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa9dc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_DF = pd.DataFrame(columns=RTDS_DF.columns)\n",
    "# display(additional_DF)\n",
    "encode_offset = max(set(RTDS_DF['ENCODED_LABEL'])) + 1\n",
    "\n",
    "files = os.listdir(DATA_DIR / 'additional_sign')\n",
    "\n",
    "sign_list = list(set([x.split('_')[0] for x in files]))\n",
    "\n",
    "for file in files:\n",
    "    sign = file.split('_')[0]\n",
    "    # print(file.split('_')[1].split('.')[0])\n",
    "    encoded_label = encode_offset + int(sign_list.index(sign))\n",
    "    \n",
    "    # print(sign)\n",
    "    row = {'filepath': str(DATA_DIR / 'additional_sign' / file), 'SIGN':sign, 'ENCODED_LABEL':encoded_label, 'SET':'valid'} \n",
    "    additional_DF = additional_DF.append(row, ignore_index=True)\n",
    "display(additional_DF)\n",
    "\n",
    "minimal_transform = A.Compose(\n",
    "        [\n",
    "        LongestMaxSize(img_size),\n",
    "        PadIfNeeded(\n",
    "            img_size, \n",
    "            img_size, \n",
    "            border_mode=cv2.BORDER_CONSTANT, \n",
    "            value=0\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "additional_dataset = SignDataset(\n",
    "    additional_DF,\n",
    "    transform=minimal_transform\n",
    ")\n",
    "\n",
    "add_dataset_dict = dict(zip(additional_DF.ENCODED_LABEL, additional_DF.SIGN))\n",
    "\n",
    "IMG_COUNT = 18\n",
    "nrows, ncols = 70, 6\n",
    "fig = plt.figure(figsize = (16,200))\n",
    "\n",
    "PLOT_SOFT_LIMIT = 20\n",
    "\n",
    "for idx, (img, encoded_label, info) in enumerate(additional_dataset):\n",
    "    \n",
    "    img = torch.Tensor.permute(img, [1, 2, 0]).numpy() \n",
    "    ax = fig.add_subplot(nrows, ncols, idx+1)\n",
    "        \n",
    "    ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), aspect=1)\n",
    "    \n",
    "    title = str(info[1])\n",
    "    \n",
    "    ax.set_title(title, fontsize=15)\n",
    "    \n",
    "    if idx > PLOT_SOFT_LIMIT:\n",
    "        print('[!] plot soft limit reached. Breaking.')\n",
    "        break\n",
    "plt.tight_layout()\n",
    "\n",
    "add_dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889d6465",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange, tqdm\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from umap import UMAP\n",
    "import plotly.graph_objects as go\n",
    "from ipywidgets import Output, VBox\n",
    "    \n",
    "\n",
    "@torch.no_grad()\n",
    "def plotSmth(model, \n",
    "             dataset, \n",
    "             batch_size, \n",
    "             device='cpu', \n",
    "             dim3=False, \n",
    "             fcn='PCA', \n",
    "             reducer_arg=None, \n",
    "             dsc='', \n",
    "             label_dict=None, \n",
    "             dot_limit=5000,\n",
    "             additional_dataset=None,\n",
    "             main_dataset_marker_size=10,\n",
    "             additional_dataset_marker_size=20,\n",
    "             color_dict=None\n",
    "            ):\n",
    "    \n",
    "    model.eval()\n",
    "        \n",
    "    if len(dataset) > dot_limit:\n",
    "        print(\"[!] Dot limit! Random choice\", dot_limit, '\\nSrc len', len(dataset))\n",
    "        indicies = np.random.choice(len(dataset), dot_limit, replace=False)\n",
    "        dataset = torch.utils.data.Subset(dataset, indicies)\n",
    "    \n",
    "    # main dataset data\n",
    "    embeddings, labels, info = simpleGetAllEmbeddings(model, dataset, batch_size, dsc)\n",
    "    embeddings = embeddings.cpu().numpy()\n",
    "    labels = labels.cpu().numpy().flatten()[:, None]\n",
    "    size = np.ones(labels.shape) * main_dataset_marker_size\n",
    "\n",
    "    # additional dataset data\n",
    "    if additional_dataset:\n",
    "        embeddings_addon, labels_addon, info_addon = simpleGetAllEmbeddings(\n",
    "            model, \n",
    "            additional_dataset, \n",
    "            batch_size, \n",
    "            dsc='for addon')\n",
    "        \n",
    "        embeddings_addon = embeddings_addon.cpu().numpy()\n",
    "        labels_addon = labels_addon.cpu().numpy().flatten()[:, None]\n",
    "        \n",
    "        size_addon = np.ones(labels_addon.shape) * additional_dataset_marker_size\n",
    "        \n",
    "        size = np.concatenate((size, size_addon))\n",
    "        embeddings = np.concatenate((embeddings, embeddings_addon))\n",
    "        labels = np.concatenate((labels, labels_addon))\n",
    "        info.extend(info_addon)\n",
    "        \n",
    "        del embeddings_addon, labels_addon, size_addon, info_addon\n",
    "            \n",
    "    # plt.figure(figsize=(32, 32))    \n",
    "    # plt.clf()\n",
    "    \n",
    "    n_components = 3 if dim3 else 2\n",
    "    \n",
    "    fcn = fcn.upper()\n",
    "    \n",
    "    if reducer_arg == None:\n",
    "        if fcn == 'PCA':\n",
    "            reducer = PCA(n_components=n_components, random_state=RANDOM_STATE) \n",
    "        elif fcn == 'TSNE':\n",
    "            reducer = TSNE(n_components=n_components, init='random', random_state=RANDOM_STATE)\n",
    "        elif fcn == 'UMAP':\n",
    "            reducer = UMAP(n_components=n_components, init='random', random_state=RANDOM_STATE)\n",
    "        else:\n",
    "            assert False, \"wrong fcn arg\"        \n",
    "    else:\n",
    "        reducer = reducer_arg\n",
    "        \n",
    "\n",
    "    if reducer_arg == None:\n",
    "        X_embedded = reducer.fit_transform(embeddings)\n",
    "    else:\n",
    "        X_embedded = reducer.transform(embeddings)      \n",
    "    \n",
    "    # print(int(x) for x in labels)\n",
    "    if label_dict:\n",
    "        try:\n",
    "            target_arr_color = [label_dict[int(x)] for x in labels]\n",
    "        except:\n",
    "            print('label dict broken')\n",
    "            target_arr_color = labels\n",
    "    else:\n",
    "        target_arr_color = labels\n",
    "        \n",
    "    target_arr_color = np.array(target_arr_color)[:, None]\n",
    "    # print(target_arr_color.shape)\n",
    "    hover_data = np.array([x[1] + ':' + x[0] for x in info])[:, None]\n",
    "    \n",
    "    # now embeedings, labels, info, size are concatenated. Let's build dataframe from it\n",
    "    \n",
    "    plot_df_data = np.concatenate([X_embedded, target_arr_color, size, hover_data], axis=1)\n",
    "    if dim3:\n",
    "        columns = columns=['x', 'y', 'z', 'group', 'size', 'hover_data']\n",
    "    else:\n",
    "        columns = columns=['x', 'y', 'group', 'size', 'hover_data']\n",
    "        \n",
    "    plot_df = pd.DataFrame(plot_df_data, columns=columns)\n",
    "    plot_df['size'] = plot_df['size'].apply(pd.to_numeric)\n",
    "    \n",
    "    fig = go.FigureWidget()\n",
    "    \n",
    "    groups = plot_df['group'].unique()\n",
    "    \n",
    "    if dim3:\n",
    "        plot_df[['x', 'y', 'z']] = plot_df[['x', 'y', 'z']].apply(pd.to_numeric)\n",
    "        \n",
    "        for group in groups: \n",
    "            df = plot_df.loc[plot_df['group'] == group]\n",
    "            group_size = df['size'].iloc[0]\n",
    "            symbol = 'circle' if group_size == main_dataset_marker_size else 'diamond'\n",
    "            \n",
    "            fig.add_trace(go.Scatter3d(\n",
    "                x=df['x'],\n",
    "                y=df['y'],\n",
    "                z=df['z'],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=df['size'],\n",
    "                    opacity=1,\n",
    "                    symbol=symbol\n",
    "                ),\n",
    "                text=df['hover_data'],\n",
    "                hovertemplate='Sign: %{text}<extra></extra>',\n",
    "                name=group,\n",
    "            ))\n",
    "    else:\n",
    "        plot_df[['x', 'y']] = plot_df[['x', 'y']].apply(pd.to_numeric)\n",
    "        \n",
    "        for group in groups: \n",
    "            df = plot_df.loc[plot_df['group'] == group]\n",
    "            group_size = df['size'].iloc[0]\n",
    "            symbol = 'circle' if group_size == main_dataset_marker_size else 'diamond'\n",
    "            \n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=df['x'],\n",
    "                y=df['y'],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=df['size'],\n",
    "                    opacity=1,\n",
    "                    symbol=symbol\n",
    "                ),\n",
    "                text=df['hover_data'],\n",
    "                name=group,\n",
    "            ))\n",
    "            \n",
    "    fig.update_layout(\n",
    "        hoverlabel=dict(\n",
    "            bgcolor=\"white\",\n",
    "            font_size=12,\n",
    "            font_family=\"Rockwell\"\n",
    "        ),\n",
    "        width=800,\n",
    "        height=900\n",
    "    )\n",
    "    fig.show()\n",
    "    \n",
    "    return reducer, plot_df\n",
    "\n",
    "label_dict = dict(zip(RTDS_DF.ENCODED_LABEL, RTDS_DF.SIGN))\n",
    "label_dict.update(add_dataset_dict)\n",
    "\n",
    "dim3=False\n",
    "r = plotSmth(encoder, \n",
    "             train_dataset, \n",
    "             batch_size=batch_size, \n",
    "             device=device, \n",
    "             dim3=dim3, \n",
    "             fcn='umap', \n",
    "             label_dict=label_dict, \n",
    "             dot_limit=9000,\n",
    "             additional_dataset=additional_dataset\n",
    "            )\n",
    "assert False\n",
    "plotSmth(encoder, \n",
    "         valid_dataset, \n",
    "         batch_size=batch_size, \n",
    "         device=device, \n",
    "         dim3=dim3, \n",
    "         reducer_arg=r, \n",
    "         label_dict=label_dict,\n",
    "         dot_limit=6000,\n",
    "         additional_dataset=additional_dataset\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa102eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "RTDS_DF[RTDS_DF['SET'] == 'train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b6a2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "RTDS_DF[RTDS_DF['SET'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cea4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "RTDS_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44003eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c97e3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_DF = pd.DataFrame(columns=RTDS_DF.columns)\n",
    "# display(additional_DF)\n",
    "encode_offset = max(set(RTDS_DF['ENCODED_LABEL'])) + 1\n",
    "\n",
    "files = os.listdir(DATA_DIR / 'additional_sign')\n",
    "\n",
    "for file in files:\n",
    "    sign = file.split('_')[0]\n",
    "    encoded_label = encode_offset\n",
    "    # print(sign)\n",
    "    row = {'filepath': str(DATA_DIR / 'additional_sign' / file), 'SIGN':sign, 'ENCODED_LABEL':encoded_label, 'SET':'valid'} \n",
    "    additional_DF = additional_DF.append(row, ignore_index=True)\n",
    "display(additional_DF)\n",
    "\n",
    "additional_dataset = SignDataset(\n",
    "    additional_DF,\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473d6261",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows, ncols = 70, 6\n",
    "fig = plt.figure(figsize = (16,200))\n",
    "\n",
    "PLOT_SOFT_LIMIT = 80\n",
    "\n",
    "TEMP_DS = additional_dataset\n",
    "for idx, (img, encoded_label, info) in enumerate(TEMP_DS):\n",
    "    \n",
    "    img = torch.Tensor.permute(img, [1, 2, 0]).numpy() \n",
    "    ax = fig.add_subplot(nrows, ncols, idx+1)\n",
    "        \n",
    "    ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), aspect=1)\n",
    "    \n",
    "    title = str(info[1])\n",
    "    \n",
    "    ax.set_title(title, fontsize=15)\n",
    "    \n",
    "    if idx > PLOT_SOFT_LIMIT:\n",
    "        print('[!] plot soft limit reached. Breaking.')\n",
    "        break\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:adas] *",
   "language": "python",
   "name": "conda-env-adas-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
