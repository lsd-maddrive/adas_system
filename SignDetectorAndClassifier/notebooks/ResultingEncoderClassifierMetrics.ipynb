{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6698098e",
   "metadata": {},
   "source": [
    "## Resulting Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cc73ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# core imports\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "# append src\n",
    "try:\n",
    "    PROJECT_ROOT = Path(os.readlink(f'/proc/{os.environ[\"JPY_PARENT_PID\"]}/cwd'))\n",
    "except:\n",
    "    PROJECT_ROOT = Path(os.getcwd()).parent.parent\n",
    "DATA_DIR = PROJECT_ROOT / 'SignDetectorAndClassifier' / 'data'\n",
    "\n",
    "# Зафиксируем состояние случайных чисел\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device\n",
    "\n",
    "PLOT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717e7384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from maddrive_adas.sign_det.classifier import EncoderBasedClassifier\n",
    "encoder = EncoderBasedClassifier(\n",
    "        config_path=PROJECT_ROOT / 'classifier_archive',\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d951445",
   "metadata": {},
   "source": [
    "### Этап 1.1. Берем RTDS, из него берем *train* как *baseline*. Заменяем *valid* на *test*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33139cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PREFIX = DATA_DIR / 'ENCODER_DATASET'\n",
    "RTDS_DF = pd.read_csv(DATASET_PREFIX / 'WIDE_DATASET_4_ENCODER.csv')\n",
    "RTDS_DF['filepath'] = RTDS_DF['filepath'].apply(lambda x: str(DATASET_PREFIX / x))\n",
    "RTDS_DF.drop_duplicates(subset=['filepath'], inplace=True)\n",
    "\n",
    "# убираем доп знаки \n",
    "# RTDS_DF = RTDS_DF[RTDS_DF['filepath'].str.contains('rtsd')]\n",
    "\n",
    "TARGET_SIGNS = [\n",
    "    '1.1', '1.6', '1.8', '1.22', '1.31', '1.33', \n",
    "    '2.1', '2.2', '2.3', '2.4', '2.5', \n",
    "    '3.1', '3.18', '3.20', '3.21', '3.22', '3.23', '3.24',\n",
    "    '3.25', '3.27', '3.28', '3.31', \n",
    "    '4.1.1', '4.3', \n",
    "    '5.5', '5.6', '5.16', \n",
    "    '5.19.1', '5.20', \n",
    "    '6.3.2', '6.4', \n",
    "    '7.3', '7.4'\n",
    "]\n",
    "\n",
    "RTDS_DF = RTDS_DF[RTDS_DF['sign'].isin(TARGET_SIGNS)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bac7c929",
   "metadata": {},
   "source": [
    "### *train* как референс, *valid* - query для валидации.\n",
    "### Этап 1.2. Формируем DataFrame отсутствущих знаков в RTDS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c19ae9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "included_signs = sorted(set(RTDS_DF.sign))\n",
    "print('Included signs in ENCODER_DATASET:', included_signs)\n",
    "not_included_signs = sorted(set(TARGET_SIGNS) - set(RTDS_DF.sign))\n",
    "print('Not included in ENCODER_DATASET:', not_included_signs)\n",
    "\n",
    "print('Getting aditional sings...')\n",
    "additional_DF = pd.DataFrame(columns=RTDS_DF.columns)\n",
    "\n",
    "encode_offset = max(set(RTDS_DF['encoded'])) + 1\n",
    "files = os.listdir(DATA_DIR / 'additional_sign')\n",
    "\n",
    "skipped_signs = []\n",
    "row_list = []\n",
    "\n",
    "for file in files:\n",
    "    sign = file.split('_')[0]\n",
    "     \n",
    "    if sign.rsplit('.', 1)[0] == '3.25':\n",
    "        sign = '3.25'\n",
    "        \n",
    "    if sign.rsplit('.', 1)[0] == '3.24':\n",
    "        sign = '3.24'         \n",
    "\n",
    "    if sign in included_signs:\n",
    "        skipped_signs.append(sign)\n",
    "        continue\n",
    "        \n",
    "    row = {'filepath': str(DATA_DIR / 'additional_sign' / file), \n",
    "           'sign': sign, \n",
    "           'set': 'test', # HANDLE ME\n",
    "           'encoded': None\n",
    "          }\n",
    "\n",
    "    row_list.append(row)\n",
    "\n",
    "print('Skipped signs:', skipped_signs)\n",
    "additional_DF = pd.DataFrame(row_list, columns=RTDS_DF.columns)\n",
    "\n",
    "\n",
    "print('Including part of additional_DF for:', sorted(set(additional_DF.sign)), 'sign.')\n",
    "additional_DF = additional_DF[~additional_DF['sign'].isin(RTDS_DF['sign'])]\n",
    "\n",
    "RTDS_DF = pd.concat([RTDS_DF, additional_DF], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac509086",
   "metadata": {},
   "outputs": [],
   "source": [
    "from maddrive_adas.sign_det.detector import DetectedInstance\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "test_dataset: pd.DataFrame = RTDS_DF[RTDS_DF['set'] == 'test']\n",
    "\n",
    "labels_set = sorted(set(test_dataset['sign']))\n",
    "\n",
    "detected_instances = []\n",
    "labels = []\n",
    "for row in tqdm(test_dataset.itertuples(), total=len(test_dataset)):\n",
    "    filepath = row[1]\n",
    "    label = row[2]\n",
    "    labels.append(label)\n",
    "\n",
    "    img = cv2.imread(filepath)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    detected_instance = DetectedInstance(img)\n",
    "    detected_instance.add_rel_roi([0., 0., 1., 1.], 1.)\n",
    "    detected_instances.append(detected_instance)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5a07f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunker(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "predicted = []\n",
    "\n",
    "chunk_size = 200\n",
    "detected_instances_chuncker = chunker(detected_instances, chunk_size)\n",
    "for part_of_detected_instances in tqdm(detected_instances_chuncker, total=len(detected_instances) // chunk_size):\n",
    "    result = encoder.classify_batch(part_of_detected_instances)\n",
    "    predicted.extend(\n",
    "        [x[1][0][0] for x in result]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba8357b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "cf_ = confusion_matrix(labels, predicted, normalize='true', labels=list(set(labels)))\n",
    "cmd_ = ConfusionMatrixDisplay(cf_)\n",
    "cmd_.plot()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e267a65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf9250f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cr_ = classification_report(labels, predicted, output_dict=True)\n",
    "df = pd.DataFrame(cr_).transpose()\n",
    "df.to_csv(\"report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6136cbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"report\", index_col=0)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3f126ba513cd923a91965ccfdcd1e275957d64ce4742838d456229721288bc16"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
