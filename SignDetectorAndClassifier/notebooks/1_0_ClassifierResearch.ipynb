{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ноутбук для обучения \"классического\" классификатора на датасете RTSD. *Неактуальный*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BgZKi4UDtiDt"
   },
   "source": [
    "# датасет должен быть или скачен или сделан с помощью ноутбука RTSD-R_MERGED\n",
    "Объединенный датасет доступен по [ссылке](https://drive.google.com/drive/folders/1jmxG2zfi-Fs3m2KrMGmjD347aYiT8YFM?usp=sharing).\n",
    "\n",
    "Положить в папку data содержимое так, чтобы были следующие пути:  \n",
    "* \\$(ROOT_DIR)/data/merged-rtsd/...\n",
    "* \\$(ROOT_DIR)/data/gt.csv\n",
    "\n",
    "> *gt_Set_NaN.csv - содержит тот же датасет, но значения колонки Set обнулено*\n",
    "\n",
    "gt - датафрейм содержащий:  \n",
    "* имена файлов - поле filename\n",
    "* класс знака - поле sign_class\n",
    "* флаг присутствия знака при работе с датасетом - IsPresent. Предполагается, что вместо удаления записи, будет устанавливатся этот флаг, включающий/не влючающий знак в выборку\n",
    "* в какой набор включен знак - поле Set $\\in$ $\\{train, valid, test\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4F3G3CsMuscG",
    "outputId": "b83c41e8-2122-4eac-c866-5966c8bb6cf4"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import pathlib\n",
    "import shutil\n",
    "import cv2\n",
    "import PIL\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "\n",
    "%cd adas_system/notebooks\n",
    "\n",
    "IN_COLAB = False\n",
    "USE_COLAB_GPU = False\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    USE_COLAB_GPU = True\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    if not os.path.isfile('1_ClassifierResearch.ipynb'):\n",
    "        !git clone --branch 9_SignDetector https://github.com/lsd-maddrive/adas_system.git\n",
    "\n",
    "    !gdown --id 1-K3ee1NbMmx_0T5uwMesStmKnZO_6mWi\n",
    "    %cd adas_system/notebooks\n",
    "    !mkdir ../data/R_MERGED\n",
    "    !unzip -q -o /content/R_MERGED.zip -d ./../data/\n",
    "\n",
    "except:\n",
    "    if IN_COLAB:\n",
    "        print('[!]YOU ARE IN COLAB, BUT DIDNT MOUND A DRIVE. Model wont be synced[!]')\n",
    "\n",
    "        if not os.path.isfile('1_ClassifierResearch.ipynb'):\n",
    "            !git clone --branch 9_SignDetector https://github.com/lsd-maddrive/adas_system.git\n",
    "            !gdown --id 1-K3ee1NbMmx_0T5uwMesStmKnZO_6mWi\n",
    "            %cd adas_system/notebooks\n",
    "            !mkdir ../data/R_MERGED\n",
    "            !unzip -q -o /content/R_MERGED.zip -d ./../data/\n",
    "\n",
    "        IN_COLAB = False\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "###\n",
    "import nt_helper\n",
    "from nt_helper.helper_utils import *\n",
    "###\n",
    "\n",
    "TEXT_COLOR = 'black'\n",
    "\n",
    "# Зафиксируем состояние случайных чисел\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (17,10)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xEVqwKlEvGbN"
   },
   "source": [
    "Init dirs, init main vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 810
    },
    "id": "jIdFu3ebuhn2",
    "outputId": "661fa863-10f1-4a98-dc0b-9c2f95ae9941"
   },
   "outputs": [],
   "source": [
    "if not IN_COLAB:\n",
    "    PROJECT_ROOT = pathlib.Path(os.path.join(os.curdir, os.pardir))\n",
    "else:\n",
    "    PROJECT_ROOT = pathlib.Path('..')\n",
    "    \n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "NOTEBOOKS_DIR = PROJECT_ROOT / 'notebooks'\n",
    "\n",
    "gt = pd.read_csv(DATA_DIR / 'RTDS_DATASET.csv')\n",
    "\n",
    "SIGN_TO_NUMBER = pd.read_csv(DATA_DIR / 'sign_to_number.csv', index_col=0).T.to_dict('records')[0]\n",
    "NUMBER_TO_SIGN = pd.read_csv(DATA_DIR / 'number_to_sign.csv', index_col=0).T.to_dict('records')[0]\n",
    "\n",
    "gt['filepath'] = gt['filepath'].apply(lambda x: DATA_DIR / x)\n",
    "GT_SRC_LEN = len(gt.index)\n",
    "display(gt)\n",
    "\n",
    "_, ax = plt.subplots(nrows=3, ncols=1, figsize=(21, 8))\n",
    "LABELS = ['train', 'valid', 'test']\n",
    "\n",
    "for i in range(len(LABELS)):\n",
    "    g = sns.countplot(x='SIGN', \n",
    "                      data=gt[gt['SET']==LABELS[i]],  \n",
    "                      ax=ax[i], \n",
    "                      order=sorted(gt['SIGN'].value_counts().index.tolist())\n",
    "                     )\n",
    "    ax[i].tick_params(labelrotation=90)\n",
    "    ax[i].set_title(LABELS[i])\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-17LNMUCppi"
   },
   "source": [
    "Тестим обучалку: возьмем из трейна по N представителей каждого класса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 961
    },
    "id": "R_uSSaEoCppj",
    "outputId": "a23997e6-6d2a-4b86-d426-95d027f01589"
   },
   "outputs": [],
   "source": [
    "N = 1\n",
    "\n",
    "gt_ = gt[gt[\"SET\"]=='train'].copy()\n",
    "SIGN_SET = set(gt['SIGN'])\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "LE_LOCATION = DATA_DIR / 'le.npy'\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "if os.path.isfile(LE_LOCATION):\n",
    "    le.classes_ = np.load(LE_LOCATION)\n",
    "else:\n",
    "    le.fit_transform(gt_['SIGN'])\n",
    "    np.save(LE_LOCATION, le.classes_)\n",
    "    \n",
    "gt['ENCODED_LABELS'] = le.transform(gt['SIGN'])    \n",
    "gt_['ENCODED_LABELS'] = le.transform(gt_['SIGN'])    \n",
    "\n",
    "nrows, ncols = 7, 6\n",
    "fig = plt.figure(figsize = (16,16))\n",
    "\n",
    "new_mini_df = pd.DataFrame(columns=gt_.columns)\n",
    "\n",
    "for idx, sign_class in enumerate(SIGN_SET):\n",
    "    \n",
    "    instances = gt_[gt_['SIGN'] == sign_class].sample(N)\n",
    "    # print(instances)\n",
    "    new_mini_df = new_mini_df.append(instances)\n",
    "    # new_mini_df.loc[len(new_mini_df)] = instance.iloc[0]\n",
    "    path = str(instances['filepath'].sample(1).values[0])\n",
    "    # print(path)\n",
    "    sign = instances['SIGN'].sample(1).values[0]\n",
    "    img = cv2.imread(path)\n",
    "    \n",
    "    ax = fig.add_subplot(nrows, ncols, idx+1)\n",
    "    \n",
    "    ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), aspect=1)\n",
    "    ax.set_title('ENCODED: ' + str(le.transform([sign_class])[0]) + '\\nDECODED: ' + str(sign_class) + '\\nSIGN: ' + str(NUMBER_TO_SIGN[sign_class]))\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-kQYIuafCppj"
   },
   "source": [
    "new_mini_df хранит только по единственному представителю знаков.\n",
    "\n",
    "Создадим загрузчик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zBVIaE6pTQV7"
   },
   "outputs": [],
   "source": [
    "class SignDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, set_label, img_size=64, transform=None, le=None):\n",
    "        \n",
    "        if isinstance(img_size, int):\n",
    "            img_size = (img_size, img_size)\n",
    "        \n",
    "        self.img_size = img_size\n",
    "        self.df = df[df['SET']==set_label]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df.index)\n",
    "    \n",
    "    def __getitem__(self, index): \n",
    "        label = int(self.df.iloc[index]['ENCODED_LABELS'])\n",
    "        path = str(self.df.iloc[index]['filepath'])\n",
    "        img = cv2.imread(path)\n",
    "        img = cv2.resize(img, self.img_size, interpolation=cv2.INTER_LANCZOS4)\n",
    "        img_tnsr = torch.Tensor.permute(torch.Tensor(img), [2, 0, 1]).div(255)\n",
    "        return img_tnsr, label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IT1astLanm9u"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def train_epoch(model, loader, loss_op, optim, device):\n",
    "\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    \n",
    "    losses = 0\n",
    "    rights = 0\n",
    "    pbar = tqdm(enumerate(loader),\n",
    "                total=len(loader), \n",
    "                position=0,\n",
    "                leave=False)\n",
    "    \n",
    "    for idx, (data, target) in pbar:\n",
    "                    \n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        pred = model(data)\n",
    "\n",
    "        iter_right_count = get_rights_count(pred, target).cpu().numpy()\n",
    "        rights += iter_right_count\n",
    "        \n",
    "        loss = loss_op(pred, target)\n",
    "        losses += loss.item()\n",
    "        \n",
    "        # Gradient descent\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        pbar.set_description(\"TRAIN: INSTANT LOSS %f INSTANT ACCUR: %.4f\" % \n",
    "                             (round(loss.item(), 3), \n",
    "                              iter_right_count / len(target))\n",
    "                            )\n",
    "        \n",
    "    return losses\n",
    "\n",
    "def get_rights_count(y_pred, y_true):\n",
    "    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n",
    "    correct_pred = (y_pred_tags == y_true).float()\n",
    "    acc = correct_pred.sum()\n",
    "    return acc\n",
    "\n",
    "def valid_epoch(model, loader, device):\n",
    "    \n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    rights = 0\n",
    "    pbar = tqdm(enumerate(loader),\n",
    "                total=len(loader),\n",
    "                position=0,\n",
    "                leave=False)\n",
    "        \n",
    "    for idx, (data, target) in pbar:\n",
    "        data = data.to(device)\n",
    "            \n",
    "        target = target.to(device)\n",
    "        pred = model(data)\n",
    "\n",
    "        iter_right_count = get_rights_count(pred, target).cpu().numpy()\n",
    "        rights += iter_right_count\n",
    "        \n",
    "        pbar.set_description(\"VALIDATION: INSTANT ACCUR: %.4f\" % (iter_right_count / len(target)))\n",
    "        \n",
    "    return rights / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138,
     "referenced_widgets": [
      "367f7b421d0c493a9ce5318c91c8b225",
      "88ab3de1156c4de3a7996f2e15932b69",
      "54790b920f324da3bc2c34ccfc6a7e23",
      "1bbb2c4ee7a24d2197357c36f1d13ce1",
      "1653ca1132cf4686aa8cd8395b8e76f5",
      "8d8175e90a144fa38df7e283de589fcb",
      "69e6add3b05748d99d822b55fb0f4b9b",
      "d183eba815574819b6b4a3cbd31fd7e0",
      "9c8a643eabe8498fb37afe0cc01e534e",
      "416c4ef413784c0c878ae35fb46b5db0",
      "557cb3ab73c24335af3ae2fee1cee5ff",
      "e94a655834f0492d8d81ba206b5ae926",
      "55f4b7ae6c7647e6bd42a1a08f1498a9",
      "d9f5c9cbf79c49e09224dbb14d566946",
      "752350478097441dbfd145a0afc9279f",
      "fcb33cb0a57b4fde958e86524094274d",
      "7eb8840022c54a40b2bc2da6c9aaf4a6",
      "e33fc4b71fa749fcafb9812510638b3b",
      "7f52ddcc02184c569870eef8f1b79db3",
      "c77c77abd2be44c49f017246bdd870e9",
      "a9b81705ae6a454dbac83c90464f8f0c",
      "20fc5aedd4c64a65beb60996fcdf95af",
      "4058af29dc2f4c8fa94a9c434e60871a",
      "f7e181c2a19c429f80ebfdd326464c94",
      "665292736855469e972355aba1999c76",
      "0b06d82012c14a6b998fa7af5fd47c60",
      "4ae34ca9883745bc853343963f5f44bb",
      "baffe9b68d1c485ebd28631c9b5b2096",
      "d844fc15215f444ba26f4df6d57a9f05",
      "7921427aaa624182adcd7d7310342351",
      "8a55179676e7461d930d91c64adcf4cf",
      "0556720a1555464484a0a43bc0242115",
      "3d8c3d1c67d64e7e92efdd879ef3b9b9",
      "8c2c8b1963ad4c3fa68d70357c40f3d1",
      "8b936451b63e4651a54f465b73b7bed0",
      "3d5d94e0a5e842f29b8213d430fee86b",
      "9ea74c01eaf046a7a5818330426b620c",
      "2682dc82e3014a5da41b8245be6f979e",
      "4f186647d83441c8b37392aba4671992",
      "21a63a8117514ab7b8d7dfbf7a5cb309",
      "e6412e6bff8b4e0fb2188db638fc3cfa",
      "f099d9a3a25945dc8b75c92984eb6ed5",
      "169615787de54beb87ab8eec5fe17354",
      "23440b010110495e9a8ac0c8577c4751",
      "bb3d92483cca479499fabddfdf597d8b",
      "9d0ed09e25bf4b5783aec61c72fc71d0",
      "426f25016dd54799bd811463e18d72c1",
      "d8b9f7279f57412895fdb3eb04877511",
      "e608e155490b422293d1a057248e3b28",
      "61f976b454fc440c8831b372d9a8d181",
      "55e264497239440694773b54ef17f944",
      "2de504cf0f524809aec230bc42df2e0f",
      "c5f4ad6080e44335b297f73107921fe6",
      "abfeedd1350545298dfa16606fb669fe",
      "831873c8824a4fac840dff2062913070",
      "0ae072e0ca794af19d6aaf55797b31db",
      "7764cc38534a43029dd434eaa84b0d4f",
      "d6d3a2573b254ffd8ef18691fc117352",
      "64d62d49fb2a4503b444fe22aa2ff0f0",
      "c71e5fa352ed4acc89573e5be2ea784f",
      "748eafbaaadd47569785edd2735f79ba",
      "871c88edd5ee452fa91b9ab9930887d7",
      "4fafc4ee3cd04c6bafce46c944ba79d5",
      "3e8145b2fb494e11b973687ed2db69b0",
      "d0c7452601ae42dea3434ccfcbc7c747",
      "e67c05318c52434eb119bf92c6684fcc",
      "a57bbd95bb8b4e5491e4904985c168c6",
      "48ac89f4f4c44f8dbaa6ad3b063904ca",
      "db67aefd8f1a4ed388a9cbbcbe69debe",
      "60ce6e74c42d44419eca4d50cffc6aea",
      "1dc34ba2153245548680219c109a2a8c",
      "3d45f07faa184aa58ec6ef87601a3281",
      "dba1a99bf71c42569b5f8a32d2cbb8d4",
      "39aff400ac0f4a6a837c2555081ac82e",
      "74941133cf3c46f89e6f25b0ef1153f2",
      "986cb6264fb84f268deb52697807186b",
      "8a3d3e5efb32490e856b3ca33f57645e",
      "fa7003050413434faa0d31f033a528ab",
      "e5370774e2d54c6ab9747e347bf32628",
      "7a085ff2190c404c8e8be7f6b5b81273",
      "e050b8cfbb1549f8ad63ad453a06e7a8",
      "15631b5be8b946a88aecb8a4d5e301b3",
      "6ce934529b5d4568ad0c416d42c891ce",
      "1e710f2db9e046a9b6572ec485cdfb71",
      "0f56a2b01da249459d0005f7d4d27a64",
      "368889d09594409c9fe8e04adce558aa",
      "eee6d31528574c5b9eb2b4592c869e7b",
      "73002bd897cc476a99aac81bc152d6ad",
      "cf89d25e7be14c51919d327eb54dce4b",
      "6ccefbe598be4671a8d68d1c2de6916e",
      "a6f6ffc565a84518885d1d53dead8a0a",
      "05cd7a6c88144606ba86df8558ea19e7",
      "d3b0feb88369445ba57db1efae8d0cf0",
      "b15869e0522247e1aa71f4056b103e46",
      "8f352bd141da43d387ecb48aac82d8bd",
      "d4e847d4a4054a67b3ac9e8bf26f5b06",
      "b0947bd8a9b046a98e85becf8326e914",
      "79fa97c467984e23b38bf3ac293968ba",
      "a45daf4958344c608b80a9de0662609b",
      "a8394a9ac5df409f86b0bdd1aa487934",
      "716dc434009943e09607686688d793db",
      "9ffe24f5eeac43e0b4eff2166cda68bc",
      "6effaa79950a48c8bf156d20333ef459",
      "d50409ae26d74892b13953829de3f2c7",
      "1dfa83d819684bcb9a98b149a358b788",
      "5b46aedbbd064fdca2d6f4560b30b0a4",
      "03ce012524704e3da3f7a1915ba2d60c",
      "ed76b69239e340e591910b78752ebf41",
      "5d4eae3a6a8c403bbfb897c7fc324730",
      "e2638f28cf8b43feb4ce32e1f2cbd23a",
      "7aaef562e91f4305ab99dc7d8584bb46",
      "7888f5bb2572433c88044422edab2dab",
      "9c1b4e10770944c8853e10816fae077d",
      "09e4989ab7234ead8aacdcca4f59c72d",
      "208526fe5bc441898e3ea77366602de2",
      "a624272f300a4ddeb2a5258f6f5da05a",
      "e8a52633c98e4be996768fd3a6e8c828",
      "e10af3251eb44090abd0f9e764bd28ca",
      "2a457cd45ace4a83adf6f04301bba4dd",
      "4f73d5c911134dc29d05f83faca0b3d4",
      "735fda37f9de4c2194896e618110085b",
      "79fb493ad66942bca71a04ecca7b1a3b",
      "efc501ba8e9d4db89b2c28d7d1d58bac",
      "906c2554b2814b91bf353bb5d5fceead",
      "887d991c55964ffa9552b102f2dd2828",
      "62999fff26284a73b21c0fbd96bfdf79",
      "e97cdfa7baa249389f79cfefb0254cec",
      "d1928796e056432b8e28caff5876a37a",
      "1647b53f1bb143fc9afced8ce78d2922",
      "fd25c19347384260ad914aa930536a94",
      "67c47f1fedc74c9ca25cdc874fa4cbc6",
      "03372b61d9204aa59608b2666afa40d0",
      "b750ba1f012d4ee0ba2b5eb19766c92b",
      "9927e30363494485b199307531162501",
      "876af46d8c8741a68a3aa8c10e9b28e6",
      "fccae16c105c464d864d151d837aede9",
      "2897c48152e6448d9f3014f259738d2f",
      "723e8e2976fe4012a6e610914282adca",
      "b92c206e6a0a4e0b91926d092f94b209",
      "27e98e5fb07f47bfb417fabc4d0aae3e",
      "9ec0f6a349b54bddad31028b3324ace3",
      "dc67dea7002248608209d0e9ca9d50d7",
      "703fdae3ad3d42cc952e54998bafaade",
      "727b48c0b65a41918aae0ecc1d1f40dd",
      "acfd91c7d7464af09e5220d2f303e695",
      "54fa4b886d714001ad1437c6c1293bc2",
      "39483b9bbfe24668b7d148d2d0b3e501",
      "34e514b5c4534654b9e413eed294075b",
      "d142ddf282724cea9262650be6934bc7",
      "2fd726e9fe23485eb8dff2db42853574",
      "9ca4bb584ae64e35aea7aeb20e2eb7b5",
      "626f18d7d8ee4fbba8e4522f1e7786da",
      "d58654a45ac94ea08380e398ed994569",
      "25ce9c9f07814e42ba2374a6f53fc0fc",
      "eb9601c5434447c48d9ccbc63fcbf0be",
      "f108d273ac4745338b73964c21f10bbf",
      "d5064fa5f53a4ae2aa5fbd20e9eb0d03",
      "77087f086989493b94d19fc018e8b9a9",
      "68569cc9035345bd91c574e45bd08196",
      "5d6efa663d69409486e2419de97e3d62",
      "15f856e4399a4ab98c0f89071139263d",
      "76afc7d1f128408eb3fc1250791cc925",
      "d09bbfe9e4dd44ba9d4c761c553f829e",
      "3f432474448d496c92c0395b3182c11c",
      "b7d52d9ddb4a4fa6b7c56bfc29d28ed9",
      "894d5af713654677986715bb914d515c",
      "7d1304724251409c86fcef21fc0c5148",
      "2a8a0f009c4f45ddb5fe910122213b5a",
      "d4cb25f3ff174741bf1236b1916ba37c",
      "c460b2e70bfc40589654b6b8726705ad",
      "adf7784ed656477d88ce41e45e1c2ceb",
      "dddb40e46f5b48e0bbdf6d4d8732083d",
      "8c6845f6b3424eb1bece4a117458d883",
      "6fa4a90af6034e80b8440308a5da4007",
      "b35f861dfe0c4817a31ce7dc5fc1ea23",
      "4b1db20128f04b4b9f5bfd64f819b8e5",
      "a783440a94f54f4dbefa7597f0454f47",
      "bf526ebb69d442e2a6f3c5e17b7e1886",
      "8513877ced7b463c89d8254b9c762df7",
      "d444eaf9c7f74ff3b9ee2e988d48a515",
      "0f3ed1ff8494442089617e1c4f96a836",
      "c79bbb8e8e3d4cc98e1bb53399df6cc6",
      "656c1a1e58564e66b0c514fd3997cf48",
      "7cdede80a97c4795be6000513a6e79db",
      "af9eb36f71b04ed697ff17a58918f367",
      "84980d00e1e34827aa5f9af1ac1ba056",
      "f8a30e2fed9c421cb9c6a8d83e604523",
      "34d39d03646840d3afaad5419ecf1534",
      "9f96b45966f24fc1b6b9a445cc2d8cad",
      "914b7b13f16e4411ac3b73efadd046e0",
      "0a3a3fcaae854d8d9b052298aff19d6e",
      "9f2dc09cd2514f6c8453914eb882b95f",
      "a85b4fb009c74592a876d1d33304fdeb",
      "8232321cdf374f9484fcaf054d277e98",
      "12499442de2a4ef0ad1bf44664bedb9d",
      "8f43316420c840eca2703ad163e3dfa5",
      "425173c726a044d4b4e805a7f8d86fee",
      "e1f2a963532b48a9b708514489e38d5a",
      "9ee1333958884bc285beaae8f805fe65",
      "d9902455a0de4e219c94d22b1b117f19",
      "bbeea3e4feac4795b74f53ee9fb2b613",
      "f664e32d4a254dd98fb322f429f4f0db",
      "ecce40d462064f8bbdb210ba55737171",
      "34ad0b3736724a67ad26f6a63b3c8d4b",
      "0340c84ad8354df78966107998a77ea4",
      "dadbc42c8ff146f196d98be9fd6b416b",
      "25dfbe3bee6e494da0e84578fc0326fe",
      "d3eee0a439f9470496315db65dc55d9a",
      "f9563cf60d5c475282dbc3d9f30e9deb"
     ]
    },
    "id": "OboAl3B5TQV8",
    "outputId": "88d1ce68-eae5-40e4-d74f-e50c6a5b7fad"
   },
   "outputs": [],
   "source": [
    "SHOULD_I_TRAIN = False\n",
    "\n",
    "config = {\n",
    "    'lr': 0.1,\n",
    "    'epochs': 5,\n",
    "}\n",
    "\n",
    "DEFAULT_MODEL_LOCATION = DATA_DIR / 'CLASSIFIER'\n",
    "\n",
    "from torchvision import models\n",
    "model = models.resnet18(pretrained=True)\n",
    "MODEL_CLASSES = len(set(gt['SIGN']))\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(512, MODEL_CLASSES),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "    \n",
    "\n",
    "if os.path.isfile(DEFAULT_MODEL_LOCATION):\n",
    "    model.load_state_dict(torch.load(DEFAULT_MODEL_LOCATION))\n",
    "    print('[+] Model restored from', DEFAULT_MODEL_LOCATION)\n",
    "    \n",
    "model.eval()    \n",
    "\n",
    "loss_op = nn.CrossEntropyLoss().cuda()\n",
    "optim = torch.optim.Adadelta(model.parameters(), lr=config['lr'])\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "img_size = 64\n",
    "train_dataset = SignDataset(gt, 'train', img_size)\n",
    "valid_dataset = SignDataset(gt, 'valid', img_size)\n",
    "\n",
    "num_workers=0\n",
    "if IN_COLAB or USE_COLAB_GPU:\n",
    "    num_workers=4\n",
    "\n",
    "batch_size = 260\n",
    "if IN_COLAB or USE_COLAB_GPU:\n",
    "    batch_size = 2500\n",
    "    \n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        shuffle=False)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        shuffle=False)\n",
    "\n",
    "\n",
    "if SHOULD_I_TRAIN:\n",
    "    pbar = tqdm(range(config['epochs']),\n",
    "                total=config['epochs'],\n",
    "                position=0,\n",
    "                leave=True,\n",
    "                desc='WAITING FOR FIRST EPOCH END...')\n",
    "    \n",
    "    for epoch in pbar:\n",
    "\n",
    "        train_loss = train_epoch(model, train_loader, loss_op, optim, device)\n",
    "        mean_train_acc = valid_epoch(model, train_loader, device)        \n",
    "        mean_valid_acc = valid_epoch(model, valid_loader, device)\n",
    "                \n",
    "        torch.save(model.state_dict(), DEFAULT_MODEL_LOCATION)\n",
    "        model_save_name = 'CLASSIFIER_{}_TRAIN_ACC{:.4f}_VALID_ACC{:.4f}'.format(datetime.now().strftime(\"%m.%d_%H.%M\"),\n",
    "                                                                      mean_train_acc,\n",
    "                                                                      mean_valid_acc)    \n",
    "        \n",
    "        torch.save(model.state_dict(), DATA_DIR / model_save_name)\n",
    "        if IN_COLAB:\n",
    "            shutil.copy2(DATA_DIR / model_save_name, '/content/drive/MyDrive/')\n",
    "\n",
    "        # torch.save(model.state_dict(), DEFAULT_MODEL_LOCATION)\n",
    "        # if IN_COLAB:\n",
    "        #     shutil.copy2(DEFAULT_MODEL_LOCATION, '/content/drive/MyDrive/')\n",
    "            \n",
    "            \n",
    "        pbar.set_description(\"PER EPOCH: TRAIN LOSS: %4f; TRAIN ACCUR %.4f; VALID ACCUR: %.4f\" % (train_loss, \n",
    "                                                                                                   mean_train_acc,\n",
    "                                                                                                   mean_valid_acc)\n",
    "                            )\n",
    "\n",
    "    print(\"END TRAIN ACCUR: %.4f; VALID ACCUR %.4f\" % (mean_train_acc, mean_valid_acc))\n",
    "else:\n",
    "    print('SHOULD I TRAIN == FALSE, SKIP TRAINING')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jbOTGvbpM_QL"
   },
   "source": [
    "DataSet sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iKSYD8h9Cppm"
   },
   "outputs": [],
   "source": [
    "def getNSamplesFromDataSet(ds, N):\n",
    "    random_index = random.sample(range(0, len(ds)), N)\n",
    "    ret = []\n",
    "    for index in random_index:\n",
    "        ret.append(ds[index])\n",
    "    return ret\n",
    "\n",
    "def checkModelOutAndCompareToTargetLabel(pred, target):\n",
    "    '''\n",
    "    ret is prediction right flag, predicted sign, target sign, confidence\n",
    "    '''\n",
    "    isPredictionRight = False\n",
    "    \n",
    "    # transform prediction to sign\n",
    "    argmax = np.argmax(pred)\n",
    "    model_pred_decoded = le.inverse_transform([argmax])[0]\n",
    "    model_pred_sign = NUMBER_TO_SIGN[model_pred_decoded]\n",
    "    \n",
    "    # transform target to sign\n",
    "    decoded_label = le.inverse_transform([target])[0]\n",
    "    target_sign = NUMBER_TO_SIGN[decoded_label]\n",
    "    \n",
    "    if model_pred_decoded == decoded_label:\n",
    "        isPredictionRight = True\n",
    "    \n",
    "    confidence = pred[0][argmax]\n",
    "    \n",
    "    return isPredictionRight, model_pred_sign, target_sign, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "URfAHsQNCppm",
    "outputId": "4e6d26fc-3f6d-45de-9192-7ba5731f3a02"
   },
   "outputs": [],
   "source": [
    "nrows, ncols = 70, 6\n",
    "fig = plt.figure(figsize = (16,200))\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "wrongs = 0\n",
    "\n",
    "test_dataset = SignDataset(gt, 'test', img_size=64)\n",
    "test_samples = getNSamplesFromDataSet(test_dataset, 300)\n",
    "for idx, (img, encoded_label) in enumerate(test_samples):\n",
    "    \n",
    "    pred = model(img[None, ...].to(device)).cpu().detach().numpy()\n",
    "    \n",
    "    # make img from tensor\n",
    "    img = torch.Tensor.permute(img, [1, 2, 0]).numpy()   \n",
    "    \n",
    "    isPredictionRight, model_pred_sign, target_sign, confidence = checkModelOutAndCompareToTargetLabel(pred,\n",
    "                                                                                                       encoded_label\n",
    "                                                                                                      )\n",
    "    \n",
    "    ax = fig.add_subplot(nrows, ncols, idx+1)\n",
    "    ax.patch.set_linewidth('20')\n",
    "    \n",
    "    if isPredictionRight and confidence > 0.9:\n",
    "        ax.patch.set_edgecolor('green')\n",
    "    elif isPredictionRight and confidence > 0.7:\n",
    "        print('low conf for', [(idx+1) // ncols , (idx+1) % ncols])\n",
    "        ax.patch.set_edgecolor('yellow')\n",
    "    else:\n",
    "        if confidence > 0.7:\n",
    "            print('mismatch with high conf for', [(idx+1) // ncols , (idx+1) % ncols])\n",
    "            ax.patch.set_edgecolor('black')\n",
    "        else:\n",
    "            print('mismatch for', [(idx+1) // ncols , (idx+1) % ncols])\n",
    "            ax.patch.set_edgecolor('red')\n",
    "        wrongs += 1\n",
    "        \n",
    "    ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), aspect=1)\n",
    "    \n",
    "    FACT_SIGN = 'FACT: ' + str(target_sign)\n",
    "    PRED_SIGN = '\\nPRED: ' + str(model_pred_sign)\n",
    "    CONF = '(%.3f)' % confidence\n",
    "    \n",
    "    title = FACT_SIGN + PRED_SIGN + CONF\n",
    "    title = ax.set_title(title, fontsize=15)\n",
    "    \n",
    "print('Accuracy:',  1 - wrongs / len(test_samples))\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "1_ClassifierResearch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "3.8.10"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ],
   "remote_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "3.10.5"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
