{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled2.ipynb","provenance":[],"authorship_tag":"ABX9TyP1skU0ab1B9JIfKNdPMa9S"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Sam4lcv8y3zV"},"outputs":[],"source":["import torch\n","import os\n","import numpy as np\n","import scipy.misc as m\n","import imageio as im\n","from PIL import Image, ImageColor\n","from torch.utils import data\n","from torch.utils.data import DataLoader\n","import torch.nn as nn\n","from skimage.transform import rescale, resize, downscale_local_mean\n","import sklearn.metrics as skm\n","import torch.optim as optim\n","import tensorflow as tf\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt  \n","import torch.nn.functional as F\n","import time\n","import cv2"]},{"cell_type":"code","source":[" import torch\n"," torch.cuda.empty_cache()"],"metadata":{"id":"xukJRsSUy_Uk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"id":"sLSZJYcSzCqs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import glob\n","\n","dir_file = '/content/drive/My Drive/a2d2'\n","\n","dir_files = os.listdir(dir_file)\n","print(dir_files)"],"metadata":{"id":"cTWW6yNqzFiX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cam_file_val = '/content/drive/My Drive/a2d2/camera/cam_front_center/val'\n","label_file_val = '/content/drive/My Drive/a2d2/label/cam_front_center/val'\n","cam_file_train = '/content/drive/My Drive/a2d2/camera/cam_front_center/train'\n","label_file_train = '/content/drive/My Drive/a2d2/label/cam_front_center/train'\n","cam_file_test = '/content/drive/My Drive/a2d2/camera/cam_front_center/test'\n","label_file_test = '/content/drive/My Drive/a2d2/label/cam_front_center/test'\n","\n","\n","cam_files_val = []\n","label_files_val = []\n","cam_files_test = []\n","label_files_test = []\n","cam_files_train = []\n","label_files_train = []\n","\n","for file in os.listdir(label_file_val):\n","   if file.endswith(\".png\"):\n","     label_files_val.append(file)\n","\n","for file in os.listdir(cam_file_val):\n","   if file.endswith(\".png\"):\n","     cam_files_val.append(file)\n","\n","for file in os.listdir(label_file_test):\n","   if file.endswith(\".png\"):\n","     label_files_test.append(file)\n","\n","for file in os.listdir(cam_file_test):\n","   if file.endswith(\".png\"):\n","     cam_files_test.append(file)\n","\n","for file in os.listdir(label_file_train):\n","   if file.endswith(\".png\"):\n","     label_files_train.append(file)\n","\n","for file in os.listdir(cam_file_train):\n","   if file.endswith(\".png\"):\n","     cam_files_train.append(file)\n","\n","cam_files_val.sort()\n","label_files_val.sort()\n","\n","cam_files_test.sort()\n","label_files_test.sort()\n","\n","cam_files_train.sort()\n","label_files_train.sort()\n","\n","print(cam_files_val)\n","print(label_files_val)\n","print(\"cam files val\",len(cam_files_val))\n","print(\"label files val\",len(label_files_val))\n","\n","print(cam_files_test)\n","print(label_files_test)\n","print(\"cam files test\",len(cam_files_test))\n","print(\"label files test\",len(label_files_test))\n","\n","print(cam_files_train)\n","print(label_files_train)\n","print(\"cam files train\",len(cam_files_train))\n","print(\"label files train\",len(label_files_train))"],"metadata":{"id":"x12WnbUzzf65"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab.patches import cv2_imshow\n","img = cv2.imread(cam_file_train + '/' + cam_files_train[22])\n","img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","gt_img = cv2.imread(label_file_train + '/' + label_files_train[22])\n","gt_img = cv2.cvtColor(gt_img, cv2.COLOR_BGR2RGB)\n","\n","\n","plt.figure(figsize=(16, 8))\n","plt.subplot(1,2,1)\n","plt.imshow(img)\n","plt.axis('off')\n","plt.subplot(1,2,2)\n","plt.imshow(gt_img)\n","plt.axis('off')\n","plt.show()"],"metadata":{"id":"b66f6iCHzuhl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["path_data = \"/content/drive/My Drive/a2d2\"\n","\n","list_dir = []\n","\n","learning_rate = 1e-6\n","train_epochs = 10\n","n_classes = 4\n","batch_size = 2\n","num_workers = 2\n","\n","list_dir = os.listdir(path_data)\n","print(list_dir)"],"metadata":{"id":"d4xdjwjMz9Qs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def recursive_glob(rootdir=\".\", suffix=\"\"):\n","    return [\n","        os.path.join(looproot, filename)\n","        for looproot, _, filenames in os.walk(rootdir)\n","        for filename in filenames\n","        if filename.endswith(suffix)\n","    ]"],"metadata":{"id":"H9vNVi_l0CBu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class A2D2Loader(data.Dataset):\n","    colors =[\n","        [255,  255, 255],     \n","        [255,193, 37],\n","        [128, 0,255],\n","        [255, 0,255],\n","    ]\n","    n_classes=4\n","    label_colours = dict(zip(range(n_classes), colors))\n","\n","    def __init__(\n","        self,\n","        root,\n","        split=\"train\",\n","        is_transform=True,\n","        img_size=(512, 256)\n","    ):\n","        self.root = root\n","        self.split = split\n","        self.is_transform = is_transform\n","        self.n_classes = 4\n","        self.img_size = img_size if isinstance(img_size, tuple) else (img_size, img_size)\n","        self.files = {}\n","\n","        # makes it: /content/drive/My Drive/a2d2/20181108_091945 + /camera/cam_front_center/ + train \n","        self.images_base = os.path.join(self.root, \"camera/cam_front_center\", self.split)\n","        self.annotations_base = os.path.join(self.root, \"label/cam_front_center\", self.split)\n","        \n","        # contains list of all pngs inside all different folders. Recursively iterates \n","        self.files[split] = recursive_glob(rootdir=self.images_base, suffix=\".png\")\n","\n","        #  так лучше было не делать, кажется)))))\n","\n","        self.void_classes =  [11, 26, 38, 43, 45, 46, 48, 52, 56, 60, 62, 75, 76, 86, 88, 91, 97, 104, 107, 112, 113, 117, 120, 128, 133, 135, 137, 138, 145, 146, 150, 151, 156, 157, 163, 165, 167, 179, 180, 186, 190, 215, 226, 228, 230, 236, 237, 249]\n","        \n","        self.valid_classes = [255, 194, 67, 105]\n","        \n","        \n","        self.class_names = [\n","            \"unlabelled\",\n","            \"Solid line\",\n","            \"Dashed line\",\n","            \"RD normal street\",\n","        ]\n","        \n","        # for void_classes; useful for loss function\n","        self.ignore_index = 255\n","        \n","        self.class_map = dict(zip(self.valid_classes, range(self.n_classes)))\n","\n","        if not self.files[split]:\n","            raise Exception(\"No files for split=[%s] found in %s\" % (split, self.images_base))\n","        \n","        # prints number of images found\n","        print(\"Found %d %s images\" % (len(self.files[split]), split))\n","\n","    def __len__(self):\n","        return len(self.files[self.split])\n","\n","    def __getitem__(self, index):\n","        # path of image\n","        img_path = self.files[self.split][index].rstrip()\n","        # path of label\n","        lbl_path = os.path.join(\n","            self.annotations_base,\n","            os.path.basename(img_path)[:14] + \"_label_\" + os.path.basename(img_path)[-25:],\n","        )\n","        # read image\n","        img = im.imread(img_path)\n","        # convert to numpy array\n","        img = np.array(img, dtype=np.uint8)\n","\n","        # read label\n","        lbl = Image.open(lbl_path).convert('L')\n","        # encode using encode_segmap function\n","        lbl = self.encode_segmap(np.array(lbl, dtype=np.uint8))\n","\n","        if self.is_transform:\n","            img, lbl = self.transform(img, lbl)\n","        \n","        return img, lbl\n","\n","    def transform(self, img, lbl):       \n","        # Image resize; I think imresize outputs in different format than what it received\n","        img =np.array(Image.fromarray(img).resize((self.img_size[0], self.img_size[1])))  # uint8 with RGB mode\n","        # change to BGR\n","        img = img[:, :, ::-1]  # RGB -> BGR\n","        # change data type to float64\n","        img = img.astype(np.float64)\n","        # subtract mean\n","        # NHWC -> NCHW\n","        img = img.transpose(2, 0, 1)\n","\n","        \n","        classes = np.unique(lbl)\n","        lbl = lbl.astype(float)\n","        lbl = np.array(Image.fromarray(lbl).resize((self.img_size[0], self.img_size[1]), resample = Image.NEAREST))\n","        lbl = lbl.astype(int)\n","\n","        if not np.all(classes == np.unique(lbl)):\n","            print(\"WARN: resizing labels yielded fewer classes\")\n","\n","        if not np.all(np.unique(lbl[lbl != self.ignore_index]) < self.n_classes):\n","            print(\"after det\", classes, np.unique(lbl))\n","            raise ValueError(\"Segmentation map contained invalid class values\")\n","\n","        img = torch.from_numpy(img).float()\n","        lbl = torch.from_numpy(lbl).long()\n","\n","        return img, lbl\n","      \n","    def decode_segmap(self, temp):\n","        r = temp.copy()\n","        g = temp.copy()\n","        b = temp.copy()\n","        for l in range(0, self.n_classes):\n","            r[temp == l] = self.label_colours[l][0]\n","            g[temp == l] = self.label_colours[l][1]\n","            b[temp == l] = self.label_colours[l][2]\n","\n","        rgb = np.zeros((temp.shape[0], temp.shape[1], 3))\n","        rgb[:, :, 0] = r / 255.0\n","        rgb[:, :, 1] = g / 255.0\n","        rgb[:, :, 2] = b / 255.0\n","        return rgb\n","\n","\n","    def encode_segmap(self, mask):\n","\n","        for _voidc in self.void_classes:\n","            mask[mask == _voidc] = self.ignore_index\n","        for _validc in self.valid_classes:\n","            mask[mask == _validc] = self.class_map[_validc]\n","        return mask"],"metadata":{"id":"u0lUTE0z0Gca"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["path_data = \"/content/drive/My Drive/a2d2/\"\n","\n","train_data = A2D2Loader(\n","    root = path_data, \n","    split='train'\n","    )\n","\n","val_data = A2D2Loader(\n","    root = path_data, \n","    split='val'\n","    )\n","\n","test_data = A2D2Loader(\n","    root = path_data, \n","    split='test'\n","    )\n","\n","train_loader = DataLoader(\n","    train_data,\n","    batch_size = batch_size,\n","    shuffle=True,\n","    num_workers = num_workers,\n",")\n","\n","val_loader = DataLoader(\n","    val_data,\n","    batch_size = batch_size,\n","    num_workers = num_workers,\n",")\n","\n","test_loader = DataLoader(\n","    test_data,\n","    batch_size = batch_size,\n","    num_workers = num_workers,\n",")\n","\n","print(type(train_data))\n","print(type(train_loader))"],"metadata":{"id":"Eb42L_qz0etc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Up_Sample_Conv(nn.Module):\n","    def __init__(self, ch_in, ch_out):\n","        super(Up_Sample_Conv, self).__init__()\n","        self.up = nn.Sequential(\n","            nn.Upsample(scale_factor=2), # Nearest neighbour for upsampling are two \n","            nn.Conv2d(ch_in, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\n","            nn.BatchNorm2d(ch_out),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","    def forward(self, x):\n","        x = self.up(x)\n","        return x\n","\n","    \n","class Repeat(nn.Module):\n","    def __init__(self, ch_out):\n","        super(Repeat, self).__init__()\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(ch_out, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\n","            nn.BatchNorm2d(ch_out),\n","            nn.ReLU(inplace=True)) \n","\n","    def forward(self, x):\n","        for i in range(2):\n","            if i == 0:\n","                x_rec = self.conv(x)\n","            x_rec = self.conv(x + x_rec)\n","        return x_rec\n","\n","class RR_Conv(nn.Module):\n","    def __init__(self, ch_in, ch_out):\n","        super(RR_Conv, self).__init__()\n","        self.Repeat_block = nn.Sequential(Repeat(ch_out),Repeat(ch_out))\n","        self.Conv = nn.Conv2d(ch_in, ch_out, 1, 1, 0)\n","\n","    def forward(self, input_img):\n","        input_img = self.Conv(input_img)\n","        conv_input_img = self.Repeat_block(input_img)\n","        return input_img + conv_input_img \n","    \n","############\n","############\n","\n","class R2U_Net(nn.Module):\n","#output_ch=n_classes\n","    def __init__(self, img_ch=3, output_ch=4):\n","        super(R2U_Net, self).__init__()\n","        \n","        self.MaxPool = nn.MaxPool2d(kernel_size=2, stride=2)\n","        \n","        self.channel_1 = 64 \n","        self.channel_2 = 2*self.channel_1\n","        self.channel_3 = 2*self.channel_2\n","        self.channel_4 = 2*self.channel_3\n","        self.channel_5 = 2*self.channel_4\n","        \n","        # For new layer added\n","        self.channel_6 = 2*self.channel_5\n","        \n","        self.channels = [self.channel_1, self.channel_2, self.channel_3, self.channel_4, self.channel_5, self.channel_6]\n","            \n"," \n","        self.Layer1 = RR_Conv(img_ch, self.channels[0])\n","        self.Layer2 = RR_Conv(self.channels[0], self.channels[1])\n","        self.Layer3 = RR_Conv(self.channels[1], self.channels[2])\n","        self.Layer4 = RR_Conv(self.channels[2], self.channels[3])\n","        self.Layer5 = RR_Conv(self.channels[3], self.channels[4])\n","        self.Layer6 = RR_Conv(self.channels[4], self.channels[5]) \n","\n","        self.DeConvLayer6 = Up_Sample_Conv(self.channels[5], self.channels[4]) \n","        self.DeConvLayer5 = Up_Sample_Conv(self.channels[4], self.channels[3])\n","        self.DeConvLayer4 = Up_Sample_Conv(self.channels[3],self.channels[2])\n","        self.DeConvLayer3 = Up_Sample_Conv(self.channels[2], self.channels[1])\n","        self.DeConvLayer2 = Up_Sample_Conv(self.channels[1], self.channels[0])\n","        \n","        self.Up_Layer6 = RR_Conv(self.channels[5], self.channels[4]) \n","        self.Up_Layer5 = RR_Conv(self.channels[4], self.channels[3])\n","        self.Up_Layer4 = RR_Conv(self.channels[3], self.channels[2])\n","        self.Up_Layer3 = RR_Conv(self.channels[2], self.channels[1])\n","        self.Up_Layer2 = RR_Conv(self.channels[1], self.channels[0])\n","        \n","        self.Conv = nn.Conv2d(self.channels[0], output_ch, kernel_size=1, stride=1, padding=0)        \n","        \n","    def forward(self, x):\n","        conv1 = self.Layer1(x)\n","        mp1 = self.MaxPool(conv1)\n","        conv2 = self.Layer2(mp1)\n","        mp2 = self.MaxPool(conv2)\n","        conv3 = self.Layer3(mp2)\n","        mp3 = self.MaxPool(conv3)\n","        conv4 = self.Layer4(mp3)\n","        mp4 = self.MaxPool(conv4)\n","        conv5 = self.Layer5(mp4)\n","        mp5 = self.MaxPool(conv5)\n","        conv6 = self.Layer6(mp5)\n","\n","        deconv6 = self.DeConvLayer6(conv6)\n","        deconv6 = torch.cat((conv5, deconv6), dim=1)\n","        deconv6 = self.Up_Layer6(deconv6)\n","\n","        \n","        deconv5 = self.DeConvLayer5(deconv6)\n","        deconv5 = torch.cat((conv4, deconv5), dim=1)\n","        deconv5 = self.Up_Layer5(deconv5)\n","        deconv4 = self.DeConvLayer4(deconv5)\n","        deconv4 = torch.cat((conv3, deconv4), dim=1)\n","        deconv4 = self.Up_Layer4(deconv4)\n","        deconv3 = self.DeConvLayer3(deconv4)\n","        deconv3 = torch.cat((conv2, deconv3), dim=1)\n","        deconv3 = self.Up_Layer3(deconv3)\n","        deconv2 = self.DeConvLayer2(deconv3)\n","        deconv2 = torch.cat((conv1, deconv2), dim=1)\n","        deconv2 = self.Up_Layer2(deconv2)\n","        deconv1 = self.Conv(deconv2)\n","\n","        return deconv1"],"metadata":{"id":"LTicIqcM0jfC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import itertools\n","import os\n","\n","import matplotlib.pylab as plt\n","import numpy as np\n","\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","\n","print(\"TF version:\", tf.__version__)\n","print(\"Hub version:\", hub.__version__)\n","print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"],"metadata":{"id":"PzlMOfhe0snz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model = R2U_Net().to(device)"],"metadata":{"id":"lZIhIHkJ0zOH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.load_state_dict(torch.load(\"/content/drive/My Drive/a2d2_13_06_22.h5\"))\n","model.eval()"],"metadata":{"id":"uXPTzWSk03ps"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","def cross_entropy2d(input, target, weight=None, reduction='mean'):\n","    n, c, h, w = input.size()\n","    nt, ht, wt = target.size()\n","\n","    if h != ht and w != wt:  # upsample labels\n","        input = F.interpolate(input, size=(ht, wt), mode=\"bilinear\", align_corners=True)\n","\n","    input = input.transpose(1, 2).transpose(2, 3).contiguous().view(-1, c)\n","    target = target.view(-1)\n","    loss = F.cross_entropy(\n","        input, target, weight=weight, reduction=reduction, ignore_index=255\n","    )\n","    return loss"],"metadata":{"id":"ai1U57j2076X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_metrics(gt_label, pred_label):\n","    #Accuracy Score\n","    acc = skm.accuracy_score(gt_label, pred_label, normalize=True)\n","    \n","    #Jaccard Score/IoU\n","    js = skm.jaccard_score(gt_label, pred_label, average='micro')\n","    \n","    result_gm_sh = [acc, js]\n","    return(result_gm_sh)\n","\n","class runningScore(object):\n","    def __init__(self, n_classes):\n","        self.n_classes = n_classes\n","        self.confusion_matrix = np.zeros((n_classes, n_classes))\n","\n","    def _fast_hist(self, label_true, label_pred, n_class):\n","        mask = (label_true >= 0) & (label_true < n_class)\n","        hist = np.bincount(\n","            n_class * label_true[mask].astype(int) + label_pred[mask], minlength=n_class ** 2\n","        ).reshape(n_class, n_class)\n","        return hist\n","\n","    def update(self, label_trues, label_preds):\n","        for lt, lp in zip(label_trues, label_preds):\n","            self.confusion_matrix += self._fast_hist(lt.flatten(), lp.flatten(), self.n_classes)\n","\n","    def get_scores(self):\n","        # confusion matrix\n","        hist = self.confusion_matrix \n","\n","        TP = np.diag(hist)\n","        TN = hist.sum() - hist.sum(axis = 1) - hist.sum(axis = 0) + np.diag(hist)\n","        FP = hist.sum(axis = 1) - TP\n","        FN = hist.sum(axis = 0) - TP\n","        \n","        \n","        # Specificity: TN / TN + FP\n","        specif_cls = (TN) / (TN + FP + 1e-7)\n","        specif = np.nanmean(specif_cls)\n","        \n","        # Senstivity/Recall: TP / TP + FN\n","        sensti_cls = (TP) / (TP + FN + 1e-7)\n","        sensti = np.nanmean(sensti_cls)\n","        \n","        # Precision: TP / (TP + FP)\n","        prec_cls = (TP) / (TP + FP + 1e-7)\n","        prec = np.nanmean(prec_cls)\n","        \n","        # F1 = 2 * Precision * Recall / Precision + Recall\n","        f1 = (2 * prec * sensti) / (prec + sensti + 1e-7)\n","        \n","        return (\n","            {\n","                \"Specificity\": specif,\n","                \"Senstivity\": sensti,\n","                \"F1\": f1,\n","                \"Precision\": prec,\n","            }\n","        )\n","\n","    def reset(self):\n","        self.confusion_matrix = np.zeros((self.n_classes, self.n_classes))"],"metadata":{"id":"ma5wo4qF1C9E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def validate(val_loader, model, epoch_i):\n","    \n","    # tldr: to make layers behave differently during inference (vs training)\n","    model.eval()\n","    running_metrics_val = runningScore(4)\n","    \n","    # empty list to add Accuracy and Jaccard Score Calculations\n","    acc_sh = []\n","    js_sh = []\n","    \n","    with torch.no_grad():\n","        for image_num, (val_images, val_labels) in tqdm(enumerate(val_loader)):\n","            \n","            val_images = val_images.to(device)\n","            val_labels = val_labels.to(device)\n","            \n","            # Model prediction\n","            val_pred = model(val_images)\n","            \n","            pred = val_pred.data.max(1)[1].cpu().numpy()\n","            gt = val_labels.data.cpu().numpy()\n","            \n","            # Updating Mertics\n","            running_metrics_val.update(gt, pred)\n","            sh_metrics = get_metrics(gt.flatten(), pred.flatten())\n","            acc_sh.append(sh_metrics[0])\n","            js_sh.append(sh_metrics[1])\n","\n","            val_pred = model(val_images)\n","              \n","            prediction = val_pred.data.max(1)[1].cpu().numpy()\n","            ground_truth = val_labels.data.cpu().numpy()\n","\n","            if image_num == 0:\n","            \n","            # Model Prediction\n","              decoded_pred = val_data.decode_segmap(prediction[0])\n","              plt.imshow(decoded_pred)\n","              plt.axis('off')\n","              plt.show()\n","              plt.clf()\n","            \n","            # Ground Truth\n","              decode_gt = val_data.decode_segmap(ground_truth[0])\n","              plt.imshow(decode_gt)\n","              plt.axis('off')\n","              plt.show()\n","                               \n","    score = running_metrics_val.get_scores()\n","    running_metrics_val.reset()\n","    \n","    acc_s = sum(acc_sh)/len(acc_sh)\n","    js_s = sum(js_sh)/len(js_sh)\n","    score[\"acc\"] = acc_s\n","    score[\"js\"] = js_s\n","    \n","    print(\"Different Metrics were: \", score)  \n","    return(score)"],"metadata":{"id":"oIQSaXPu1JwC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","\n","    loss_all_epochs = []\n","    Specificity_ = []\n","    Senstivity_ = []\n","    F1_ = []\n","    acc_ = []\n","    js_ = []\n","    Precision = []\n","    t = []\n","    \n","    for epoch_i in range(train_epochs):\n","        # train\n","        print(f\"Epoch {epoch_i + 1}\\n-------------------------------\")\n","        t1 = time.time()\n","        loss_i = train(train_loader, model, optimizer, epoch_i, train_epochs)\n","        loss_all_epochs.append(loss_i)\n","        t2 = time.time()\n","        print(\"It took: \", t2-t1, \" unit time\")\n","\n","        # metrics \n","        dummy_list = validate(val_loader, model, epoch_i)   \n","        \n","        # Add metrics to empty list\n","        t.append(t2-t1)\n","        Specificity_.append(dummy_list[\"Specificity\"])\n","        Senstivity_.append(dummy_list[\"Senstivity\"])\n","        F1_.append(dummy_list[\"F1\"])\n","        acc_.append(dummy_list[\"acc\"])\n","        js_.append(dummy_list[\"js\"])\n","        Precision.append(dummy_list[\"Precision\"])"],"metadata":{"id":"LDmjW0231RZL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.save(model.state_dict(),\"/content/drive/My Drive/a2d2_13_06_22.h5\")"],"metadata":{"id":"CqXmlHbD1Xv5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss_1d_list = [item for sublist in loss_all_epochs for item in sublist]\n","loss_list_numpy = []\n","for i in range(len(loss_1d_list)):\n","    z = loss_1d_list[i].cpu().detach().numpy()\n","    loss_list_numpy.append(z)\n","plt.xlabel(\"Images used in training epochs\")\n","plt.ylabel(\"Cross Entropy Loss\")\n","plt.plot(loss_list_numpy)\n","plt.show()"],"metadata":{"id":"kxmyfLGk1bR8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.clf()\n","\n","x = [i for i in range(1, train_epochs + 1)]\n","\n","# plot 5 metrics: Specificity, Senstivity, F1 Score, Accuracy, Jaccard Score\n","plt.plot(x,Specificity_, label='Specificity')\n","plt.plot(x,Senstivity_, label='Senstivity')\n","plt.plot(x,F1_, label='F1 Score')\n","plt.plot(x,acc_, label='Accuracy')\n","plt.plot(x,js_, label='Jaccard Score')\n","\n","plt.grid(linestyle = '--', linewidth = 0.5)\n","\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Score\")\n","plt.legend()\n","plt.show()"],"metadata":{"id":"hCSlXWRq1fbR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.eval()\n","\n","Specificity_ = []\n","Senstivity_ = []\n","F1_ = []\n","acc_ = []\n","js_ = []\n","Precision = []\n","t = []\n","    \n","with torch.no_grad():\n","    for image_num, (test_images, test_labels) in tqdm(enumerate(test_loader)):\n","\n","        test_images = test_images.to(device)\n","        test_labels = test_labels.to(device)\n","        \n","        # model prediction\n","        test_pred = model(test_images)\n","    \n","        prediction = test_pred.data.max(1)[1].cpu().numpy()\n","        ground_truth = test_labels.data.cpu().numpy()\n","\n","        if image_num % 2 == 0:\n","            \n","            # Model Prediction\n","            decoded_pred = test_data.decode_segmap(prediction[0])\n","            plt.imshow(decoded_pred)\n","            plt.show()\n","            plt.clf()\n","            \n","            # Ground Truth\n","            decode_gt = test_data.decode_segmap(ground_truth[0])\n","            plt.imshow(decode_gt)\n","            plt.show()\n","\n","dummy_list = validate(test_loader, model, epoch_i)   \n","        \n","        # Add metrics to empty list above\n","t.append(t2-t1)\n","Specificity_.append(dummy_list[\"Specificity\"])\n","Senstivity_.append(dummy_list[\"Senstivity\"])\n","F1_.append(dummy_list[\"F1\"])\n","acc_.append(dummy_list[\"acc\"])\n","js_.append(dummy_list[\"js\"])\n","Precision.append(dummy_list[\"Precision\"])"],"metadata":{"id":"6b0Bi9ET1j8N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"ULFgZN-x1pWN"},"execution_count":null,"outputs":[]}]}