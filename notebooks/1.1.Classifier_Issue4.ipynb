{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bf48619",
   "metadata": {},
   "source": [
    "Задачи:\n",
    "* Добавление классов в датасет\n",
    "* Оценка показателей классификации\n",
    "* Анализ ошибок классификатора\n",
    "* Дополнение обучения классификации аугментациями https://albumentations.ai/\n",
    "* Дополнение вывода метрик при обучении в Tensorboard train: loss, valid: calssification metrics (по каждому классу и усредненные по macro), loss, а также lr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146139e3",
   "metadata": {},
   "source": [
    "Инклудим все подряд, потому что можем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f58e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import pathlib\n",
    "import shutil\n",
    "import cv2\n",
    "import PIL\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "\n",
    "%cd adas_system/notebooks\n",
    "\n",
    "IN_COLAB = False\n",
    "USE_COLAB_GPU = False\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    USE_COLAB_GPU = True\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    if not os.path.isfile('1_ClassifierResearch.ipynb'):\n",
    "        !git clone --branch 9_SignDetector https://github.com/lsd-maddrive/adas_system.git\n",
    "\n",
    "    !gdown --id 1-K3ee1NbMmx_0T5uwMesStmKnZO_6mWi\n",
    "    %cd adas_system/notebooks\n",
    "    !mkdir ../data/R_MERGED\n",
    "    !unzip -q -o /content/R_MERGED.zip -d ./../data/\n",
    "\n",
    "except:\n",
    "    if IN_COLAB:\n",
    "        print('[!]YOU ARE IN COLAB, BUT DIDNT MOUND A DRIVE. Model wont be synced[!]')\n",
    "\n",
    "        if not os.path.isfile('1_ClassifierResearch.ipynb'):\n",
    "            !git clone --branch 9_SignDetector https://github.com/lsd-maddrive/adas_system.git\n",
    "            !gdown --id 1-K3ee1NbMmx_0T5uwMesStmKnZO_6mWi\n",
    "            %cd adas_system/notebooks\n",
    "            !mkdir ../data/R_MERGED\n",
    "            !unzip -q -o /content/R_MERGED.zip -d ./../data/\n",
    "\n",
    "        IN_COLAB = False\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "###\n",
    "import nt_helper\n",
    "from nt_helper.helper_utils import *\n",
    "###\n",
    "\n",
    "TEXT_COLOR = 'black'\n",
    "\n",
    "# Зафиксируем состояние случайных чисел\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (17,10)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9ad914",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not IN_COLAB:\n",
    "    PROJECT_ROOT = pathlib.Path(os.path.join(os.curdir, os.pardir))\n",
    "else:\n",
    "    PROJECT_ROOT = pathlib.Path('..')\n",
    "    \n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "NOTEBOOKS_DIR = PROJECT_ROOT / 'notebooks'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf11503",
   "metadata": {},
   "source": [
    "1. Добавить инфы в датасет. \n",
    "\n",
    "> RTSD Public не содержит 14 необходимых знаков: 1.6, 1.31, 2.4, 3.21, 3.22, 3.23. 3.24 (90, 100, 110, 120, 130), 3.25, 3.31, 6.3.2.\n",
    "\n",
    "| Знак | Описание | Источник |\n",
    "| ------------- | ------------- | ---- |\n",
    "| 1.6 | Пересечение равнозначных дорог | - |\n",
    "| 1.31 | Туннель | - |\n",
    "| 2.4 | Уступите дорогу | GTSRB Recognition |\n",
    "| 3.21 | Конец запрещения обгона | GTSRB Recognition |\n",
    "| 3.22 | Обгон грузовым автомобилям запрещен | GTSRB Recognition |\n",
    "| 3.23 | Конец запрещения обгона грузовым автомобилям | GTSRB Recognition |\n",
    "| 3.24-90 | Огр 90 | - |\n",
    "| 3.24-100 | Огр 100 | GTSRB Recognition |\n",
    "| 3.24-110 | Огр 110 | - |\n",
    "| 3.24-120 | Огр 120 | GTSRB Recognition |\n",
    "| 3.24-130 | Огр 130 | - |\n",
    "| 3.25 | Конец огр. максимальной скорости | GTSRB Recognition |\n",
    "| 3.31 | Конец всех ограничений | GTSRB Recognition |\n",
    "| 6.3.2 | Зона для разворота | - |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438bad6d",
   "metadata": {},
   "source": [
    "Идея: берем стандартные изображения знаков, обучаем классификатор на них"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e53ae27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pathlib\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "STOCK_SIGNS_CSV_LOCATION = DATA_DIR / 'STOCK_SIGNS.csv'\n",
    "PREFIX = 'STOCK_SIGNS\\\\'\n",
    "STOCK_SIGNS_DIR = pathlib.Path('D:\\\\d_tsw\\\\1_SIGN_CLASSIFIER\\\\data\\\\STOCK_SIGNS')\n",
    "STOCK_SIGNS_LIST = os.listdir(STOCK_SIGNS_DIR)\n",
    "\n",
    "STOCK_SIGNS_DATAFRAME = pd.DataFrame(columns=['filepath', 'SIGN'])\n",
    "for sign in STOCK_SIGNS_LIST:\n",
    "    sign_as_series = pd.DataFrame(data=[[PREFIX + sign, '.'.join(sign.split('.')[:-1])]], \n",
    "                                  columns=STOCK_SIGNS_DATAFRAME.columns\n",
    "                                 )\n",
    "    STOCK_SIGNS_DATAFRAME = STOCK_SIGNS_DATAFRAME.append(sign_as_series, ignore_index=True)\n",
    "\n",
    "STOCK_SIGNS_DATAFRAME.to_csv(STOCK_SIGNS_CSV_LOCATION, index=False)\n",
    "\n",
    "### FIX 5.19.2\n",
    "STOCK_SIGNS_DATAFRAME.loc[STOCK_SIGNS_DATAFRAME['SIGN'] == '5.19.2', 'SIGN'] = '5.19.1'\n",
    "### /FIX 5.19.2\n",
    "STOCK_SIGNS_DATAFRAME[::6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc76aee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "LE_LOCATION = DATA_DIR / 'STOCK_SIGNS_LE.npy'\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "if os.path.isfile(LE_LOCATION):\n",
    "    le.classes_ = np.load(LE_LOCATION, allow_pickle=True)\n",
    "else:\n",
    "    le.fit_transform(STOCK_SIGNS_DATAFRAME['SIGN'])\n",
    "    np.save(LE_LOCATION, le.classes_)\n",
    "\n",
    "STOCK_SIGNS_DATAFRAME['ENCODED_LABELS'] = le.transform(STOCK_SIGNS_DATAFRAME['SIGN'])\n",
    "STOCK_SIGNS_DATAFRAME['filepath'] = STOCK_SIGNS_DATAFRAME['filepath'].apply(lambda x: DATA_DIR / x)\n",
    "STOCK_SIGNS_DATAFRAME[::6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063fc463",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIGN_LIST = list(STOCK_SIGNS_DATAFRAME['SIGN'])\n",
    "\n",
    "nrows, ncols = 20, 6\n",
    "fig = plt.figure(figsize = (16,50))\n",
    "\n",
    "# for idx, sign_class in enumerate(STOCK_SIGNS_DATAFRAME['SIGN']):\n",
    "for idx, row in enumerate(STOCK_SIGNS_DATAFRAME.iterrows()):\n",
    "    instance = row[1]\n",
    "    # print(instance)\n",
    "    # instance = STOCK_SIGNS_DATAFRAME[STOCK_SIGNS_DATAFRAME['SIGN'] == sign_class].iloc[-1]\n",
    "    path = str(instance['filepath'])\n",
    "    sign = instance['SIGN']\n",
    "    encoded_label = instance['ENCODED_LABELS']\n",
    "    \n",
    "    img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "    trans_mask = img[:,:,3] == 0\n",
    "    img[trans_mask] = [random.randrange(0, 256), random.randrange(0, 256), random.randrange(0, 256), 255]\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)\n",
    "    ax = fig.add_subplot(nrows, ncols, idx+1)\n",
    "    \n",
    "    ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), aspect=1)\n",
    "    ax.set_title('ENCODED: ' + str(encoded_label) \n",
    "                 + '\\nSIGN: ' + str(sign)\n",
    "                )\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0ee2e7",
   "metadata": {},
   "source": [
    "Создадим загрузчик с аугментацией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece718d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, set_label=None, img_size=64, hyp=None, transform=None, le=None):\n",
    "        \n",
    "        if isinstance(img_size, int):\n",
    "            img_size = (img_size, img_size)\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        self.transform = transform\n",
    "        \n",
    "        if set_label == None:\n",
    "            self.df = df\n",
    "        else:\n",
    "            self.df = df[df['SET']==set_label]\n",
    "        \n",
    "        self.hyp = hyp\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df.index)\n",
    "    \n",
    "    def __getitem__(self, index): \n",
    "        label = int(self.df.iloc[index]['ENCODED_LABELS'])\n",
    "        path = str(self.df.iloc[index]['filepath'])\n",
    "        \n",
    "        img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "        # check does it contains transparent channel \n",
    "        if img.shape[2] == 4:\n",
    "        # randomize transparent\n",
    "            trans_mask = img[:,:,3] == 0\n",
    "            img[trans_mask] = [random.randrange(0, 256), \n",
    "                               random.randrange(0, 256), \n",
    "                               random.randrange(0, 256), 255]\n",
    "\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)\n",
    "        # /randomize transparent\n",
    "        \n",
    "        img = cv2.resize(img, self.img_size, interpolation=cv2.INTER_LANCZOS4)\n",
    "        \n",
    "        # augment\n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "            \n",
    "        if self.hyp and self.transform:\n",
    "            img, _ =  random_perspective(img, \n",
    "                                      (),\n",
    "                                      degrees=self.hyp['degrees'],\n",
    "                                      translate=self.hyp['translate'],\n",
    "                                      scale=self.hyp['scale'],\n",
    "                                      shear=self.hyp['shear'],\n",
    "                                      perspective=self.hyp['perspective'],\n",
    "                                      border=self.hyp['border'])\n",
    "        \n",
    "        # /augment\n",
    "        img = cv2.resize(img, self.img_size, interpolation=cv2.INTER_LANCZOS4)\n",
    "        \n",
    "        img_tnsr = torch.Tensor.permute(torch.Tensor(img), [2, 0, 1]).div(255)\n",
    "        return img_tnsr, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7554607c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 64\n",
    "\n",
    "import albumentations as A\n",
    "transform = A.Compose([\n",
    "    A.RandomCrop(p=0.2, width=int(img_size*0.9), height=int(img_size*0.9)),\n",
    "    A.Blur(blur_limit=2, p=0.21),\n",
    "    # A.MedianBlur(p=0.2),\n",
    "    A.ToGray(p=0.05),\n",
    "    # A.CLAHE(p=0.01),\n",
    "    # A.RandomBrightnessContrast(p=0.0),\n",
    "    # A.RandomGamma(p=0.01),\n",
    "    A.ImageCompression(quality_lower=40, p=0.2),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.2),\n",
    "    \n",
    "    ]\n",
    ")\n",
    "\n",
    "hyp = {\n",
    "    'degrees': 10,\n",
    "    'translate': 0.0,\n",
    "    'scale': 0.1,\n",
    "    'shear': 5,\n",
    "    'perspective': 0.00001,\n",
    "    'border': (0, 0)\n",
    "}\n",
    "train_dataset = SignDataset(STOCK_SIGNS_DATAFRAME, \n",
    "                            set_label=None, \n",
    "                            img_size=img_size, \n",
    "                            transform=transform, \n",
    "                            hyp=hyp)\n",
    "# train_dataset[0]\n",
    "nrows, ncols = 70, 6\n",
    "fig = plt.figure(figsize = (16,200))\n",
    "\n",
    "for idx, (img, encoded_label) in enumerate(train_dataset):\n",
    "    \n",
    "    img = torch.Tensor.permute(img, [1, 2, 0]).numpy() \n",
    "    ax = fig.add_subplot(nrows, ncols, idx+1)\n",
    "    ax.patch.set_linewidth('20')\n",
    "    \n",
    "    ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), aspect=1)\n",
    "    ax.set_title(str(le.inverse_transform([encoded_label])[0]), fontsize=15)\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ee1574",
   "metadata": {},
   "source": [
    "Тренимся только на этом датасете. Валидационный возьмем gt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d362baf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = pd.read_csv(DATA_DIR / 'RTDS_DATASET.csv')\n",
    "NUMBER_TO_SIGN = pd.read_csv(DATA_DIR / 'number_to_sign.csv', index_col=0).T.to_dict('records')[0]\n",
    "\n",
    "gt['SIGN'] = gt['SIGN'].apply(lambda x: NUMBER_TO_SIGN[x].replace('_', '.').replace('n', ''))\n",
    "gt['filepath'] = gt['filepath'].apply(lambda x: DATA_DIR / x)\n",
    "gt.loc[gt['SIGN'] == '2.3', 'SIGN'] = '2.3.1'\n",
    "gt.loc[gt['SIGN'] == '3.18', 'SIGN'] = '3.18.1'\n",
    "\n",
    "gt['ENCODED_LABELS'] = le.transform(gt['SIGN'])\n",
    "GT_SIGNS_SET  = set(gt['SIGN'])\n",
    "STOCK_SIGNS_SET = set(STOCK_SIGNS_DATAFRAME['SIGN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21e7110",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(GT_SIGNS_SET.difference(STOCK_SIGNS_SET))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40b55d4",
   "metadata": {},
   "source": [
    "STOCK_SIGNS_SET.difference(GT_SIGNS_SET) - пустой, т.к. пак дефолтных изображений содержит всех представителей знаков RTDS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4015b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = SignDataset(gt, \n",
    "                            set_label='test',\n",
    "                            img_size=img_size\n",
    "                           )\n",
    "\n",
    "num_workers=0\n",
    "if IN_COLAB or USE_COLAB_GPU:\n",
    "    num_workers=4\n",
    "\n",
    "batch_size = 260\n",
    "if IN_COLAB or USE_COLAB_GPU:\n",
    "    batch_size = 2500\n",
    "    \n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        shuffle=False)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5022c1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def train_epoch(model, loader, loss_op, optim, device):\n",
    "\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    \n",
    "    losses = 0\n",
    "    rights = 0\n",
    "    pbar = tqdm(enumerate(loader),\n",
    "                total=len(loader), \n",
    "                position=0,\n",
    "                leave=False)\n",
    "    \n",
    "    for idx, (data, target) in pbar:\n",
    "                    \n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        pred = model(data)\n",
    "\n",
    "        iter_right_count = get_rights_count(pred, target).cpu().numpy()\n",
    "        rights += iter_right_count\n",
    "        \n",
    "        loss = loss_op(pred, target)\n",
    "        losses += loss.item()\n",
    "        \n",
    "        # Gradient descent\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        pbar.set_description(\"TRAIN: INSTANT LOSS %f INSTANT ACCUR: %.4f\" % \n",
    "                             (round(loss.item(), 3), \n",
    "                              iter_right_count / len(target))\n",
    "                            )\n",
    "        \n",
    "    return losses\n",
    "\n",
    "def get_rights_count(y_pred, y_true):\n",
    "    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n",
    "    correct_pred = (y_pred_tags == y_true).float()\n",
    "    acc = correct_pred.sum()\n",
    "    return acc\n",
    "\n",
    "def valid_epoch(model, loader, device):\n",
    "    \n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    rights = 0\n",
    "    pbar = tqdm(enumerate(loader),\n",
    "                total=len(loader),\n",
    "                position=0,\n",
    "                leave=False)\n",
    "        \n",
    "    for idx, (data, target) in pbar:\n",
    "        data = data.to(device)\n",
    "            \n",
    "        target = target.to(device)\n",
    "        pred = model(data)\n",
    "\n",
    "        iter_right_count = get_rights_count(pred, target).cpu().numpy()\n",
    "        rights += iter_right_count\n",
    "        \n",
    "        pbar.set_description(\"VALIDATION: INSTANT ACCUR: %.4f\" % (iter_right_count / len(target)))\n",
    "        \n",
    "    return rights / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441bf020",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOULD_I_TRAIN = True\n",
    "\n",
    "config = {\n",
    "    'lr': 0.1,\n",
    "    'epochs': 5,\n",
    "}\n",
    "\n",
    "DEFAULT_MODEL_LOCATION = DATA_DIR / 'CLASSIFIER_ON_STOCK'\n",
    "\n",
    "from torchvision import models\n",
    "model = models.resnet18(pretrained=True)\n",
    "MODEL_CLASSES = len(STOCK_SIGNS_SET)\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(512, MODEL_CLASSES),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "model.eval()    \n",
    "\n",
    "loss_op = nn.CrossEntropyLoss().to(device)\n",
    "optim = torch.optim.Adadelta(model.parameters(), lr=config['lr'])\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "if SHOULD_I_TRAIN:\n",
    "    pbar = tqdm(range(config['epochs']),\n",
    "                total=config['epochs'],\n",
    "                position=0,\n",
    "                leave=True,\n",
    "                desc='WAITING FOR FIRST EPOCH END...')\n",
    "    \n",
    "    for epoch in pbar:\n",
    "\n",
    "        train_loss = train_epoch(model, train_loader, loss_op, optim, device)\n",
    "        mean_train_acc = valid_epoch(model, train_loader, device)        \n",
    "        mean_valid_acc = valid_epoch(model, valid_loader, device)\n",
    "                \n",
    "        torch.save(model.state_dict(), DEFAULT_MODEL_LOCATION)\n",
    "        model_save_name = 'CLASSIFIER_ON_STOCK_{}_TRAIN_ACC{:.4f}_VALID_ACC{:.4f}'.format(datetime.now().strftime(\"%m.%d_%H.%M\"),\n",
    "                                                                      mean_train_acc,\n",
    "                                                                      mean_valid_acc)    \n",
    "        \n",
    "        torch.save(model.state_dict(), DATA_DIR / model_save_name)\n",
    "        if IN_COLAB:\n",
    "            shutil.copy2(DATA_DIR / model_save_name, '/content/drive/MyDrive/')\n",
    "\n",
    "            \n",
    "        pbar.set_description(\"PER EPOCH: TRAIN LOSS: %4f; TRAIN ACCUR %.4f; VALID ACCUR: %.4f\" % (train_loss, \n",
    "                                                                                                   mean_train_acc,\n",
    "                                                                                                   mean_valid_acc)\n",
    "                            )\n",
    "\n",
    "    print(\"END TRAIN ACCUR: %.4f; VALID ACCUR %.4f\" % (mean_train_acc, mean_valid_acc))\n",
    "else:\n",
    "    print('SHOULD I TRAIN == FALSE, SKIP TRAINING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6f3171",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c92d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99da638",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform(img, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df51948a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b834e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "[PREFIX + sign, '.'.join(sign.split('.')[:-1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8d3120",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data=[[PREFIX + sign, '.'.join(sign.split('.')[:-1])]], columns=STOCK_SIGNS_DATAFRAME.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9189165",
   "metadata": {},
   "outputs": [],
   "source": [
    "'.'.join(sign.split('.')[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b3e87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX+sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e266404",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOCK_SIGNS_LIST"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
