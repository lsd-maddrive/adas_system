{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef5ffffc",
   "metadata": {
    "id": "ef5ffffc"
   },
   "source": [
    "Объединенный датасет [FIX MEне доступен по ссылке](*).\n",
    "\n",
    "Положить в папку data содержимое так, чтобы были следующие пути:  \n",
    "* \\$(ROOT_DIR)/data/full-rtsd/...\n",
    "* \\$(ROOT_DIR)/data/full-gt.csv\n",
    "\n",
    "> *gt_Set_NaN.csv - содержит тот же датасет, но значения колонки Set обнулено*\n",
    "\n",
    "gt - датафрейм содержащий:  \n",
    "* имена файлов - поле filename\n",
    "* класс знака - поле sign_class\n",
    "* координаты знаков\n",
    "* в какой набор включен знак - поле Set $\\in$ $\\{train, valid, test\\}$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6560301",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e6560301",
    "outputId": "2910fa40-d1b7-4378-9798-77b23b12cd3f"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import pathlib\n",
    "import shutil\n",
    "import cv2\n",
    "import sys\n",
    "\n",
    "%cd adas_system/notebooks\n",
    "\n",
    "try:\n",
    "    USE_TPU = bool(os.environ['COLAB_TPU_ADDR'])\n",
    "except:\n",
    "    USE_TPU = False\n",
    "\n",
    "if USE_TPU:\n",
    "    # !pip uninstall pytorch\n",
    "    # !pip install cloud-tpu-client==0.10 torch==1.10.0\n",
    "    # !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
    "    !pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n",
    "    import torch_xla\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    USE_TPU = True\n",
    "\n",
    "else:\n",
    "    USE_TPU = False\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    if not os.path.isfile('1_ClassifierResearch.ipynb'):\n",
    "        print('already exist')\n",
    "        !git clone --branch 9_SignDetector https://github.com/lsd-maddrive/adas_system.git\n",
    "        %cd adas_system/notebooks\n",
    "        !mkdir ../data/rtsd-frames\n",
    "        !unzip -j -q /content/drive/MyDrive/USER_FULL_FRAMES.zip -d ./../data/rtsd-frames\n",
    "        !pwd\n",
    "        !ls\n",
    "\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "###\n",
    "import nt_helper\n",
    "from nt_helper.helper_utils import *\n",
    "###\n",
    "\n",
    "TEXT_COLOR = 'black'\n",
    "\n",
    "# Зафиксируем состояние случайных чисел\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (17,10)\n",
    "\n",
    "if USE_TPU:\n",
    "    device = xm.xla_device()\n",
    "else:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1gQZscZB0Mr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "e1gQZscZB0Mr",
    "outputId": "633c2a1f-2df5-43bf-d7f7-47160aaef9e8"
   },
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ec2ac0",
   "metadata": {
    "id": "90ec2ac0"
   },
   "source": [
    "## Init dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8974552a",
   "metadata": {
    "id": "8974552a"
   },
   "outputs": [],
   "source": [
    "if not IN_COLAB:\n",
    "    PROJECT_ROOT = pathlib.Path(os.path.join(os.curdir, os.pardir))\n",
    "else:\n",
    "    PROJECT_ROOT = pathlib.Path('..')\n",
    "    \n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "NOTEBOOKS_DIR = PROJECT_ROOT / 'notebooks'\n",
    "\n",
    "if (NOTEBOOKS_DIR / 'full-gt.csv').is_file():\n",
    "    full_gt = pd.read_csv(NOTEBOOKS_DIR / 'full-gt.csv')\n",
    "else:\n",
    "    full_gt = pd.read_csv(DATA_DIR / 'full-gt.csv')\n",
    "\n",
    "FORMATED_GT_PATH = \"formated_full_gt.csv\"\n",
    "FULL_GT_SRC_LEN = len(full_gt.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a582a79b",
   "metadata": {
    "id": "a582a79b"
   },
   "source": [
    "## Init dataset DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edbb41f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "1edbb41f",
    "outputId": "98c2371b-faad-4551-c2da-85fcd6bb1722"
   },
   "outputs": [],
   "source": [
    "if os.path.isfile(FORMATED_GT_PATH):\n",
    "    print(\"FORMATED GT EXIST. LOAD IT\")\n",
    "    \n",
    "    import ast\n",
    "    \n",
    "    formated_full_gt_df = pd.read_csv(FORMATED_GT_PATH, dtype=object)\n",
    "    formated_full_gt_df['coords'].replace({'\\n ':',', ' \\s+': ' ', '\\[ ': '['}, regex=True, inplace=True)\n",
    "    \n",
    "    formated_full_gt_df['coords'] = formated_full_gt_df['coords'].apply(\n",
    "        lambda x: ast.literal_eval(x)\n",
    "    )\n",
    "    \n",
    "    formated_full_gt_df['size'] = formated_full_gt_df['size'].apply(\n",
    "        lambda x: ast.literal_eval(x)\n",
    "    )\n",
    "\n",
    "    formated_full_gt_df['filepath'] = formated_full_gt_df['filepath'].apply(\n",
    "        lambda x: x.replace('\\\\', '/')\n",
    "    )\n",
    "else:\n",
    "    print(\"FORMATED GT DOESNT EXIST. CREATE IT\")\n",
    "    \n",
    "    # get all original filenames\n",
    "    full_gt_unique_filenames = set(full_gt['filename'])\n",
    "    full_gt_unique_filenames_size = len(full_gt_unique_filenames)\n",
    "    \n",
    "    formated_full_gt_list = []\n",
    "\n",
    "    import imagesize\n",
    "    i = 0\n",
    "    for src_filename_iterator in list(full_gt_unique_filenames):\n",
    "\n",
    "        mask = np.in1d(full_gt['filename'], [src_filename_iterator])\n",
    "        coord_data_arr = full_gt[mask][['x_from', 'y_from', 'width', 'height']].to_numpy()\n",
    "        \n",
    "        filepath = DATA_DIR / \"rtsd-frames\" / src_filename_iterator\n",
    "        origW, origH = imagesize.get(filepath)\n",
    "                \n",
    "        rel_coord = []\n",
    "        for coord in coord_data_arr:\n",
    "            # make from x, y, dx, dx -> x1, y1, x2, y2\n",
    "            CV2RectangleCoords = ConvertAbsTLWH2CV2Rectangle(coord)\n",
    "   \n",
    "            # make from x1, y1, x2, y2 -> x, y, w, h\n",
    "            CV2CircleCoords = ConvertCV2Rectangle2CenterXYWH(CV2RectangleCoords)\n",
    "            \n",
    "            # make x, y, w, h -> relative x, y, w, h\n",
    "            rel_instance = MakeRel(CV2CircleCoords, origW, origH)\n",
    "            rel_coord.append(rel_instance)\n",
    "            \n",
    "        if i % 100 == 0:\n",
    "            printProgressEnum(i, full_gt_unique_filenames_size)\n",
    "        i += 1\n",
    "\n",
    "        formated_full_gt_list.append([str(filepath), rel_coord, [origW, origH]])\n",
    "\n",
    "    formated_full_gt_df = pd.DataFrame(formated_full_gt_list, columns=['filepath', 'coords', 'size'])\n",
    "    formated_full_gt_df.to_csv(\"formated_full_gt.csv\", index=False)\n",
    "\n",
    "if 'set' in formated_full_gt_df.columns:\n",
    "    print('SET ALREADY EXIST')\n",
    "else:\n",
    "    print('SET DOESNT EXIST. LETS CREATE IT')\n",
    "    formated_full_gt_df_index_count = len(formated_full_gt_df.index)\n",
    "    TRAIN_SIZE = round(0.7 * formated_full_gt_df_index_count)\n",
    "    VALID_SIZE = round(0.2 * formated_full_gt_df_index_count)\n",
    "    TEST_SIZE = round(formated_full_gt_df_index_count - TRAIN_SIZE - VALID_SIZE)\n",
    "        \n",
    "    assert TRAIN_SIZE + VALID_SIZE + TEST_SIZE == formated_full_gt_df_index_count, 'wrong split'\n",
    "    set_series = pd.Series('test', index=range(TEST_SIZE)).append(\n",
    "        pd.Series('train', index=range(TRAIN_SIZE)).append(\n",
    "            pd.Series('valid', index=range(VALID_SIZE))\n",
    "        )\n",
    "    ).sample(frac=1).reset_index(drop=True)\n",
    "    formated_full_gt_df['set'] = set_series\n",
    "    formated_full_gt_df.to_csv(\"formated_full_gt.csv\", index=False)\n",
    "    \n",
    "display(formated_full_gt_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33148d2",
   "metadata": {
    "id": "e33148d2"
   },
   "source": [
    "# simple test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2de1281",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 748
    },
    "id": "e2de1281",
    "outputId": "abe912b2-f1bc-4e03-c441-2c85eb18a25e"
   },
   "outputs": [],
   "source": [
    "instance = formated_full_gt_df.iloc[15466]\n",
    "\n",
    "path_ = str(instance['filepath'])\n",
    "img = cv2.imread(path_)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "h, w = img.shape[0], img.shape[1]\n",
    "print('Shape:', w, h)\n",
    "\n",
    "for i in instance['coords']:\n",
    "    \n",
    "    xywh = UnmakeRel(i, w, h)\n",
    "    x1y1x2y2 = ConvertCenterXYWH2CV2Rectangle(xywh)\n",
    "    print('+', MakeRel(x1y1x2y2, w, h))\n",
    "    print('xywh', xywh)\n",
    "    print('x1y1x2y2', x1y1x2y2)\n",
    "    \n",
    "    \n",
    "    img = cv2.rectangle(img, (x1y1x2y2[0], x1y1x2y2[1]), \n",
    "                        (x1y1x2y2[2], x1y1x2y2[3]), \n",
    "                        (255, 0, 0), \n",
    "                        3)\n",
    "    \n",
    "    img = cv2.circle(img, \n",
    "                     (xywh[0], xywh[1]), \n",
    "                     xywh[2] // 2, \n",
    "                     (255, 255, 0), \n",
    "                     3)\n",
    "\n",
    "plt.figure(figsize = (20, 20))  \n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a67aff",
   "metadata": {
    "id": "98a67aff"
   },
   "source": [
    "# Now we have pd.DataFrame that contains filenames, list of relative coordinates, corresponding photo resoulutions and marks for set. \n",
    "## createDataLoaderAndDataSet function in utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f8ff5c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "e1f8ff5c",
    "outputId": "faaaae6e-4ae7-41da-c3b7-f59a4f10c10e"
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "hyps_file = \"hyp.scratch.yaml\"\n",
    "with open(hyps_file, errors='ignore') as f:\n",
    "    hyp = yaml.safe_load(f)\n",
    "\n",
    "\n",
    "IMG_SIZE = 640\n",
    "batch_size = 160\n",
    "train_loader, train_dataset = createDataLoaderAndDataSet(formated_full_gt_df, \n",
    "                                                         'train',\n",
    "                                                         hyp_arg=hyp,\n",
    "                                                         imgsz=IMG_SIZE, \n",
    "                                                         batch_size=batch_size, \n",
    "                                                         augment=False)\n",
    "\n",
    "test_loader, test_dataset = createDataLoaderAndDataSet(formated_full_gt_df, \n",
    "                                                         'test',\n",
    "                                                         hyp_arg=hyp,\n",
    "                                                         imgsz=IMG_SIZE, \n",
    "                                                         batch_size=batch_size, \n",
    "                                                         augment=True)\n",
    "\n",
    "img, labels_out, filepath, shapes = train_dataset[6]\n",
    "img_, labels_out_, filepath_, shapes_ = test_dataset[random.randrange(0, len(test_dataset))]\n",
    "\n",
    "imgNT = img.numpy().transpose(1, 2, 0).astype(np.uint8).copy() #, cv2.COLOR_BGR2RGB)\n",
    "print(labels_out)\n",
    "print(filepath)\n",
    "for coord in labels_out[:, 2:]:\n",
    "    # print(coord)\n",
    "    h, w = shapes[0]\n",
    "    xywh = UnmakeRel(coord, IMG_SIZE, IMG_SIZE)\n",
    "    x1y1x2y2 = ConvertCenterXYWH2CV2Rectangle(xywh)\n",
    "    print(x1y1x2y2)\n",
    "    imgNT = cv2.rectangle(imgNT, (x1y1x2y2[0], x1y1x2y2[1]), \n",
    "                        (x1y1x2y2[2], x1y1x2y2[3]), \n",
    "                        (255, 0, 0), \n",
    "                        3)\n",
    "\n",
    "\n",
    "plt.figure(figsize = (20, 20))  \n",
    "plt.imshow(imgNT)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XGelWDPIcxGl",
   "metadata": {
    "id": "XGelWDPIcxGl"
   },
   "source": [
    "ROAD SIGN ANCHORS: 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326 src: https://grechka.family/dmitry/blog/2019/09/yolo-v3-anchors-for-traffic-sign-detection/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cf049f",
   "metadata": {
    "id": "92cf049f"
   },
   "source": [
    "# Train?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b909113",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0b909113",
    "outputId": "cb9c92f0-3e28-4ea0-d5cc-8dce5ef852ad"
   },
   "outputs": [],
   "source": [
    "from torch.optim import SGD, Adam, AdamW, lr_scheduler\n",
    "from torch.cuda import amp\n",
    "from utils.general import one_cycle, LOGGER\n",
    "from utils.loss import ComputeLoss\n",
    "from utils.torch_utils import ModelEMA, de_parallel\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from models.yolo import Model\n",
    "\n",
    "import yaml\n",
    "\n",
    "hyps_file = \"hyp.scratch.yaml\"\n",
    "with open(hyps_file, errors='ignore') as f:\n",
    "    hyp = yaml.safe_load(f)\n",
    "\n",
    "def train(epochs, train_loader, test_loader, device, opt=None, imgsz=640, model=None, restore=None, model_cfg_path=None):\n",
    "    \n",
    "    if not model and not restore:\n",
    "        print(\"ARG ERR NOT MODEL NOT RESTORE\")\n",
    "        return\n",
    "    if restore:\n",
    "        if model_cfg_path:         \n",
    "            model_cfg_file = model_cfg_path\n",
    "            model = Model(cfg=model_cfg_file, ch=3, nc=1)\n",
    "            if os.path.isfile(restore):\n",
    "                print('[+] restore passed, loading it')\n",
    "                model.load_state_dict(torch.load(restore, map_location=device))\n",
    "            else:\n",
    "                print('[-] restore passed, but doesnt exist. just train')\n",
    "        else:\n",
    "            print(\"ARG ERR CFG PATH\")\n",
    "            return\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    ###\n",
    "    start_epoch = 0\n",
    "    nc = 1\n",
    "    # print(device.type)\n",
    "    cuda = device.type == 'cuda'\n",
    "    # print(cuda)\n",
    "    nb = len(train_loader)\n",
    "    nw = max(round(hyp['warmup_epochs'] * nb), 1000)\n",
    "    nbs = 64  # nominal batch size\n",
    "    batch_size = train_loader.batch_size\n",
    "    last_opt_step = -1\n",
    "    ###\n",
    "        \n",
    "    g0, g1, g2 = [], [], []  # optimizer parameter groups\n",
    "    for v in model.modules():\n",
    "        if hasattr(v, 'bias') and isinstance(v.bias, nn.Parameter):  # bias\n",
    "            g2.append(v.bias)\n",
    "        if isinstance(v, nn.BatchNorm2d):  # weight (no decay)\n",
    "            g0.append(v.weight)\n",
    "        elif hasattr(v, 'weight') and isinstance(v.weight, nn.Parameter):  # weight (with decay)\n",
    "            g1.append(v.weight)\n",
    "    \n",
    "    if opt==None:\n",
    "        optimizer = SGD(g0, lr=hyp['lr0'], momentum=hyp['momentum'], nesterov=True)\n",
    "    else:\n",
    "        if opt.optimizer == 'Adam':\n",
    "            optimizer = Adam(g0, lr=hyp['lr0'], betas=(hyp['momentum'], 0.999))  # adjust beta1 to momentum\n",
    "        elif opt.optimizer == 'AdamW':\n",
    "            optimizer = AdamW(g0, lr=hyp['lr0'], betas=(hyp['momentum'], 0.999))  # adjust beta1 to momentum\n",
    "        else:\n",
    "            optimizer = SGD(g0, lr=hyp['lr0'], momentum=hyp['momentum'], nesterov=True)\n",
    "    \n",
    "    optimizer.add_param_group({'params': g1, 'weight_decay': hyp['weight_decay']})  # add g1 with weight_decay\n",
    "    optimizer.add_param_group({'params': g2})  # add g2 (biases)\n",
    "    del g0, g1, g2\n",
    "    \n",
    "\n",
    "    lf = one_cycle(1, hyp['lrf'], epochs)  # cosine 1->hyp['lrf']\n",
    "    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)\n",
    "    \n",
    "    ema = None # ModelEMA(model)\n",
    "    \n",
    "    nl = de_parallel(model).model[-1].nl  # number of detection layers (to scale hyps)\n",
    "    hyp['box'] *= 3 / nl  # scale to layers\n",
    "    hyp['cls'] *= nc / 80 * 3 / nl  # scale to classes and layers\n",
    "    hyp['obj'] *= (imgsz / 640) ** 2 * 3 / nl  # scale to image size and layers\n",
    "    hyp['label_smoothing'] = opt.label_smoothing if opt else 0.\n",
    "    \n",
    "    model.nc = nc  # attach number of classes to model\n",
    "    model.hyp = hyp  # attach hyperparameters to model\n",
    "    model.names = ['sign']\n",
    "    \n",
    "    scaler = amp.GradScaler(enabled=cuda)\n",
    "    compute_loss = ComputeLoss(model)\n",
    "    \n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        model.train()\n",
    "        mloss = torch.zeros(3, device=device)\n",
    "        \n",
    "        pbar = enumerate(train_loader)\n",
    "        LOGGER.info(('\\n' + '%10s' * 7) % ('Epoch', 'gpu_mem', 'box', 'obj', 'cls', 'labels', 'img_size'))\n",
    "        pbar = tqdm(pbar, total=nb, bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}')  # progress bar\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for i, (imgs, targets, paths, _) in pbar:\n",
    "            ni = i + nb * epoch  # number integrated batches (since train start)\n",
    "            imgs = imgs.to(device, non_blocking=True).float() / 255  # uint8 to float32, 0-255 to 0.0-1.0\n",
    "            \n",
    "            # Warmup\n",
    "            if ni <= nw:\n",
    "                xi = [0, nw]  # x interp\n",
    "                # compute_loss.gr = np.interp(ni, xi, [0.0, 1.0])  # iou loss ratio (obj_loss = 1.0 or iou)\n",
    "                accumulate = max(1, np.interp(ni, xi, [1, nbs / batch_size]).round())\n",
    "                for j, x in enumerate(optimizer.param_groups):\n",
    "                    # bias lr falls from 0.1 to lr0, all other lrs rise from 0.0 to lr0\n",
    "                    x['lr'] = np.interp(ni, xi, [hyp['warmup_bias_lr'] if j == 2 else 0.0, x['initial_lr'] * lf(epoch)])\n",
    "                    if 'momentum' in x:\n",
    "                        x['momentum'] = np.interp(ni, xi, [hyp['warmup_momentum'], hyp['momentum']])\n",
    "\n",
    "            # Forward\n",
    "            with amp.autocast(enabled=cuda):\n",
    "                pred = model(imgs)  # forward\n",
    "                # print(pred[0].shape)\n",
    "                # return pred, targets, paths\n",
    "                loss, loss_items = compute_loss(pred, targets.to(device))  # loss scaled by batch_size\n",
    "                \n",
    "                \n",
    "            # Backward\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # Optimize\n",
    "            if ni - last_opt_step >= accumulate:\n",
    "                scaler.step(optimizer)  # optimizer.step\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                if ema:\n",
    "                    ema.update(model)\n",
    "                last_opt_step = ni\n",
    "            \n",
    "            if True:\n",
    "                mloss = (mloss * i + loss_items) / (i + 1)  # update mean losses\n",
    "                mem = f'{torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0:.3g}G'  # (GB)\n",
    "                pbar.set_description(('%10s' * 2 + '%10.4g' * 5) % (\n",
    "                    f'{epoch}/{epochs - 1}', mem, *mloss, targets.shape[0], imgs.shape[-1]))\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        now = datetime.now()\n",
    "        model_save_name = 'YoloV5_{}_lbox{:.4f}_lobj{:.4f}.pt'.format(\n",
    "            now.strftime(\"%d.%m_%H.%M\"),\n",
    "            mloss[0], mloss[1]\n",
    "        )\n",
    "        \n",
    "        torch.save(model.state_dict(), model_save_name)\n",
    "        torch.save(model.state_dict(), 'YoloV5Last.pt')\n",
    "        if IN_COLAB:\n",
    "            shutil.copy2(model_save_name, '/content/drive/MyDrive/')\n",
    "\n",
    "        torch.save(model.state_dict(), 'YoloV5Last.pt')\n",
    "        if IN_COLAB:\n",
    "            shutil.copy2('YoloV5Last.pt', '/content/drive/MyDrive/')\n",
    "    \n",
    "    return model\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "IMG_SIZE = 640\n",
    "batch_size = 80\n",
    "num_workers = 4\n",
    "train_loader, train_dataset = createDataLoaderAndDataSet(formated_full_gt_df, \n",
    "                                                         'train',\n",
    "                                                         hyp_arg=hyp,\n",
    "                                                         imgsz=IMG_SIZE, \n",
    "                                                         batch_size=batch_size, \n",
    "                                                         augment=True,\n",
    "                                                         nw=num_workers)\n",
    "\n",
    "test_loader, test_dataset = createDataLoaderAndDataSet(formated_full_gt_df, \n",
    "                                                         'test',\n",
    "                                                         hyp_arg=hyp,\n",
    "                                                         imgsz=IMG_SIZE, \n",
    "                                                         batch_size=batch_size, \n",
    "                                                         augment=False,\n",
    "                                                         nw=num_workers)\n",
    "\n",
    "restore='YoloV5Last.pt'\n",
    "model_cfg_file = 'yolov5s_custom_anchors.yaml'\n",
    "SHOULD_I_TRAIN = False\n",
    "\n",
    "if SHOULD_I_TRAIN:\n",
    "    model = train(20, train_loader, test_loader, device, imgsz=IMG_SIZE, restore=restore,\n",
    "                  model_cfg_path=model_cfg_file)\n",
    "else:\n",
    "    model = Model(cfg=model_cfg_file, ch=3, nc=1)\n",
    "    model.load_state_dict(torch.load(restore, map_location=device))\n",
    "\n",
    "model.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gZJhb5cKixBy",
   "metadata": {
    "id": "gZJhb5cKixBy"
   },
   "outputs": [],
   "source": [
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c2072c",
   "metadata": {
    "id": "b3c2072c"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from datetime import datetime\n",
    "now = datetime.now\n",
    "\n",
    "restore='YoloV5Last.pt'\n",
    "model_cfg_file = 'yolov5s_custom_anchors.yaml'\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "model = Model(cfg=model_cfg_file, ch=3, nc=1);\n",
    "model.load_state_dict(torch.load(restore, map_location=device))\n",
    "\n",
    "detectInterface = makeDetectFromModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1d9d60",
   "metadata": {
    "id": "2a1d9d60"
   },
   "outputs": [],
   "source": [
    "from utils.datasets import LoadImages\n",
    "img_, labels_out_, filepath_, shapes_ = test_dataset[155] # random.randrange(0, len(test_dataset))]\n",
    "\n",
    "img_size = 640\n",
    "dataset = LoadImages(filepath_, img_size=img_size, auto=False)\n",
    "# model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # or yolov5m, yolov5l, yolov5x, custom\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "imgsz = (img_size, img_size)\n",
    "\n",
    "detectInterface.warmup(imgsz=(1, 3, imgsz))\n",
    "detectInterface.to(device)\n",
    "\n",
    "for path, im, im0s, vid_cap, s in dataset:\n",
    "    \n",
    "    print(im.shape)\n",
    "    print(im0s)\n",
    "    im0s_orig = im0s\n",
    "    im = torch.from_numpy(im).float()\n",
    "    im /= 255\n",
    "    im = im[None, ...]\n",
    "    im = im.to(device)\n",
    "    t0 = now()\n",
    "    \n",
    "    pred = detectInterface(im)\n",
    "    print('process dT =', now() - t0)\n",
    "    data = detectInterface.translatePreds(pred, im.shape[2:], im0s.shape, conf_thres=0.05)\n",
    "    \n",
    "    \n",
    "    \n",
    "    im0s = cv2.cvtColor(im0s, cv2.COLOR_BGR2RGB)\n",
    "    for i in range(data['count']):\n",
    "        im0s = cv2.rectangle(im0s, (data['coords'][i][0], data['coords'][i][1]), \n",
    "                        (data['coords'][i][2], data['coords'][i][3]), \n",
    "                        (255, 0, 0), \n",
    "                        3)\n",
    "        \n",
    "        im0s = cv2.putText(im0s, str(round(data['confs'][i], 3)), \n",
    "                           (data['coords'][i][2] - 40, data['coords'][i][3]),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                           0.7, (255, 255, 0),\n",
    "                           5, cv2.LINE_AA\n",
    "                          )\n",
    "        \n",
    "        im0s = cv2.putText(im0s, str(round(data['confs'][i], 3)), \n",
    "                           (data['coords'][i][2] - 40, data['coords'][i][3]),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                           0.7, (0, 0, 0),\n",
    "                           2, cv2.LINE_AA\n",
    "                          )\n",
    "        \n",
    "        \n",
    "\n",
    "    print('plot dT =', now() - t0)\n",
    "    \n",
    "fig, axs = plt.subplots(2, figsize=(20, 20))\n",
    "axs[0].imshow(im0s)\n",
    "\n",
    "im0s_orig = cv2.cvtColor(im0s_orig, cv2.COLOR_BGR2RGB)\n",
    "axs[1].imshow(im0s_orig)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of 2_YoloDetection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
