{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef5ffffc",
   "metadata": {},
   "source": [
    "Объединенный датасет [FIX MEне доступен по ссылке](*).\n",
    "\n",
    "Положить в папку data содержимое так, чтобы были следующие пути:  \n",
    "* \\$(ROOT_DIR)/data/full-rtsd/...\n",
    "* \\$(ROOT_DIR)/data/full-gt.csv\n",
    "\n",
    "> *gt_Set_NaN.csv - содержит тот же датасет, но значения колонки Set обнулено*\n",
    "\n",
    "gt - датафрейм содержащий:  \n",
    "* имена файлов - поле filename\n",
    "* класс знака - поле sign_class\n",
    "* координаты знаков\n",
    "* в какой набор включен знак - поле Set $\\in$ $\\{train, valid, test\\}$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6560301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (25,8)\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import pathlib\n",
    "import shutil\n",
    "import cv2\n",
    "import PIL\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "TEXT_COLOR = 'black'\n",
    "\n",
    "# Зафиксируем состояние случайных чисел\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8974552a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not IN_COLAB:\n",
    "    %run utils.ipynb\n",
    "    PROJECT_ROOT = pathlib.Path(os.path.join(os.curdir, os.pardir))\n",
    "else:\n",
    "    PROJECT_ROOT = pathlib.Path('')\n",
    "    \n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "NOTEBOOKS_DIR = PROJECT_ROOT / 'notebooks'\n",
    "\n",
    "if (NOTEBOOKS_DIR / 'full-gt.csv').is_file():\n",
    "    full_gt = pd.read_csv(NOTEBOOKS_DIR / 'full-gt.csv')\n",
    "else:\n",
    "    full_gt = pd.read_csv(DATA_DIR / 'full-gt.csv')\n",
    "\n",
    "FORMATED_GT_PATH = \"formated_full_gt.csv\"\n",
    "FULL_GT_SRC_LEN = len(full_gt.index)\n",
    "display(full_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edbb41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_gt_unique_filenames = set(full_gt['filename'])\n",
    "full_gt_unique_filenames_size = len(full_gt_unique_filenames)\n",
    "%run utils.ipynb\n",
    "import ast\n",
    "import re\n",
    "\n",
    "i = 0;\n",
    "\n",
    "if os.path.isfile(FORMATED_GT_PATH):\n",
    "    print(\"FORMATED GT EXIST. LOAD IT\")\n",
    "    formated_full_gt_df = pd.read_csv(FORMATED_GT_PATH, dtype=object)\n",
    "    # display(formated_full_gt_df)\n",
    "    formated_full_gt_df['coords'].replace({'\\n ':',', ' \\s+': ' ', '\\[ ': '['}, regex=True, inplace=True)\n",
    "    # display(formated_full_gt_df)\n",
    "    formated_full_gt_df['coords'] = formated_full_gt_df['coords'].apply(\n",
    "        lambda x: ast.literal_eval(x)\n",
    "    )\n",
    "    \n",
    "    formated_full_gt_df['size'] = formated_full_gt_df['size'].apply(\n",
    "        lambda x: ast.literal_eval(x)\n",
    "    )\n",
    "else:\n",
    "    print(\"FORMATED GT DOESNT EXIST. CREATE IT\")\n",
    "    # get all original filenames\n",
    "    full_gt_unique_filenames = set(full_gt['filename'])\n",
    "    \n",
    "    formated_full_gt_list = []\n",
    "\n",
    "    import imagesize\n",
    "    \n",
    "    for src_filename_iterator in list(full_gt_unique_filenames):\n",
    "\n",
    "        mask = np.in1d(full_gt['filename'], [src_filename_iterator])\n",
    "        coord_data_arr = full_gt[mask][['x_from', 'y_from', 'width', 'height']].to_numpy()\n",
    "        \n",
    "        filepath = DATA_DIR / \"rtsd-frames\" / src_filename_iterator\n",
    "        origW, origH = imagesize.get(filepath)\n",
    "                \n",
    "        rel_coord = []\n",
    "        for coord in coord_data_arr:\n",
    "            # make from x, y, dx, dx -> x1, y1, x2, y2\n",
    "            CV2RectangleCoords = ConvertAbsTLWH2CV2Rectangle(coord)\n",
    "   \n",
    "            # make from x1, y1, x2, y2 -> x, y, w, h\n",
    "            CV2CircleCoords = ConvertCV2Rectangle2CenterXYWH(CV2RectangleCoords)\n",
    "            \n",
    "            # make x, y, w, h -> relative x, y, w, h\n",
    "            rel_instance = MakeRel(CV2CircleCoords, origW, origH)\n",
    "\n",
    "            rel_coord.append(rel_instance)\n",
    "            \n",
    "        if i % 100 == 0:\n",
    "            printProgressEnum(i, full_gt_unique_filenames_size)\n",
    "        i += 1\n",
    "\n",
    "        formated_full_gt_list.append([str(filepath), rel_coord, [origW, origH]])\n",
    "\n",
    "    formated_full_gt_df = pd.DataFrame(formated_full_gt_list, columns=['filepath', 'coords', 'size'])\n",
    "    formated_full_gt_df.to_csv(\"formated_full_gt.csv\", index=False)\n",
    "\n",
    "formated_full_gt_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33148d2",
   "metadata": {},
   "source": [
    "# simple test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2de1281",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = formated_full_gt_df.iloc[15466]\n",
    "print(instance)\n",
    "img = cv2.imread(str(instance['filepath']))\n",
    "img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "\n",
    "h, w = img.shape[0], img.shape[1]\n",
    "print('Shape:', w, h)\n",
    "\n",
    "\n",
    "for i in instance['coords']:\n",
    "    \n",
    "    xywh = UnmakeRel(i, w, h)\n",
    "    x1y1x2y2 = ConvertCenterXYWH2CV2Rectangle(xywh)\n",
    "    print('+', MakeRel(x1y1x2y2, w, h))\n",
    "    print('xywh', xywh)\n",
    "    print('x1y1x2y2', x1y1x2y2)\n",
    "    \n",
    "    \n",
    "    img = cv2.rectangle(img, (x1y1x2y2[0], x1y1x2y2[1]), \n",
    "                        (x1y1x2y2[2], x1y1x2y2[3]), \n",
    "                        (255, 0, 0), \n",
    "                        3)\n",
    "    \n",
    "    img = cv2.circle(img, \n",
    "                     (xywh[0], xywh[1]), \n",
    "                     xywh[2] // 2, \n",
    "                     (255, 255, 0), \n",
    "                     3)\n",
    "\n",
    "    \n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628d20fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'set' in formated_full_gt_df.columns:\n",
    "    print('SET ALREADY EXIST')\n",
    "else:\n",
    "    print('SET DOESNT EXIST. LETS CREATE IT')\n",
    "    formated_full_gt_df_index_count = len(formated_full_gt_df.index)\n",
    "    TRAIN_SIZE = round(0.7 * formated_full_gt_df_index_count)\n",
    "    VALID_SIZE = round(0.2 * formated_full_gt_df_index_count)\n",
    "    TEST_SIZE = round(formated_full_gt_df_index_count - TRAIN_SIZE - VALID_SIZE)\n",
    "    \n",
    "    # print('assert:', TRAIN_SIZE + VALID_SIZE + TEST_SIZE, '==', formated_full_gt_df_index_count)\n",
    "    \n",
    "    assert TRAIN_SIZE + VALID_SIZE + TEST_SIZE == formated_full_gt_df_index_count, 'wrong split'\n",
    "    set_series = pd.Series('test', index=range(TEST_SIZE)).append(\n",
    "        pd.Series('train', index=range(TRAIN_SIZE)).append(\n",
    "            pd.Series('valid', index=range(VALID_SIZE))\n",
    "        )\n",
    "    ).sample(frac=1).reset_index(drop=True)\n",
    "    formated_full_gt_df['set'] = set_series\n",
    "    display(formated_full_gt_df)\n",
    "    formated_full_gt_df.to_csv(\"formated_full_gt.csv\", index=False)\n",
    "    \n",
    "display(formated_full_gt_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a67aff",
   "metadata": {},
   "source": [
    "Now we have pd.DataFrame that contains filenames, list of relative coordinates, corresponding photo resoulutions and marks for set. \n",
    "\n",
    "Let's make DataSets for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8843a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "YOLO_MODEL_HOME_DIR = DATA_DIR / 'YoloV5'\n",
    "AUGMENT_HOME_DIR = YOLO_MODEL_HOME_DIR / 'utils'\n",
    "\n",
    "if YOLO_MODEL_HOME_DIR not in sys.path:\n",
    "    sys.path.append(str(YOLO_MODEL_HOME_DIR))\n",
    "\n",
    "from models.yolo import Model\n",
    "from torch.optim import SGD, Adam, AdamW, lr_scheduler\n",
    "from utils.augmentations import Albumentations, augment_hsv, copy_paste, letterbox, mixup, random_perspective\n",
    "\n",
    "hyps_file = YOLO_MODEL_HOME_DIR / 'data/hyps' / \"hyp.scratch.yaml\"\n",
    "with open(hyps_file, errors='ignore') as f:\n",
    "    hyp = yaml.safe_load(f)\n",
    "\n",
    "class CreateDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, set_label, img_size=640, batch_size=16, augment=False, hyp=None):\n",
    "        \n",
    "        self.img_size = img_size\n",
    "        self.augment = augment\n",
    "        self.hyp = hyp\n",
    "\n",
    "        self.df = df[df['set']==set_label]\n",
    "        self.albumentations = Albumentations() if augment else None\n",
    "        \n",
    "    def loadImage(self, instance):\n",
    "        path, (w0, h0) = instance['filepath'], instance['size']\n",
    "        im = cv2.imread(path)\n",
    "        assert im is not None, f'Image Not Found {path}'\n",
    "        \n",
    "        r = self.img_size / max(h0, w0)  # ratio\n",
    "        if r != 1:  # if sizes are not equal\n",
    "            im = cv2.resize(im, (int(w0 * r), int(h0 * r)),\n",
    "                            interpolation=cv2.INTER_AREA if r < 1 and not self.augment else cv2.INTER_LINEAR)\n",
    "        return im, (h0, w0), im.shape[:2]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # locate img info from DataFrame\n",
    "        instance = self.df.iloc[index]\n",
    "        \n",
    "        # get Img, src height, width and resized height, width\n",
    "        img, (h0, w0), (h, w) = self.loadImage(instance)\n",
    "        \n",
    "        shape = self.img_size\n",
    "        \n",
    "        # make img square\n",
    "        # print('>', (img>1).sum())\n",
    "        # print('<=', (img<=1).sum())\n",
    "        img, ratio, pad = letterbox(img, shape, auto=False, scaleup=self.augment)\n",
    "        # print(pad)\n",
    "        # store core shape info\n",
    "        shapes = (h0, w0), ((h / h0, w / w0), pad)  # for COCO mAP rescaling\n",
    "        \n",
    "        # add class to labels. We have 1 class, so just add zeros into first column\n",
    "        labels = np.array(instance['coords'])\n",
    "        labels = np.c_[np.zeros(labels.shape[0]), labels]\n",
    "        # print(labels)\n",
    "        \n",
    "        # fix labels location caused by letterbox\n",
    "        labels[:, 1:] = xywhn2xyxy(labels[:, 1:], ratio[0] * w, ratio[1] * h, padw=pad[0], padh=pad[1])\n",
    "        \n",
    "        if self.augment:\n",
    "            img, labels = random_perspective(img, labels,\n",
    "                degrees=hyp['degrees'],\n",
    "                translate=hyp['translate'],\n",
    "                scale=hyp['scale'],\n",
    "                shear=hyp['shear'],\n",
    "                perspective=hyp['perspective'])\n",
    "                \n",
    "        labels[:, 1:5] = xyxy2xywhn(labels[:, 1:5], w=img.shape[1], h=img.shape[0], clip=False, eps=1E-3)\n",
    "        \n",
    "        # YOLO augmentation technique (!copy-paste!)\n",
    "        if self.augment:\n",
    "            # print('augm for', index, instance['filepath'])\n",
    "            # Albumentations\n",
    "            img, labels = self.albumentations(img, labels)\n",
    "            nl = len(labels)  # update after albumentations\n",
    "\n",
    "            # HSV color-space\n",
    "            augment_hsv(img, hgain=hyp['hsv_h'], sgain=hyp['hsv_s'], vgain=hyp['hsv_v'])\n",
    "\n",
    "            # Flip up-down\n",
    "            if random.random() < hyp['flipud']:\n",
    "                img = np.flipud(img)\n",
    "                if nl:\n",
    "                    labels[:, 2] = 1 - labels[:, 2]\n",
    "\n",
    "            # Flip left-right\n",
    "            if random.random() < hyp['fliplr']:\n",
    "                img = np.fliplr(img)\n",
    "                if nl:\n",
    "                    labels[:, 1] = 1 - labels[:, 1]\n",
    "                    \n",
    "        nl = len(labels)\n",
    "        \n",
    "        # why out size (?, 6)?? \n",
    "        labels_out = torch.zeros((nl, 6))\n",
    "        if nl:\n",
    "            labels_out[:, 1:] = torch.from_numpy(labels)   \n",
    "\n",
    "        img = img.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
    "        img = np.ascontiguousarray(img)\n",
    "        \n",
    "        return torch.from_numpy(img), labels_out, instance['filepath'], shapes\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df.index)\n",
    "    \n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        img, label, path, shapes = zip(*batch)  # transposed\n",
    "        for i, l in enumerate(label):\n",
    "            l[:, 0] = i  # add target image index for build_targets()\n",
    "        return torch.stack(img, 0), torch.cat(label, 0), path, shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f8ff5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataLoaderAndDataSet(df, set_label, imgsz, batch_size, hyp=None, augment=False,shuffle=True):\n",
    "    \n",
    "    \n",
    "    from torch.utils.data import DataLoader\n",
    "    \n",
    "    dataset = CreateDataSet(df, set_label, img_size=imgsz, augment=augment)\n",
    "    batch_size = min(batch_size, len(dataset))\n",
    "    \n",
    "    sampler = None #distributed.DistributedSampler(dataset, shuffle=shuffle)\n",
    "    \n",
    "    loader = DataLoader(dataset, # InfiniteDataLoader ?\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=shuffle and sampler is None,\n",
    "                        # num_workers=nw,  # doesnt work in Windows\n",
    "                        sampler=sampler,\n",
    "                        pin_memory=True,\n",
    "                        collate_fn=CreateDataSet.collate_fn)\n",
    "\n",
    "    \n",
    "    return loader, dataset\n",
    "\n",
    "IMG_SIZE = 1280\n",
    "train_loader, train_dataset = createDataLoaderAndDataSet(formated_full_gt_df, \n",
    "                                                         'test', \n",
    "                                                         imgsz=IMG_SIZE, \n",
    "                                                         batch_size=20, \n",
    "                                                         augment=True)\n",
    "\n",
    "img, labels_out, filepath, shapes = train_dataset[2190 ]\n",
    "\n",
    "imgNT = img.numpy().transpose(1, 2, 0).astype(np.uint8).copy() #, cv2.COLOR_BGR2RGB)\n",
    "print(labels_out)\n",
    "for coord in labels_out[:, 2:]:\n",
    "    # print(coord)\n",
    "    h, w = shapes[0]\n",
    "    xywh = UnmakeRel(coord, IMG_SIZE, IMG_SIZE)\n",
    "    x1y1x2y2 = ConvertCenterXYWH2CV2Rectangle(xywh)\n",
    "    print(x1y1x2y2)\n",
    "    imgNT = cv2.rectangle(imgNT, (x1y1x2y2[0], x1y1x2y2[1]), \n",
    "                        (x1y1x2y2[2], x1y1x2y2[3]), \n",
    "                        (255, 0, 0), \n",
    "                        3)\n",
    "    \n",
    "plt.imshow(imgNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50aa976e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cfg_file = YOLO_MODEL_HOME_DIR / 'models/yolov5l_custom_anchors.yaml'\n",
    "model = Model(cfg=model_cfg_file, ch=3, nc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44850036",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "start_epoch = 0\n",
    "epochs = 5\n",
    "\n",
    "nb = len(train_loader)  # number of batches\n",
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    model.train()\n",
    "    mloss = torch.zeros(3, device=device)  # mean losses\n",
    "    pbar = enumerate(train_loader)\n",
    "    pbar = tqdm(pbar, total=nb, bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}')  # progress bar\n",
    "    optimizer.zero_grad()\n",
    "    nw = max(round(hyp['warmup_epochs'] * nb), 1000)  # number of warmup iterations, max(3 epochs, 1k iterations)\n",
    "        \n",
    "    for i, (imgs, targets, paths, _) in pbar:\n",
    "        ni = i + nb * epoch  # number integrated batches (since train start)\n",
    "        # Warmup\n",
    "        if ni <= nw:\n",
    "            xi = [0, nw]  # x interp\n",
    "            # compute_loss.gr = np.interp(ni, xi, [0.0, 1.0])  # iou loss ratio (obj_loss = 1.0 or iou)\n",
    "            accumulate = max(1, np.interp(ni, xi, [1, nbs / batch_size]).round())\n",
    "            for j, x in enumerate(optimizer.param_groups):\n",
    "                # bias lr falls from 0.1 to lr0, all other lrs rise from 0.0 to lr0\n",
    "                x['lr'] = np.interp(ni, xi, [hyp['warmup_bias_lr'] if j == 2 else 0.0, x['initial_lr'] * lf(epoch)])\n",
    "                if 'momentum' in x:\n",
    "                    x['momentum'] = np.interp(ni, xi, [hyp['warmup_momentum'], hyp['momentum']])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f68837a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = enumerate(train_loader)\n",
    "for i, (imgs, targets, paths, _) in pbar:\n",
    "    print(i)\n",
    "    pass\n",
    "print('completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a305545f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[15466]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1737096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "batch_size = 1\n",
    "nbs = 64  # nominal batch size\n",
    "accumulate = max(round(nbs / batch_size), 1)  # accumulate loss before optimizing\n",
    "hyp['weight_decay'] *= batch_size * accumulate / nbs  # scale weight_decay\n",
    "\n",
    "g0, g1, g2 = [], [], []  # optimizer parameter groups\n",
    "for v in model.modules():\n",
    "    if hasattr(v, 'bias') and isinstance(v.bias, nn.Parameter):  # bias\n",
    "        g2.append(v.bias)\n",
    "    if isinstance(v, nn.BatchNorm2d):  # weight (no decay)\n",
    "        g0.append(v.weight)\n",
    "    elif hasattr(v, 'weight') and isinstance(v.weight, nn.Parameter):  # weight (with decay)\n",
    "        g1.append(v.weight)\n",
    "\n",
    "optimizer = 'AdamW'\n",
    "if optimizer == 'Adam':\n",
    "    optimizer = Adam(g0, lr=hyp['lr0'], betas=(hyp['momentum'], 0.999))  # adjust beta1 to momentum\n",
    "elif optimizer == 'AdamW':\n",
    "    optimizer = AdamW(g0, lr=hyp['lr0'], betas=(hyp['momentum'], 0.999))  # adjust beta1 to momentum\n",
    "else:\n",
    "    optimizer = SGD(g0, lr=hyp['lr0'], momentum=hyp['momentum'], nesterov=True)\n",
    "        \n",
    "optimizer.add_param_group({'params': g1, 'weight_decay': hyp['weight_decay']})  # add g1 with weight_decay\n",
    "optimizer.add_param_group({'params': g2})  # add g2 (biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aacea8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d109d480",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.anchors\n",
    "# 7,12,  10,17,  13,24,  26,20,  17,32,  23,41,  31,54,  41,72,  58,100 # custom anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2777680",
   "metadata": {},
   "outputs": [],
   "source": [
    " t_ = torch.tensor([[[ 10.,  13.],\n",
    "         [ 16.,  30.],\n",
    "         [ 33.,  23.]],  # P3/8-small\n",
    "\n",
    "        [[ 30.,  61.],\n",
    "         [ 62.,  45.],\n",
    "         [ 59., 119.]],  # P4/16-medium\n",
    "\n",
    "        [[116.,  90.],\n",
    "         [156., 198.],\n",
    "         [373., 326.]]], dtype=torch.float16) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29eeba3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
