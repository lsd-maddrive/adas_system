{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BgZKi4UDtiDt"
   },
   "source": [
    "Объединенный датасет доступен по [ссылке](https://drive.google.com/drive/folders/1jmxG2zfi-Fs3m2KrMGmjD347aYiT8YFM?usp=sharing).\n",
    "\n",
    "Положить в папку data содержимое так, чтобы были следующие пути:  \n",
    "* \\$(ROOT_DIR)/data/merged-rtsd/...\n",
    "* \\$(ROOT_DIR)/data/gt.csv\n",
    "\n",
    "> *gt_Set_NaN.csv - содержит тот же датасет, но значения колонки Set обнулено*\n",
    "\n",
    "gt - датафрейм содержащий:  \n",
    "* имена файлов - поле filename\n",
    "* класс знака - поле sign_class\n",
    "* флаг присутствия знака при работе с датасетом - IsPresent. Предполагается, что вместо удаления записи, будет устанавливатся этот флаг, включающий/не влючающий знак в выборку\n",
    "* в какой набор включен знак - поле Set $\\in$ $\\{train, valid, test\\}$\n",
    "\n",
    "~~\\# !gdown --id '1eKNfEuNQadRW1H4NOoMw5sdnyHV14ze0'\n",
    "\\# !unzip rtsd-r3.zip\n",
    "\\# !rm -rf rtsd-r3.zip~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kror7RorULDp"
   },
   "outputs": [],
   "source": [
    "# !gdown --id 1Hmp8mi0F3f_pzM7bQ95EtA_3C6yefQsb\n",
    "# !7z x data.7z\n",
    "# !mkdir data\n",
    "# !mv gt.csv data\n",
    "# !mv merged-rtsd data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4F3G3CsMuscG",
    "outputId": "ebc46644-a5e4-43ac-b9ac-b8d2a0ac41a4"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import pathlib\n",
    "import shutil\n",
    "import cv2\n",
    "import PIL\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "TEXT_COLOR = 'black'\n",
    "\n",
    "# Зафиксируем состояние случайных чисел\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xEVqwKlEvGbN"
   },
   "source": [
    "Читаем данные, настраиваем дирректории. Инклудим *utils.ipynb*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 917
    },
    "id": "jIdFu3ebuhn2",
    "outputId": "582c4755-ac7d-4186-d0dc-9884c9f62f8b"
   },
   "outputs": [],
   "source": [
    "if not IN_COLAB:\n",
    "    %run utils.ipynb\n",
    "    PROJECT_ROOT = pathlib.Path(os.path.join(os.curdir, os.pardir))\n",
    "else:\n",
    "    PROJECT_ROOT = pathlib.Path('')\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "\n",
    "NOTEBOOKS_DIR = PROJECT_ROOT / 'notebooks'\n",
    "\n",
    "if (NOTEBOOKS_DIR / 'gt.csv').is_file():\n",
    "    gt = pd.read_csv(NOTEBOOKS_DIR / 'gt.csv')\n",
    "else:\n",
    "    gt = pd.read_csv(DATA_DIR / 'gt.csv')\n",
    "    \n",
    "GT_SRC_LEN = len(gt.index)\n",
    "display(gt)\n",
    "\n",
    "_, ax = plt.subplots(nrows=1, ncols=1, figsize=(21, 7))\n",
    "ax.tick_params(labelrotation=90)\n",
    "\n",
    "g = sns.countplot(x='sign_class', data=gt,  order=sorted(gt['sign_class'].value_counts().index.tolist()));\n",
    "g.set_yscale(\"log\")\n",
    "ax.set_title('SIGN CLASS DISTRIBUTION')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJsXDhtFu_6e",
    "outputId": "d8efeb10-2403-4294-bf18-164f594eba7d"
   },
   "source": [
    "Выкинем все неинтересующие классы.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 563
    },
    "id": "OXSgsegxTQV3",
    "outputId": "e8498fbe-3ef6-4d0d-ee58-8074b0bbd6b2"
   },
   "outputs": [],
   "source": [
    "if len(gt['set'].unique()) == 1:\n",
    "    CLASSES = [\n",
    "        '1_1', '1_6', '1_8', '1_22', '1_31', '1_33', '2_1', '2_2', \n",
    "        # ~2_3_1\n",
    "        '2_3', \\\n",
    "        # /~2_3_1\n",
    "        '2_4', '2_5', '3_1', \n",
    "        # ~3_18_1\n",
    "        '3_18', \n",
    "        # /~3_18_1\n",
    "        '3_20', '3_21', '3_22', '3_23', \\\n",
    "        # all speed limits\n",
    "        '3_24_n10', '3_24_n20', '3_24_n30', '3_24_n40', '3_24_n50', '3_24_n60', \\\n",
    "        '3_24_n70', '3_24_n80', '3_24_n90', '3_24_n100', '3_24_n110', '3_24_n120', '3_24_n130', \\\n",
    "        # /all speed limits\n",
    "        '3_25', '3_27', '3_28', '3_31', '4_1_1', '4_3', '5_5', '5_6', '5_16', \n",
    "        '5_19_1',  \n",
    "        #'5_19_2', \\ ~ '5_19_1'\n",
    "        '5_20', '6_3_2', '6_4', '7_3', '7_4'\n",
    "    ]\n",
    "    \n",
    "    INCLUDED_CLASSES = set()\n",
    "    IS_PRESENT_COLUMN_INDEX = gt.columns.get_loc(\"is_present\")\n",
    "    \n",
    "    for row in gt.itertuples():\n",
    "\n",
    "        if row.sign_class in CLASSES:          \n",
    "            INCLUDED_CLASSES.add(row.sign_class)\n",
    "        else:\n",
    "            gt.iat[row.Index, IS_PRESENT_COLUMN_INDEX] = 0\n",
    "\n",
    "        if (row.Index % 500 == 0) or (row.Index + 1 == GT_SRC_LEN):\n",
    "            printProgressEnum(row.Index, GT_SRC_LEN)\n",
    "            \n",
    "    NOT_INCLUDED_SIGNS = set(CLASSES) - INCLUDED_CLASSES\n",
    "    if NOT_INCLUDED_SIGNS:\n",
    "        print('\\n[!] These signs were not included: \\n', NOT_INCLUDED_SIGNS)\n",
    "        \n",
    "    UNDERSAMPLING_COMPLITED = False\n",
    "    OVERSAMPLING_COMPLITED = False\n",
    "else:\n",
    "    print('using existing gt')\n",
    "    INCLUDED_CLASSES = set(gt[gt['is_present'] == 1]['sign_class'].unique())\n",
    "    UNDERSAMPLING_COMPLITED = True\n",
    "    OVERSAMPLING_COMPLITED = True\n",
    "        \n",
    "_, ax = plt.subplots(nrows=1, ncols=1, figsize=(21, 7))\n",
    "ax.tick_params(labelrotation=90)\n",
    "\n",
    "g = sns.countplot(x='sign_class', data=gt[gt['is_present']==1],  order=sorted(gt[gt['is_present']==1]['sign_class'].value_counts().index.tolist()));\n",
    "ax.set_title('PRESENTED SIGNS CLASS DISTRIBUTION')\n",
    "plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXD4ihDpTQV4"
   },
   "source": [
    "Сплитим по возможности в таком соотношении 20:20:60.  \n",
    "Проблема - одного класса сильно больше чем другого.\n",
    "Используем комбинацию методов, описанных [тут](https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets?scriptVersionId=1756536&cellId=11): применим undersampling до среднего количества по группам. В недостающих группах докинем oversampling'ом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5g9GaTFvTQV5",
    "outputId": "2966c934-9031-4068-c753-1e9adba8fe07"
   },
   "outputs": [],
   "source": [
    "print('Stage 1: UNDERSAMPLING ALL BY MEAN VALUE_COUNTS')\n",
    "\n",
    "if UNDERSAMPLING_COMPLITED == False:\n",
    "    gt_present = gt[gt['is_present']==1]\n",
    "    gt_present_grouped = gt_present.groupby('sign_class', axis=0)\n",
    "    MEAN_BY_GROUPS = int(np.floor(gt_present_grouped.size().mean()))\n",
    "\n",
    "    SET_COLUMN_INDEX = gt.columns.get_loc(\"set\")\n",
    "\n",
    "    for key, items in gt_present_grouped.groups.items():\n",
    "        # print(items)\n",
    "        items = list(items)     # явно приведем к списку для душевного спокойствия\n",
    "        random.shuffle(items)   # перемешаем\n",
    "\n",
    "        # print(key)\n",
    "\n",
    "        if len(items) > MEAN_BY_GROUPS:\n",
    "            # выбираем рандомные значения из этой группы в колличестве MEAN_BY_GROUPS*0.6 для train\n",
    "            # MEAN_BY_GROUPS*0.2 для valid, остальное кинем в test\n",
    "            # print(int(MEAN_BY_GROUPS*0.8))\n",
    "            TEMP_ITEMS_INCLUDED = items[0:MEAN_BY_GROUPS]\n",
    "            TEMP_ITEMS_EXCLUDED = items[MEAN_BY_GROUPS::]\n",
    "            TRAIN_GROUP, VALID_GROUP, TEST_GPOUP = np.split(TEMP_ITEMS_INCLUDED, [int(len(TEMP_ITEMS_INCLUDED)*0.6), int(len(TEMP_ITEMS_INCLUDED)*0.8)])\n",
    "            TEST_GPOUP = np.append(TEST_GPOUP, TEMP_ITEMS_EXCLUDED)\n",
    "            #if key == '3_24_n40':\n",
    "            #    print('TRAIN', sorted(TRAIN_GROUP), '\\nVALID', sorted(VALID_GROUP), '\\nTEST', sorted(TEST_GPOUP))\n",
    "            #    # print('t', len(TRAIN_GROUP), 'v', len(VALID_GROUP), 't', len(TEST_GPOUP))\n",
    "        else:\n",
    "            TRAIN_GROUP, VALID_GROUP, TEST_GPOUP = np.split(items, [int(len(items)*0.6), int(len(items)*0.8)])\n",
    "            # print('t', len(TRAIN_GROUP), 'v', len(VALID_GROUP), 't', len(TEST_GPOUP), '\\n')\n",
    "            # print('t', TRAIN_GROUP, 'v', VALID_GROUP, 't', TEST_GPOUP, '\\n')    \n",
    "\n",
    "\n",
    "        gt.iloc[TRAIN_GROUP, SET_COLUMN_INDEX] = 'train'\n",
    "        gt.iloc[VALID_GROUP, SET_COLUMN_INDEX] = 'valid'\n",
    "        gt.iloc[TEST_GPOUP, SET_COLUMN_INDEX] = 'test'\n",
    "    \n",
    "    print('Stage 1: UNDERSAMPLING COMPLITED')\n",
    "    UNDERSAMPLING_COMPLITED = True\n",
    "else:\n",
    "    print('[!] Stage 1: UNDERSAMPLING ALREADY COMPLITED')\n",
    "\n",
    "print('total ~NaN values in [set]:',gt['set'].value_counts().sum())\n",
    "print('total [is_present]==1:', len(gt[gt['is_present']==1].index))\n",
    "assert len(gt[gt['is_present']==1].index) == gt['set'].value_counts().sum()\n",
    "\n",
    "\n",
    "\n",
    "_, ax = plt.subplots(nrows=3, ncols=1, figsize=(21, 14))\n",
    "\n",
    "LABELS = ['train', 'valid', 'test']\n",
    "for i in range(len(LABELS)):\n",
    "    g = sns.countplot(x='sign_class', data=gt[gt['set']==LABELS[i]],  ax=ax[i], order=sorted(gt[gt['is_present']==1]['sign_class'].value_counts().index.tolist()));\n",
    "    ax[i].tick_params(labelrotation=90)\n",
    "    ax[i].set_title(LABELS[i])\n",
    "    plt.tight_layout()\n",
    "    \n",
    "ax[2].set_yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Yq0ZzofeTQV6",
    "outputId": "70c1c1e3-a382-4b65-b4b5-e7f28c4c66c9"
   },
   "outputs": [],
   "source": [
    "print('Stage 2: OVERSAMPLING TRAIN')\n",
    "\n",
    "if OVERSAMPLING_COMPLITED == False:\n",
    "    gt_present = gt[gt['set']=='train']\n",
    "    gt_present_grouped = gt_present.groupby('sign_class', axis=0)\n",
    "\n",
    "    for key, items in gt_present_grouped.groups.items():\n",
    "\n",
    "        items = list(items)     # явно приведем к списку для душевного спокойствия\n",
    "        random.shuffle(items)   # перемешаем\n",
    "        # print(len(items))\n",
    "        if len(items) < MEAN_BY_GROUPS:\n",
    "            ROWS_TO_FILL_COUNT = int(MEAN_BY_GROUPS*0.6) - len(items)\n",
    "            ROWS_TO_APPEND = gt.iloc[items].sample(ROWS_TO_FILL_COUNT, replace=True)\n",
    "            # print(ROWS_TO_APPEND)\n",
    "            #print(len(gt.index))\n",
    "            gt = gt.append(ROWS_TO_APPEND, ignore_index=True)\n",
    "            #print(len(gt.index))\n",
    "            \n",
    "    print('Stage 2: OVERSAMPLING COMPLITED')\n",
    "    OVERSAMPLING_COMPLITED = True\n",
    "else:\n",
    "    print('[!] Stage 2: OVERSAMPLING ALREADY COMPLITED')\n",
    "\n",
    "gt.to_csv(NOTEBOOKS_DIR / 'gt.csv', index=False)\n",
    "\n",
    "_, ax = plt.subplots(nrows=3, ncols=1, figsize=(21, 14))\n",
    "\n",
    "LABELS = ['train', 'valid', 'test']\n",
    "for i in range(len(LABELS)):\n",
    "    g = sns.countplot(x='sign_class', data=gt[gt['set']==LABELS[i]],  ax=ax[i], order=sorted(gt[gt['is_present']==1]['sign_class'].value_counts().index.tolist()));\n",
    "    ax[i].tick_params(labelrotation=90)\n",
    "    ax[i].set_title(LABELS[i])\n",
    "    plt.tight_layout()\n",
    "    \n",
    "ax[2].set_yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zBVIaE6pTQV7"
   },
   "outputs": [],
   "source": [
    "MODEL_CLASSES = sorted(list(INCLUDED_CLASSES))\n",
    "from sklearn import preprocessing\n",
    "MODEL_LABEL_ENCODER = preprocessing.LabelEncoder()\n",
    "\n",
    "MODEL_CLASSES_TARGETS = MODEL_LABEL_ENCODER.fit_transform(MODEL_CLASSES)\n",
    "MODEL_CLASS_MAP = dict(zip(MODEL_CLASSES, MODEL_CLASSES_TARGETS))\n",
    "MODEL_CLASS_UNMAP = dict(zip(MODEL_CLASSES_TARGETS, MODEL_CLASSES))\n",
    "\n",
    "class SignDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, directory, set_label):\n",
    "        # print((df['is_present'] == 1) & (df['set']==set_label))\n",
    "        self.df = df[(df['is_present'] == 1) & (df['set']==set_label)]\n",
    "        self.dir = directory\n",
    "        # print(self.df.head(50))\n",
    "    def __len__(self):\n",
    "        return len(self.df.index)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        label = self.df.iloc[index]['sign_class']\n",
    "        path = self.dir / self.df.iloc[index]['filename']\n",
    "        # print(path)\n",
    "        img = cv2.imread(str(path))\n",
    "        # plt.imshow(img)\n",
    "        # plt.show()\n",
    "        img = cv2.resize(img, (160, 160))\n",
    "        # print(img)\n",
    "        img_tnsr = torch.Tensor.permute(torch.Tensor(img), [2, 0, 1]).div(255)\n",
    "        # print(label)\n",
    "        encoded_label = MODEL_CLASS_MAP[label]\n",
    "        # print(encoded_label)\n",
    "        return img_tnsr, encoded_label\n",
    "    \n",
    "train_dataset = SignDataset(gt, DATA_DIR / 'merged-rtsd', 'train')\n",
    "valid_dataset = SignDataset(gt, DATA_DIR / 'merged-rtsd', 'valid')\n",
    "test_dataset = SignDataset(gt, DATA_DIR / 'merged-rtsd', 'test')\n",
    "\n",
    "if IN_COLAB:\n",
    "    batch_size = 512\n",
    "else:\n",
    "    batch_size = 4\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "        shuffle=True)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "        shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 936
    },
    "id": "OboAl3B5TQV8",
    "outputId": "e88adb23-8164-4393-d1f0-1701db1eeecf"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'lr': 0.01,\n",
    "    'epochs': 150,\n",
    "}\n",
    "\n",
    "from torchvision import models, transforms as transforms\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.fc = nn.Linear(512, len(MODEL_CLASSES))\n",
    "\n",
    "loss_op = nn.CrossEntropyLoss().cuda()\n",
    "optim = torch.optim.Adadelta(model.parameters(), lr=config['lr'])\n",
    "\n",
    "if os.path.isfile(DATA_DIR / 'resnet18_rtsd_test'):\n",
    "    model.load_state_dict(torch.load(DATA_DIR / 'resnet18_rtsd_test'))\n",
    "    print('[+] Model restored from save file!')\n",
    "\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'valid_acc': []\n",
    "}\n",
    "\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "SHOULD_I_TRAIN = False\n",
    "if SHOULD_I_TRAIN:\n",
    "    for epoch in range(config['epochs']):\n",
    "\n",
    "        history['train_loss'].append(train_epoch(model, train_loader, loss_op, optim, device))\n",
    "\n",
    "        print(f'Epoch {epoch}:')\n",
    "        print('  Train loss:', history['train_loss'][-1])\n",
    "\n",
    "        history['valid_acc'].append(valid_epoch(model, valid_loader, device))\n",
    "        # model.incIter()\n",
    "        print('  Valid acc:', history['valid_acc'][-1])\n",
    "\n",
    "        now = datetime.now()\n",
    "        model_save_name = 'resnet18_rtsd_test_{}_loss{:.4f}_acc_{:.4f}'.format(now.strftime(\"%m.%d_%H.%M\"),\n",
    "                                                                      history['train_loss'][-1],\n",
    "                                                                      history['valid_acc'][-1])    \n",
    "        torch.save(model.state_dict(), model_save_name)\n",
    "        if IN_COLAB:\n",
    "            shutil.copy2(model_save_name, '/content/drive/MyDrive/MastersD/RTSD_dataset+/rtsd-r3/')\n",
    "\n",
    "        torch.save(model.state_dict(), 'resnet18_rtsd_test')\n",
    "        if IN_COLAB:\n",
    "            shutil.copy2('resnet18_rtsd_test', '/content/gdrive/MyDrive/MastersD/RTSD_dataset+/rtsd-r3/')\n",
    "\n",
    "\n",
    "    print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRandomFromDataset(gt: pd.DataFrame, label='test'):\n",
    "    gt = gt[gt['is_present'] == 1]\n",
    "    random_instance = gt[gt['set']==label].sample(1)\n",
    "    img_path = DATA_DIR / 'merged-rtsd' / random_instance['filename'].values[0]\n",
    "    sign_class = random_instance['sign_class'].values[0]\n",
    "    # print(img_path)\n",
    "    img = cv2.imread(str(img_path))\n",
    "    # print(img)\n",
    "    img_NT = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    img = cv2.resize(img, (160, 160))\n",
    "    img_T = torch.Tensor.permute(torch.Tensor(img), [2, 0, 1]).div(255)\n",
    "\n",
    "        \n",
    "    return img_NT, img_T.cuda(), sign_class, img_path\n",
    "\n",
    "img, img_t, target_label, filepath = getRandomFromDataset(gt)\n",
    "print(img.shape)\n",
    "# showImg(img, target_label)\n",
    "predicted_label = getLabelFromModelOutput(model(img_t[None, ...]))\n",
    "# print('Predicted Label', predicted_label)\n",
    "#while(target_label == predicted_label):\n",
    "#    img, img_t, target_label, filepath = getRandomFromDataset(gt)\n",
    "#    predicted_label = getLabelFromModelOutput(model(img_t[None, ...]))\n",
    "\n",
    "showImg(img, target_label)\n",
    "print('Predicted Label', predicted_label, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1FkYsLnd9IWk",
    "outputId": "f35d1634-ae0a-4cc5-a26e-ccd1abe0c0ca"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "79DuV9tV9oz8",
    "outputId": "82f22e9c-b75e-40fa-e0a5-1e1e04e8f71f"
   },
   "outputs": [],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VwuZo3RC2QZC",
    "outputId": "d480a577-675d-4b43-f720-a74077021fe5"
   },
   "outputs": [],
   "source": [
    "!7z a resnets resnet18_*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XcB8vXrDTQV8"
   },
   "outputs": [],
   "source": [
    "img_t, label_e = test_dataset[8]\n",
    "showTensorPicture(img_t, label=MODEL_CLASS_UNMAP[label_e])\n",
    "print(MODEL_CLASS_UNMAP[label_e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MznT1TWVOvkB"
   },
   "outputs": [],
   "source": [
    "def showTensorPicture(tensor_image):\n",
    "    plt.imshow(tensor_image.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZJgjH-Ks5eXs"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download(\"/content/merged.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lhDFzpAnOr4S"
   },
   "outputs": [],
   "source": [
    "gt = pd.concat([gt_train, gt_test_copy])\n",
    "gt['Set'] = pd.Series([None for x in range(len(gt.index))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTKgGcJ-NBcn"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rXQPvXv3MlMp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "whyqWj47Lw9I"
   },
   "outputs": [],
   "source": [
    "gt_test_copy = gt_test.copy()\n",
    "for index, row in gt_test_copy.iterrows():\n",
    "  # print(row)\n",
    "  target_filename = incrementFile(row['filename'], inc_val)\n",
    "  print(row['filename'], '->', target_filename)\n",
    "\n",
    "  gt_test_copy.loc[index, 'filename'] = incrementFile(row['filename'], inc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ndPIXBXyIHPu"
   },
   "outputs": [],
   "source": [
    "display(gt_test)\n",
    "display(gt_train)\n",
    "print(inc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C6uO0UiXAHVA"
   },
   "outputs": [],
   "source": [
    "!ls rtsd-r3/data/ALL"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "1_ClassifierResearch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
